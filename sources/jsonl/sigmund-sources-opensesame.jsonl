{"content": "title: StimSync\n\nStimSync is an open-source open-hardware device for handling input (e.g., button presses) and output (e.g., triggers) in psychological and neuroscientific experiments. StimSync offers examples for use with OpenSesame.\n\nFor more information, see:\n\n- <http://www.mccauslandcenter.sc.edu/crnl/stimsync-0>", "url": "https://osdoc.cogsci.nl/4.0/manual/devices/stimsync", "title": "StimSync"}
{"content": "title: random functions (random-ext)\n\n\nThe `random-ext` library is available as `random`. This library provides many convenient, higher-level functions for randomization.\n\n__Example:__\n\nDraw eight circle with a random color and a location that is randomly sampled from a five by five grid: \n\n```js\nlet positions = xy_grid(5, 50)\npositions = random.subArray(positions, 8)\nconst cnv = Canvas()\ncnv.fixdot()\nfor (const [x, y] of positions) {\n    cnv.circle({x: x, y: y, r: 20, fill: true, color: random.color()})\n}\ncnv.show()\n```\n\nFor an overview, see:\n\n- <https://www.npmjs.com/package/random-ext>", "url": "https://osdoc.cogsci.nl/4.0/manual/javascript/random", "title": "random functions (random-ext)"}
{"content": "title: SR Box\n\n[TOC]\n\n## About the srbox plugin\n\nThe serial response (SR) box is a button box, specifically designed for response collection in psychological experiments. The original version, developed by Psychology Software Tools, has 5 buttons, 5 lights, and is connected to the PC trough the serial port. There are also SR Box compatible devices by other manufacturers, which may differ in the number of buttons and lights and often use a USB connection, which emulates a serial port.\n\nThe SRBOX plugin for OpenSesame allows you to use the SR Box or compatible device in your OpenSesame experiments.\n\n## Screenshot\n\n\n![/pages/manual/response/img/srbox/srbox.png](/4.0/pages/manual/response/img/srbox/srbox.png)\n\n__Figure 1.__ The srbox plugin in OpenSesame.\n{: .fig-caption #FigSrbox}\n\n\n\n## Setting the device name\n\nBy default, the plugin tries to autodetect your SR Box. If this works, you don't have to change it. If your experiment freezes, OpenSesame has chosen the wrong serial port and you must enter the device name manually. Under Windows, the device is probably called something like\n\n```text\nCOM4\n```\n\nUnder Linux the device is probably called something like\n\n```text\n/dev/tty0\n```\n\n## Requirements\n\nAn SR Box or compatible button box. Not all button boxes are compatible, see:\n\n- <https://osdoc.cogsci.nl/4.0/manual/response/buttonbox>\n\n## Using the SR Box from Python inline code\n\nThe `srbox` object does *not* exist when the plug-in is in dummy mode.\n\n<div class=\"ClassDoc YAMLDoc\" markdown=\"1\">\n\n# instance __srbox__\n\nIf you insert the srbox plugin at the start of your experiment, an\ninstance of SRBOX automatically becomes part of the experiment\nobject and\ncan be accessed within an inline_script item as SRBOX.\n\n__Important note1:__\n\nIf you do not specify a device, the plug-in will try to autodetect\nthe\nSR Box port. However, on some systems this freezes the experiment, so\nit is better to explicitly specify a device.\n\n__Important note 2:__\n\nYou\nneed to call [srbox.start] to put the SR Box in sending mode,\nbefore\ncalling [srbox.get_button_press] to collect a button press.\n\n__Example:__\n~~~ .python\nt0 = clock.time()\nsrbox.start()\nbutton, t1 = srbox.get_button_press(allowed_buttons=[1, 2],\n                                    require_state_change=True)\nif button == 1:\n    response_time = t1 - t0\nprint(f'Button 1 was pressed in {response_time} ms!')\nsrbox.stop()\n~~~\n[TOC]\n\n## get_button_press(allowed_buttons=None, timeout=None, require_state_change=False)\n\nCollects a button press from the SR box.\n\n\n__Parameters__\n\n- **allowed_buttons**: A list of buttons that are accepted or `None` to accept all\nbuttons. Valid buttons are integers 1 through 8.\n- **timeout**: A timeout value in milliseconds or `None` for no timeout.\n- **require_state_change    Indicates whether already pressed button should be accepted**: (False), or whether only a state change from unpressed to pressed\nis accepted (True).\n\n__Returns__\n\n- A `(button_list, timestamp)` tuple. `button_list` is `None` if no \nbutton was pressed (i.e. a timeout occurred).\n\n\n## send(ch)\n\nSends a single character to the SR Box. Send '`' to turn off all\nlights, 'a' for light 1 on, 'b' for light 2 on,'c' for lights\n1 and 2 on etc.\n\n\n__Parameters__\n\n- **ch**: The character to send. If a `str` is passed, it is encoded to\n`bytes` using utf-8 encoding.\n\n\n## start(self)\n\nTurns on sending mode, so that the SR Box starts to send output.\nThe SR Box must be in sending mode when you call\n[srbox.get_button_press].\n\n\n\n\n## stop(self)\n\nTurns off sending mode, so that the SR Box stops giving output.\n\n\n\n\n</div>\n\n", "url": "https://osdoc.cogsci.nl/4.0/manual/response/srbox", "title": "SR Box"}
{"content": "title: Visual stimuli\n\nThe most common way to present visual stimuli is using the SKETCHPAD item, or, for non-time-critical stimuli, the FEEDBACK item.\n\n\n[TOC]\n\n\n## Using the sketchpad and feedback items\n\nThe SKETCHPAD and FEEDBACK item offer basic what-you-see-is-what-you get drawing tools ([Figure 1](#FigSketchpad)).\n\n\n![/pages/manual/stimuli/img/visual/sketchpad.png](/4.0/pages/manual/stimuli/img/visual/sketchpad.png)\n\n__Figure 1.__ The SKETCHPAD provides built-in drawing tools.\n{: .fig-caption #FigSketchpad}\n\n\n\n\n## Using show-if expressions\n\nYou can use show-if expressions to determine whether or not a particular element should be shown. For example, if you have an image of a happy face that should be shown only when the variable `valence` has the value 'positive', then you can set the show-if expression for the corresponding image element to:\n\n```python\nvalence == 'positive'\n```\n\nIf you leave a show-if expression empty or enter `True`, element will always be shown. Show-if expressions use the same syntax as other conditional expressions. For more information, see:\n\n- <https://osdoc.cogsci.nl/4.0/manual/variables>\n\nShow-if expressions are evaluated at the moment that the display is prepared. This means that for SKETCHPAD items, they are evaluated during the prepare phase, whereas for FEEDBACK items, they are evaluated during the run phase (see also the section below).\n\n\n## The difference between sketchpad and feedback items\n\nThe SKETCHPAD and FEEDBACK items are identical in most ways, except for two important differences.\n\n\n### Sketchpad items are prepared in advance, feedback items are not\n\nThe contents of a SKETCHPAD are prepared during the prepare phase of the SEQUENCE that it is part of. This is necessary to ensure accurate timing: It allows the SKETCHPAD to be shown right away during the run phase, without any delays due to stimulus preparation. However, the downside of this is that the contents of a SKETCHPAD cannot depend on what happens during the SEQUENCE that it is part of. For example, you cannot use a SKETCHPAD to provide immediate feedback on the response time collected by a KEYBOARD_RESPONSE item (assuming that the SKETCHPAD and KEYBOARD_RESPONSE are part of the same sequence.)\n\nIn contrast, the contents of a FEEDBACK item are only prepared when they are actually shown, that is, during the run phase of the SEQUENCE that it is part of. This makes it possible to provide feedback on things that just happened--hence the name. However, the FEEDBACK item should not be used to present time-critical stimuli, because it suffers from delays due to stimulus preparation.\n\nFor more information about the prepare-run strategy, see:\n\n- <https://osdoc.cogsci.nl/4.0/manual/prepare-run>\n\n\n### Feedback variables are (by default) reset by feedback items\n\nThe FEEDBACK item has an option 'Reset feedback variables'. When this option is enabled (it is by default), feedback variables are reset when the FEEDBACK item is shown.\n\nFor more information about feedback variables, see:\n\n- <https://osdoc.cogsci.nl/4.0/manual/variables>\n\n\n## Presenting visual stimuli in Python inline script\n\n### Accessing a SKETCHPAD in Python\n\nYou can access the `Canvas` object for a SKETCHPAD as the items `canvas` property. For example, say that your SKETCHPAD is called *my_sketchpad*, and contains an image elements with the name 'my_image'. You could then have this image rotate with the following script:\n\n~~~ .python\nmy_canvas = items['my_sketchpad'].canvas\nfor angle in range(360):\n\tmy_canvas['my_image'].rotation = angle\n\tmy_canvas.show()\n~~~\n\n\n### Creating a Canvas in Python\n\nYou can use the `Canvas` object to present visual stimuli in Python:\n\n- <https://osdoc.cogsci.nl/4.0/manual/python/canvas>", "url": "https://osdoc.cogsci.nl/4.0/manual/stimuli/visual", "title": "Visual stimuli"}
{"content": "The document provides an intermediate tutorial on how to create a visual-search experiment using OpenSesame, a program for developing behavioral experiments. OpenSesame supports Python and JavaScript, can be used in a traditional lab-based setting, and is freely available under the General Public License v3.\n\nThe tutorial guides users in creating a basic visual-search experiment, where participants search for a target object among distractor objects, under three different conditions. The conditions, 'Conjunction', 'Shape Feature', and 'Color Feature', vary the characteristics of the distractor objects.\n\nThe tutorial adopts a top-down and defensive programming approach, starting with abstract logic and gradually working down to the implementation details, while emphasizing the importance of building sanity checks into the code to prevent errors.\n\nThe experiment design is fully crossed, with all combinations of conditions occurring and varied within and between blocks of trials. The tutorial provides a step-by-step guide and offers resources for downloading the latest version of OpenSesame, accessing dedicated documentation, and joining the support forum.\n\nThe tutorial also teaches how to create the basic structure of the experiment, define experimental variables, give instructions, create the trial sequence, generate the search display, define the correct response, and give per-trial feedback. \n\nAdditionally, the tutorial gives a full description of how to implement the Python code for the experiment, with explanations for each step in the process. It also offers links for more information on specific topics.", "url": "https://osdoc.cogsci.nl/4.0/tutorials/intermediate", "title": "Intermediate tutorial (Python) visual search"}
{"content": "title: Running experiments online\n\n\nThis page has been moved to:\n\n- <https://osdoc.cogsci.nl/4.0/manual/osweb/workflow>", "url": "https://osdoc.cogsci.nl/4.0/manual/osweb", "title": "Running experiments online"}
{"content": "title: Runners\n\n\n[TOC]\n\n\n## About runners\n\nThere are several technically different ways in which you can execute your experiment. Each of these corresponds to a *runner*. You can select a runner under Menu \u2192 Tools \u2192 Preferences \u2192 Runner.\n\nUnless you have a reason not to, you should use the *multiprocess* runner. However, if OpenSesame sometimes crashes, you can try whether selecting a different runner resolves this.\n\n\n## Available runners\n\n### Multiprocess\n\nThe *multiprocess* runner executes your experiment in a different process. The benefit of this approach is that your experiment can crash without bringing the user interface down with it. Another advantage of the *multiprocess* runner is that it allows the variable inspector to show your experimental variables while the experiment is running.\n\n### Inprocess\n\nThe *inprocess* runner executes the experiment in the same process as the user interface. The benefit of this approach is its simplicity. The downside is that the user interface may crash if the experiment crashes, and vice versa.\n\n### External\n\nThe *external* runner executes the experiment by launching opensesamerun as a separate application. The benefit of this approach is that your experiment can crash without bringing the user interface down with it.", "url": "https://osdoc.cogsci.nl/4.0/manual/runners", "title": "Runners"}
{"content": "title: The prepare-run strategy\n\n[TOC]\n\n## About\n\nExperiments typically consist of short intervals ('trials') during which participants perceive stimuli and perform a task. Timing should be controlled during a trial, but some unpredictable variation in the duration of the interval between trials is acceptable. Therefore, a good strategy is to perform time-consuming tasks before a trial, and to keep the operations that are performed during a trial to a minimum.\n\nOpenSesame does this by calling each element from a SEQUENCE item twice. This is the *prepare-run strategy*:\n\n- During the Prepare phase, items are given the opportunity to prepare. For example, a SYNTH generates a sound (but doesn't play it); and a SKETCHPAD draws a canvas (but doesn't show it).\n- During the Run phase, items do as a little as possible. For example, a SYNTH plays back a previously prepared sound; and a SKETCHPAD shows a previously prepared canvas.\n\nThis reduces the risk of timing glitches. The prepare-run strategy is implemented at the level of SEQUENCE items, which typically contains the time-critical parts of an experiment. This means that before a SEQUENCE is started, there is some unpredictable temporal jitter.\n\n## Item-specific notes\n\n### loop items\n\nA LOOP item is not prepared in advance. It is important to take this into account when using a LOOP to implement time-critical parts. For example, you may be tempted to implement an RSVP stream using a LOOP item as follows:\n\n~~~text\nrsvp_loop item (4 cycles)\n- stimulus_item\n~~~\n\nIn this construction, *stimulus_item* will be prepared and run four times in alternation, like so:\n\n~~~text\nprepare stimulus_item\nrun stimulus_item\nprepare stimulus_item\nrun stimulus_item\nprepare stimulus_item\nrun stimulus_item\nprepare stimulus_item\nrun stimulus_item\n~~~\n\nTherefore, you need to verify that the preparation of *stimulus_item* does not cause any timing glitches.\n\n### sequence items\n\nAll items that are part of a SEQUENCE are prepared in advance. Therefore, the following construction ...\n\n~~~text\ntrial_sequence\n- fixation_sketchpad\n- target_sketchpad\n- keyboard_response\n- logger\n~~~\n\n... will be executed as follows ...\n\n~~~text\nprepare fixation_sketchpad\nprepare target_sketchpad\nprepare keyboard_response\nprepare logger\nrun fixation_sketchpad\nrun target_sketchpad\nrun keyboard_response\nrun logger\n~~~\n\n### sketchpad and feedback items\n\nSKETCHPAD and FEEDBACK items differ in when they are prepared. For SKETCHPADs preparation occurs during the Prepare phase; for FEEDBACK items, preparation occurs only during the Run phase.\n\nFor more information, see:\n\n- <https://osdoc.cogsci.nl/4.0/manual/stimuli/visual>\n\n### synth and sampler items\n\nFor SYNTH and SAMPLER items, the sound is generated and preloaded during the Prepare phase.\n\n### inline_script items\n\nIn an INLINE_SCRIPT item, you can choose how you want to implement the run and prepare strategy. In general, it is good practice to adhere to the following guidelines:\n\n- Time-consuming, preparatory functionality goes in the Prepare phase. For example, creating canvas objects, and generating sounds.\n- A minimum amount of code is put in the run phase. For example, only showing a previously prepared canvas.\n\n### Other items and plugins\n\nIn general, items should follow the principle of performing as much as possible time-consuming preparation during the Prepare phase, and minimizing the Run phase. However, every plugin is implemented differently. If you are unsure about a specific case, please post a query on the forum.\n\n## Conditional expressions (run if, show if, break if, etc)\n\nIn SEQUENCE items, the 'Run if' condition is evaluated at the last moment, during the run phase. Therefore, you can use a condition like `correct == 0` which depends on the results of a KEYBOARD_RESPONSE item which has been called just before. It is important to take into account that the 'Run if' expression applies *only* to the run phase of an item\u2014The prepare phase is *always* executed.\n\nIn COROUTINES items, the 'Run if' condition is evaluated during the Prepare phase. Therefore, the conditions cannot depend on events that occur during the execution of the COROUTINES.\n\nIn SKETCHPAD items, the 'Show if' condition is evaluated during the Prepare phase, when the canvas is constructed. In FEEDBACK items, the 'Show if' condition is evaluated during the Run phase (because the canvas is only constructed in the Run phase).", "url": "https://osdoc.cogsci.nl/4.0/manual/prepare-run", "title": "The prepare-run strategy"}
{"content": "This API provides Python access to the file pool of OpenSesame, a software for implementing psychology experiments. The `pool` object, which is automatically created when the experiment starts, provides dictionary-like access to the file pool and does not need to be imported separately. \n\nUsers can check if a file exists in the file pool or retrieve the full path to a file in the file pool using commands such as `if 'img.png' in pool:` or `pool[\"img.png\"]` respectively. \n\nThe available functions include:\n\n1. `add(path, new_name=None)`: Copies a file to the file pool. The parameters are `path` (the full path to the file on disk) and `new_name` (a new name for the file in the pool, or None to use the file's original name).\n2. `clean_up(self)`: Removes the pool folder.\n3. `fallback_folder(self)`: Returns the full path to the fallback pool folder, or `None` if this folder does not exist.\n4. `files(self)`: Returns all files in the file pool in the form of a list of full paths.\n5. `folder(self)`: Returns the full path to the main pool folder.\n6. `folders(include_fallback_folder=True, include_experiment_path=False)`: Returns a list of all folders that are searched when retrieving the full path to a file.\n7. `in_folder(path)`: Checks whether a given path is in the pool folder. It only checks the main pool folder, and not the fallback pool folder and experiment folder.\n8. `rename(old_path, new_path)`: Renames a file in the pool folder. The parameters are `old_path` (the old file name) and `new_path` (the new file name).\n9. `size(self)`: Returns the combined size in bytes of all files in the file pool.", "url": "https://osdoc.cogsci.nl/4.0/manual/python/pool", "title": "Access the file pool"}
{"content": "title: Quest staircase init\n\nInitializes a new Quest staircase procedure.", "url": "https://osdoc.cogsci.nl/4.0/items/quest_staircase_init", "title": "Quest staircase init"}
{"content": "title: OpenSesameRun (no GUI)\n\n## About\n\n`opensesamerun` is a simple tool that allows you to execute OpenSesame experiments with a minimal GUI, or directly, by specifying all necessary options via the command line. A minimal GUI will automatically appear if not all command line options have been specified, notably the experiment file, the subject number, and the log file.\n\n~~~\nUsage: opensesamerun [experiment] [options]\n\nOptions:\n  --version             show program's version number and exit\n  -h, --help            show this help message and exit\n\n  Subject and log file options:\n    -s SUBJECT, --subject=SUBJECT\n                        Subject number\n    -l LOGFILE, --logfile=LOGFILE\n                        Logfile\n\n  Display options:\n    -f, --fullscreen    Run fullscreen\n    -c, --custom_resolution\n                        Do not use the display resolution specified in the\n                        experiment file\n    -w WIDTH, --width=WIDTH\n                        Display width\n    -e HEIGHT, --height=HEIGHT\n                        Display height\n\n  Miscellaneous options:\n    -d, --debug         Print lots of debugging messages to the standard\n                        output\n    --stack             Print stack information\n\n  Miscellaneous options:\n    --pylink            Load PyLink before PyGame (necessary for using the\n                        Eyelink plug-ins in non-dummy mode)\n~~~\n\n## Example\n\nLet's say that you want to run the gaze cuing example experiment, for subject #1, and save the log file in your Documents folder (this example assumes Linux, but it works analogously on other platforms):\n\n~~~\nopensesamerun /usr/share/opensesame/examples/gaze_cuing.opensesame.tar.gz -s 1 -l /home/sebastiaan/Documents/subject1.tsv -f \n~~~\n\n\n## Alternative `libopensesame`\n\nYou can also start experiments without using the GUI through the `libopensesame` Python module:\n\n- <https://osdoc.cogsci.nl/4.0/manual/python/nogui>", "url": "https://osdoc.cogsci.nl/4.0/manual/opensesamerun", "title": "OpenSesameRun (no GUI)"}
{"content": "The documentation provides guidelines on debugging for OpenSesame, detailing how to identify and rectify errors using different debugging tools. Debugging in the user interface can be done using the Variable Inspector, which provides an overview of all active variables and can help identify potential issues. Debugging can also be done by printing debug messages to the IPython/Jupyter console using the Python `print()` function. Understanding error messages from the user interface and Python is also essential. \n\nFor debugging in a web browser, the JavaScript `console.log()` function and the `debugger` statement are used. The former prints debug messages to the browser console, while the latter allows the inspection of the JavaScript workspace by inserting breakpoints in the code.\n\nThe documentation also explains how to handle `ExperimentProcessDied` errors, which typically indicate a problem with the underlying Python process or associated libraries. The suggested remedies include changing the backend and updating OpenSesame and all associated packages.", "url": "https://osdoc.cogsci.nl/4.0/manual/debugging", "title": "Debugging"}
{"content": "title: Sound\n\nThe most common way to play sound is using the SAMPLER item, for playback of audio files, or the SYNTH item, for playback of simple beeps, etc.\n\n[TOC]\n\n## The sampler\n\nThe SAMPLER plays back a single sound file, typically from the file pool.\n\nSound files are always played back at the sampling rate that is used by the OpenSesame sampler backend. If your sample appears to be sped up (high pitch) or slowed down (low pitch), you can adjust the sampling rate of your sound file in a sound editor, or change the sampling rate used by the OpenSesame sampler backend (under 'Show backend settings and info' in the General tab).\n\nThe SAMPLER has a few options:\n\n- *Sound file* indicates the file to be played.\n- *Volume* between 0 (silent) and 1 (normal volume).\n- *Pan* turns the right (negative values) or left (positive values) channel down. For full panning, enter 'left' or 'right',\n- *Pitch* indicates the playback speed, where 1 corresponds to the original speed.\n- *Stop after* indicates for how long the sound file should be played. For example, a value of 100 ms means that playback will be stopped after 100 ms, regardless of how long the sound file is. A value of 0 ms means that the sound file will be played back completely.\n- *Fade in* indicates the fade-in time for the sound file. For example, a value of 100 ms means that the sound file will start silent, and build up to maximum value in 100 ms.\n- *Duration* indicates the duration of the sampler item, before the next item is presented. This doesn't need to match the length of the sound file. For example, if the duration of the sampler is set to 0 ms, OpenSesame will advance directly to the item that follows the SAMPLER (e.g., a sketchpad), *while the sound file continues playing in the background*. In addition to a numeric value, you can set duration to:\n\t- 'keypress' to wait for a key press\n\t- 'mouseclick' to wait for a mouse click\n\t- 'sound' to wait until the sampler has finished playing.\n\n## The synth\n\nThe SYNTH is a basic sound synthesizer.\n\nYou can specify a\nnumber of options:\n\n- *Waveform* can be set to sine, sawtooth, square, or white noise\n- *Attack* is the time it takes for the sound the reach maximum volume (i.e. fade in).\n- *Decay* is the time it takes for the sound to die out (i.e. fade out). Note that the decay occurs within the length of the sound.\n- *Volume* between 0 and 100%\n- *Pan* turns the right (negative values) or left (positive values) channel down. Setting pan to -20 or 20 completely mutes the right or left channel, respectively.\n- *Length* indicates the length of the sound (in milliseconds).\n- *Duration* indicates the duration of the SYNTH item, before the next item is presented. This doesn't need to match the length of the sound. For example, the duration of the SYNTH may be set to 0ms, in order to advance directly to the next item (e.g., a SKETCHPAD), while the sound continues playing in the background. In addition to a numeric value, you can set the duration to 'keypress', to wait for a keyboard press, 'mouseclick', to wait for a mouse click, or 'sound', to wait until the SYNTH has finished playing.\n\n## Sound playback in Python\n\nYou can use the SAMPLER object and the SYNTH function to present visual stimuli in Python:\n\n- <https://osdoc.cogsci.nl/4.0/manual/python/sampler>\n- <https://osdoc.cogsci.nl/4.0/manual/python/common>\n\n\n## Audio Low Latency plugins\n\nThe main goal of the Audio Low Latency plugins, developed by Bob Rosbag, is to play and record audio with minimal and predictable latencies to achieve a high accuracy and precision. The `PyAlsaAudio` package which uses the Linux ALSA audio system provided the best results within Python. `PortAudio` and `sounddevice` are cross-platform and work on both Windows as Linux.\n\nThe plugins are not installed by default, but can be installed through pip:\n\n```bash\npip install opensesame-plugin-audio-low-latency\n```\n\nSee also:\n\n- <https://pypi.org/project/opensesame-plugin-audio-low-latency/>", "url": "https://osdoc.cogsci.nl/4.0/manual/stimuli/sound", "title": "Sound"}
{"content": "title: Tobii\n\nPyGaze offers *experimental* support for Tobii eye trackers.\n\n`tobii-research` is the Python library for Tobii support. As of July 2023, `tobii-research` requires Python 3.10, whereas OpenSesame by default uses Python 3.11. Therefore, until `tobii-research` is updated for Python 3.11, the easiest way to install OpenSesame with Tobii support is by building a Python 3.10 environment through Anaconda.\n\nThis sounds complicated, but it is really not. To do so, first read the general procedure for installing OpenSesame through Anaconda as described on the Downloads page:\n\n- <https://osdoc.cogsci.nl/4.0/download>\n\nNext, once you understand the general procedure, start by creating a Python 3.10 environment, continue with the instructions from the Downloads page, and then install `tobii-research`:\n\n```\n# Start by creating a Python 3.10 environment\nconda create -n opensesame-py3 python=3.10\nconda activate opensesame-py3\n# Now follow the instructions from the downloads page\n# ...\n# Then install Tobii support\npip install tobii-research\n# And now launch OpenSesame!\nopensesame\n```\n\nFor more information, see:\n\n- <https://osdoc.cogsci.nl/4.0/manual/eyetracking/pygaze>\n- <https://rapunzel.cogsci.nl/manual/environment/>\n- <http://www.tobii.com/en/eye-tracking-research/global/>", "url": "https://osdoc.cogsci.nl/4.0/manual/eyetracking/tobii", "title": "Tobii"}
{"content": "title: Mechanical Turk\n\n\nThere is currently no information that is specific to running OSWeb experiments on Mechanical Turk. For general information about connecting JATOS to Mechanical Turk, see:\n\n- <http://www.jatos.org/Connect-to-Mechanical-Turk.html>", "url": "https://osdoc.cogsci.nl/4.0/manual/osweb/mturk", "title": "Mechanical Turk"}
{"content": "title: Mouse tracking\n\nMousetrap is a third-party plugin, and is not maintained by the OpenSesame team.\n{: .alert .alert-info}\n\n## About\n\nPascal Kieslich and Felix Henninger have developed the [mousetrap plugins](https://github.com/PascalKieslich/mousetrap-os) for OpenSesame [(Kieslich & Henninger, 2017)](https://dx.doi.org/10.3758/s13428-017-0900-z). These plugins allow you to track the movement of the mouse cursor, which has been used to investigate the time course of cognitive processes in many psychological domains [(Freeman, Dale, & Farmer, 2011)](https://dx.doi.org/10.3389/fpsyg.2011.00059).\n\nMousetrap offers two plugins for mouse tracking in OpenSesame that can be included in the experiment via drag-and-drop.\nThe [mousetrap response plugin](https://github.com/PascalKieslich/mousetrap-os/blob/master/plugins/mousetrap_response/mousetrap_response.md) tracks mouse movements while another stimulus (e.g., a sketchpad) is shown, analogous to a keyboard or mouse response item.\nThe [mousetrap form plugin](https://github.com/PascalKieslich/mousetrap-os/blob/master/plugins/mousetrap_form/mousetrap_form.md) allows for tracking of mouse movements in [custom forms](<https://osdoc.cogsci.nl/4.0/manual/forms/custom>).\nBesides, both plugins also provide Python classes, which can be used in Python inline scripts for maximum customizability.\n\nOnce data have been collected with the plugins, the data can be processed, analyzed and visualized using the [mousetrap R package](http://pascalkieslich.github.io/mousetrap/).\n\n## Installation\n\nInformation about how to install the mousetrap plugin can be found on its [GitHub page](https://github.com/PascalKieslich/mousetrap-os#installation). A number of example experiments that demonstrate the basic features are available in the [examples folder](https://github.com/PascalKieslich/mousetrap-os/tree/master/examples#example-experiments).\n\n\nSee also:\n\n- <https://rapunzel.cogsci.nl/manual/environment/>", "url": "https://osdoc.cogsci.nl/4.0/manual/mousetracking", "title": "Mouse tracking"}
{"content": "title: Using the interface\n\nOpenSesame has a powerful graphical interface that consists of several components ([Figure 1](#FigInterface)).\n\n\n![/pages/manual/img/interface/interface.png](/4.0/pages/manual/img/interface/interface.png)\n\n__Figure 1.__ The OpenSesame user interface.\n{: .fig-caption #FigInterface}\n\n\n\n\n[TOC]\n\n## Toolbars and menubar\n\n### The menubar\n\nThe menubar ([Figure 2](#FigMenubar)) is shown at the top of the window, or, on some operating systems, is integrated into the border around the window. The menubar contains general functionality, such as saving and opening experiments, running experiments, etc.\n\n\n![/pages/manual/img/interface/menubar.png](/4.0/pages/manual/img/interface/menubar.png)\n\n__Figure 2.__ The menubar.\n{: .fig-caption #FigMenubar}\n\n\n\n### The main toolbar\n\nThe main toolbar ([Figure 3](#FigMainToolbar)) is (by default) shown at the top of the window, just below the menubar. The main toolbar contains a selection of the most relevant functionality from the menubar.\n\n\n![/pages/manual/img/interface/main-toolbar.png](/4.0/pages/manual/img/interface/main-toolbar.png)\n\n__Figure 3.__ The main toolbar.\n{: .fig-caption #FigMainToolbar}\n\n\n\n### The item toolbar\n\nThe item toolbar ([Figure 4](#FigItemToolbar)) is (by default) shown at the left of the window. The item toolbar contains all items, that is, all building blocks of an experiment. You can add items to your experiment by dragging them from the item toolbar into the overview area.\n\n\n![/pages/manual/img/interface/item-toolbar.png](/4.0/pages/manual/img/interface/item-toolbar.png)\n\n__Figure 4.__ The item toolbar.\n{: .fig-caption #FigItemToolbar}\n\n\n\n## The tab area\n\nThe tab area is the central part of the window ([Figure 5](#FigTabArea)). The tab area is where item controls, documentation, important messages, etc. are shown. The tab area can contain multiple tabs, and functions much like a tabbed web browser.\n\n\n![/pages/manual/img/interface/tab-area.png](/4.0/pages/manual/img/interface/tab-area.png)\n\n__Figure 5.__ The tab area.\n{: .fig-caption #FigTabArea}\n\n\n\n## The overview area\n\nThe overview area ([Figure 6](#FigOverviewArea)) is (by default) shown at the left of the window, to the right of the item toolbar. The overview area shows the structure of your experiment as a tree. You can re-order the items in your experiment by dragging them from one position to another in the overview area.\n\n- Shortcut to hide/ show: `Ctrl+\\`\n\n\n![/pages/manual/img/interface/overview-area.png](/4.0/pages/manual/img/interface/overview-area.png)\n\n__Figure 6.__ The overview area.\n{: .fig-caption #FigOverviewArea}\n\n\n\n## The file pool\n\nThe file pool ([Figure 7](#FigFilePool)) is (by default) shown at the right of the window. It provides an overview of all files that are bundled with the experiment.\n\n- Shortcut to hide/ show: `Ctrl+P`\n\n\n![/pages/manual/img/interface/file-pool.png](/4.0/pages/manual/img/interface/file-pool.png)\n\n__Figure 7.__ The file pool.\n{: .fig-caption #FigFilePool}\n\n\n\n", "url": "https://osdoc.cogsci.nl/4.0/manual/interface", "title": "Using the interface"}
{"content": "## The debug window\n\nThe debug window ([Figure 8](#FigDebugWindow)) is (by default) shown at the bottom of the window. It provides an [IPython interpreter](https://ipython.org/), and is used as the standard output while an experiment is running. That is, if you use the Python `print()` function, the result will be printed to the debug window.\n\n- Shortcut to hide/ show: `Ctrl+D`\n\n\n![/pages/manual/img/interface/debug-window.png](/4.0/pages/manual/img/interface/debug-window.png)\n\n__Figure 8.__ The debug window.\n{: .fig-caption #FigDebugWindow}\n\n\n\n## The variable inspector\n\nThe variable inspector ([Figure 9](#FigVariableInspector)) is (by default) shown at the right of the window. It provides a list of all variables that are detected in your experiment. When you are running an experiment, the variable inspector also provides a real-time overview of variables and their values.\n\n- Shortcut to hide/ show: `Ctrl+I`\n\n\n![/pages/manual/img/interface/variable-inspector.png](/4.0/pages/manual/img/interface/variable-inspector.png)\n\n__Figure 9.__ The variable inspector.\n{: .fig-caption #FigVariableInspector}\n\n\n\n## Keyboard shortcuts\n\nThe keyboard shortcuts listed below are default values. Many of them can be changed through *Menu \u2192 Tools \u2192 Preferences*.\n\n### General shortcuts\n\nThe following keyboard shortcuts are available everywhere:\n\n- Quick switcher: `Ctrl+Space`\n- Command palette: `Ctrl+Shift+P`\n- New experiment: `Ctrl+N`\n- Open experiment: `Ctrl+O`\n- Save experiment: `Ctrl+S`\n- Save experiment as: `Ctrl+Shift+S`\n- Undo: `Ctrl+Alt+Z`\n- Redo: `Ctrl+Alt+Shift+Z`\n- Run experiment fullscreen: `Ctrl+R`\n- Run experiment in window: `Ctrl+W`\n- Quick-run experiment: `Ctrl+Shift+W`\n- Test experiment in browser: `Alt+Ctrl+W`\n- Show/ hide overview area: `Ctrl+\\`\n- Show/ hide debug window: `Ctrl+D`\n- Show/ hide file pool: `Ctrl+P`\n- Show/ hide variable inspector: `Ctrl+I`\n- Focus overview area: `Ctrl+1`\n- Focus tab area: `Ctrl+2`\n- Focus debug window: `Ctrl+3`\n- Focus file pool: `Ctrl+4`\n- Focus variable inspector: `Ctrl+5`\n\n### Editor shortcuts\n\nThe following keyboard shortcuts are available in editor components, such as the INLINE_SCRIPT:\n\n- (Un)comment selected line(s): `Ctrl+/`\n- Find text: `Ctrl+F`\n- Replace text: `Ctrl+H`\n- Hide find/ replace dialog: `Escape`\n- Duplicate line: `Ctrl+Shift+D`\n- Undo: `Ctrl+Z`\n- Redo: `Ctrl+Shift+Z`\n- Copy: `Ctrl+C`\n- Cut: `Ctrl+X`\n- Paste: `Ctrl+V`\n\n### Tab-area shortcuts\n\nThe following keyboard shortcuts are available in the tab area:\n\n- Next tab: `Ctrl+Tab`\n- Previous tab: `Ctrl+Shift+Tab`\n- Close other tabs: `Ctrl+T`\n- Close all tabs: `Ctrl+Alt+T`\n- Close current tab: `Alt+T`\n\n### Overview-area and sequence shortcuts\n\nThe following keyboard shortcuts are available in the overview area and the SEQUENCE item:\n\n- Context menu: `+`\n- Copy item (unlinked): `Ctrl+C`\n- Copy item (linked): `Ctrl+Shift+C`\n- Paste item: `Ctrl+V`\n- Delete item: `Del`\n- Permanently delete item: `Shift+Del`\n- Rename: `F2`\n- Change run-if statement (if applicable): `F3`", "url": "https://osdoc.cogsci.nl/4.0/manual/interface", "title": "Using the interface"}
{"content": "title: OpenSesame as a Python library (no GUI)\n\nYou can also write experiments fully programmatically by using OpenSesame as a Python module. This is mainly suited for people who prefer coding over using a graphical user interface.\n\nUsing OpenSesame as a Python module works much the same way as using Python `inline_script` items in the user interface, with two notable exceptions:\n\n- Functions and classes need to be explicitly imported from `libopensesame.python_workspace_api`. All functions and classes described under [Common functions](https://osdoc.cogsci.nl/4.0/manual/python/common) are available.\n- An `experiment` object needs to be explicitly created using the `Experiment()` factory function.\n\nA simple Hello World experiment looks like this:\n\n```python\nfrom libopensesame.python_workspace_api import \\\n  Experiment, Canvas, Keyboard, Text\n\n# Initialize the experiment window using the legacy backend\nexp, win, clock, log = Experiment(canvas_backend='legacy')\n# Prepare a stimulus canvas and a keyboard\ncnv = Canvas()\ncnv += Text('Hello world')\nkb = Keyboard()\n# Show the canvas, wait for a key press, and then end the experiment\ncnv.show()\nkb.get_key()\nexp.end()\n```\n\nYou can also programmatically open a `.osexp` experiment file and execute it:\n\n```python\nfrom libopensesame.python_workspace_api import Experiment\nexp, win, clock, log = Experiment(osexp_path='my_experiment.osexp',\n                                  subject_nr=2)\nexp.run()\n```", "url": "https://osdoc.cogsci.nl/4.0/manual/python/nogui", "title": "OpenSesame as a Python library (no GUI)"}
{"content": "The API described here is a Python API for OpenSesame, a software platform used for executing psychology experiments. It consists of various functions which enable data logging for these experiments. The `log` object, which is automatically created when an experiment starts, does not need to be manually imported by the user.\n\nThe available functions and their parameters in the API are as follows:\n\n- `close(self)`: This function is used to close the current log. It does not require any parameters.\n\n- `open(path)`: This function opens the current log. If a log was already open, it will be automatically closed and then reopened. The parameter for this function is:\n  - `path`: This refers to the path to the current logfile. Unless a custom log back-end is being used, this will typically be a filename.\n\n- `write(msg, newline=True)`: This function is used to write a single message to the log. The parameters for this function are:\n  - `msg`: This is a text message that will be written to the log. For Python 2, it should be either `unicode` or a utf-8-encoded `str`. For Python 3, it should be either `str` or a utf-8-encoded `bytes`.\n  - `newline`: This indicates whether a newline should be written after the message.\n\n- `write_vars(var_list=None)`: This function is used to write variables to the log. The parameter for this function is:\n  - `var_list`: This is a list of the variable names that will be written to the log. If `None` is specified, all variables existing in the experiment will be written.", "url": "https://osdoc.cogsci.nl/4.0/manual/python/log", "title": "Log functions"}
{"content": "---\nlayout: osdoc\ntitle: Oculus rift (virtual reality)\ngroup: Devices\npermalink: /oculus-rift/\n---\n\n<iframe src=\"http://wl.figshare.com/articles/1394986/embed?show_title=1\" width=\"640\" height=\"861\" frameborder=\"0\"></iframe>\n\nHern\u00e1ndez-Sande, A., Lorca, J. A. (2015): OpenSesame: An example of stimulus presentation in Virtual Reality headsets (Oculus Rift DK1). *Figshare*. <a href=\"http://dx.doi.org/10.6084/m9.figshare.1394986\">doi:10.6084/m9.figshare.1394986</a>", "url": "https://osdoc.cogsci.nl/4.0/manual/devices/oculusrift", "title": "Oculus rift (virtual reality)"}
{"content": "title: GazePoint / OpenGaze\n\nPyGaze offers *experimental* support for GazePoint eye trackers through the OpenGaze API as of OpenSesame 3.3.11. For more information, see:\n\n- <https://osdoc.cogsci.nl/4.0/manual/eyetracking/pygaze>\n- <https://www.gazept.com/>", "url": "https://osdoc.cogsci.nl/4.0/manual/eyetracking/gazepoint", "title": "GazePoint / OpenGaze"}
{"content": "---\nlayout: osdoc\ntitle: Emotiv EEG\ngroup: Devices\npermalink: /emotiv/\n---\n\n[Emotiv](https://emotiv.com/) is a low-cost EEG headset. Dimitrios Adamos (Neuroinformatics.GRoup of the Aristotle University of Thessaloniki) has written a tutorial for using the Emotiv with OpenSesame:\n\n- <http://neuroinformatics.gr/node/37>\n\n\n![/pages/manual/devices/img/emotiv/emotiv.png](/4.0/pages/manual/devices/img/emotiv/emotiv.png)\n\n__Figure 1.__ Emotiv is a low-cost EEG headset.\n{: .fig-caption #FigEmotiv}\n\n", "url": "https://osdoc.cogsci.nl/4.0/manual/devices/emotiv", "title": "Emotiv EEG"}
{"content": "The documentation provides information about OpenSesame script, which is a simple definitional language designed to define an experiment. It's a simplified language, not a full programming language like Python, and it's interpreted by an OpenSesame runtime environment. The script does not differentiate between types such as strings, integers, etc. \n\nThe documentation explains various statements used in OpenSesame script:\n\n- `define` statement: Starts the definition of an item. \n- `draw` statement: Defines a visual element of a sketchpad or feedback item.\n- `log` statement: Indicates that a variable should be logged.\n- `run` statement: Indicates that an item should be run. \n- `set` statement: Defines single-line variables.\n- `setcycle` statement: Sets a variable only during a specific cycle of a loop.\n- `widget` statement: Adds a widget (buttons, labels, etc.) to a form.\n\nEach statement's usage is explained with parameters and examples. \n\nThe document also explains the general syntax rules and conventions of the script, such as the use of keywords, comments, quotation, item-specific syntax, and path name resolving.", "url": "https://osdoc.cogsci.nl/4.0/manual/opensesame-script", "title": "OpenSesame script"}
{"content": "title: Inline JavaScript\n\nThis page has moved to:\n\n- <https://osdoc.cogsci.nl/4.0/manual/javascript/about>", "url": "https://osdoc.cogsci.nl/4.0/manual/osweb/javascript", "title": "Inline JavaScript"}
{"content": "title: Beginner tutorial: gaze cuing\n\n[TOC]\n\n## About OpenSesame\n\nOpenSesame is a program for easy of development of behavioral experiments for psychology, neuroscience, and experimental economy. For beginners, OpenSesame has a comprehensive graphical, point-and-click interface. For advanced users, OpenSesame supports Python scripting (not covered in this tutorial).\n\nOpenSesame is freely available under the [General Public License v3][gpl].\n\n## About this tutorial\n\nThis tutorial shows how to create a simple but complete psychological experiment using OpenSesame [(Math\u00f4t, Schreij, & Theeuwes, 2012; Math\u00f4t & March, 2022)][references]. You will use mainly the graphical user interface of OpenSesame (i.e., no Python inline coding), although you will make small modifications to the OpenSesame script. This tutorial takes approximately one hour.\n\n## Resources\n\n- __Download__ -- This tutorial assumes that you are running OpenSesame version 4.0.0 or later. To check which version you are running, see the bottom right of the 'Get started' tab (see [Figure 2](#FigGetStarted)). You can download the most recent version of OpenSesame from:\n\t- <https://osdoc.cogsci.nl/4.0/download>\n- __Documentation__ -- A dedicated documentation website can be found at:\n\t- <http://osdoc.cogsci.nl/>\n- __Forum__ -- A support forum can be found at:\n\t- <http://forum.cogsci.nl/>\n\n## The experiment\n\nIn this tutorial, you will create a gaze-cuing experiment as introduced by [Friesen and Kingstone (1998)][references]. In this experiment, a face is presented in the center of the screen ([Figure 1](#FigGazeCuing)). This face looks either to the right or to the left. A target letter (an 'F' or an 'H') is presented to the left or right of the face. A distractor stimulus (the letter 'X') is presented on the other side of the face. The task is to indicate as quickly as possible whether the target letter is an 'F' or an 'H'. In the congruent condition, the face looks at the target. In the incongruent condition, the face looks at the distractor. As you may have guessed, the typical finding is that participant respond faster in the congruent condition than in the incongruent condition, even though the direction of gaze is not predictive of the target location. This shows that our attention is automatically guided by other people's gaze, even in situations where this doesn't serve any purpose. (And even when the face is just a smiley!)\n\n\n![/pages/tutorials/img/beginner/gaze-cuing.png](/4.0/pages/tutorials/img/beginner/gaze-cuing.png)\n\n__Figure 1.__ The gaze-cuing paradigm [(Friesen and Kingstone, 1998)][references] that you will implement in this tutorial. This example depicts a trial in the incongruent condition, because the smiley looks at the distractor (&#39;X&#39;) and not at the target (&#39;F&#39;).\n{: .fig-caption #FigGazeCuing}\n\n\n\nThe experiment consists of a practice and an experimental phase. Visual feedback will be presented after every block of trials. A sound will be played after every incorrect response.\n\n## Experimental design\n\nThis design:\n\n- is *within-subject*, because all participants do all conditions\n- is *fully crossed* (or full-factorial), because all combinations of conditions occur\n- has three factors (or factors):\n    - *gaze side* with two levels (left, right)\n    - *target side* with two levels (left, right)\n    - *target letter* with two levels (F, H)\n- has N subjects\n\n\nSee also [Video 1](#DesignScreencast) for an explanation of the logic and design of the experiment:\n\n\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe id=\"DesignScreencast\" width=\"640\" height=\"360\" src=\"//www.youtube.com/embed/aWvibRH6D4E\" class=\"video youtube embed-responsive-item\" allowfullscreen>\n</iframe>\n<div class='vid-caption'><strong>Video 1.</strong> An explanation of the experimental logic and design.\n</div>\n</div>\n\n\n\n## Step 1: Create the main sequence\n\nWhen you start OpenSesame, you see the 'Get started!' tab ([Figure 2](#FigGetStarted)). A list of templates is shown below 'Start a new experiment'. These templates provide convenient starting points for new experiments. After you saved an experiment the first time, recently opened experiments are shown under 'Continue with a recent experiment'. At the bottom of the page there are links to the documentation (which includes this tutorial), the community forum, and a page with professional (paid) support options. And of course a link where you can buy us a cup of coffee to help us stay awake while we are working on providing the best free software!\n\n\n![/pages/tutorials/img/beginner/get-started.png](/4.0/pages/tutorials/img/beginner/get-started.png)\n\n__Figure 2.__ The &#39;Get started&#39; dialog on OpenSesame start-up.\n{: .fig-caption #FigGetStarted}\n\n\n\nClick on 'Default template' to start with a minimal experimental template.\n\nBy default there is a main SEQUENCE, which is simply called *experiment*. Click on *experiment* in the overview area (by default on the left side, see [Figure 3](#FigInterface)) to open its controls in the tab area. The *experiment* SEQUENCE consists of two items: a `notepad` called *getting started* and a SKETCHPAD called *welcome*.\n\nWe don't need these two items. Remove *getting_started* by right-clicking on it in the overview area and selecting 'Delete' (shortcut: `Del`). Remove *welcome* in the same way. The *experiment* SEQUENCE is now empty.\n\n\n![/pages/tutorials/img/beginner/interface.png](/4.0/pages/tutorials/img/beginner/interface.png)\n\n__Figure 3.__ The default layout of the OpenSesame interface.\n{: .fig-caption #FigInterface}\n\n\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__Names vs types__ -- Items in OpenSesame have a name and a type. The name and type can be the same, but they are usually not. For example, a SKETCHPAD item can have the name *my_target_sketchpad*. To make this distinction clear, we will use `monospace` to indicate item types, and *italics* to indicate names.\n\n__Tip__ -- The 'Extended template' is a good starting point for many experiments. It already contains the basic structure of a trial-based experiment.\n\n__Tip__ -- You can click on the Help icons in the top right of an item's tab to get context-sensitive help.\n\n__Tip__ -- Save (shortcut: `Ctrl+S`) your experiment often! In the unfortunate (and unlikely) event of data loss, you will often be able to recover your work from the back-ups that are created automatically, by default, every 10 minutes (Menu \u2192 Tools \u2192 Open backup folder).\n\n__Tip__ -- Unless you have used 'Permanently delete' (shortcut: `Shift+Del`), deleted items are still available in the 'Unused items' bin, until you select 'Permanently delete unused items' in the 'Unused items' tab. You can re-add deleted items to a SEQUENCE by dragging them out of the 'Unused items' bin to somewhere in your experiment.\n\n__Tip__ -- [Figure 4](#FigExperimentStructure) schematically shows the structure of the experiment that you will create. If you get confused during the tutorial, you can refer to [Figure 4](#FigExperimentStructure) to see where you are.\n\n\n![/pages/tutorials/img/beginner/experiment-structure.png](/4.0/pages/tutorials/img/beginner/experiment-structure.png)\n\n__Figure 4.__ A schematic representation of the structure of the &#39;Gaze cuing&#39; experiment. The item types are in bold face, item names in regular face.\n{: .fig-caption #FigExperimentStructure}\n\n\n\n</div>\n\n__Append a form_text_display item for the instruction display__\n\nAs the name suggests, a `form_text_display` is a form that displays text. We are going to use a `form_text_display` to give instructions to the participant at the beginning of the experiment.\n\nClick on *experiment* in the overview area to open its controls in the tab area. You will see an empty SEQUENCE. Drag a `form_text_display` from the item toolbar (under 'Form', see [Figure 3](#FigInterface)) onto the *experiment* SEQUENCE in the tab area. When you let go, a new `form_text_display` item will be inserted into the SEQUENCE. (We will get back to this in Step 12.)\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__Tip__ -- You can drag items into the overview area and into SEQUENCE tabs.\n\n__Tip__ -- If a drop action is ambiguous, a pop-up menu will ask you what you want to do.\n\n__Tip__ -- A `form_text_display` only shows text. If you require images etc., you can use a SKETCHPAD item. We will meet the SKETCHPAD in Step 5.\n\n</div>\n\n__Append a loop item, containing a new sequence item, for the practice phase__\n\nWe need to append a LOOP item to the *experiment* SEQUENCE. We will use this LOOP for the practice phase of the experiment. Click on the *experiment* SEQUENCE to open its controls in the tab area.\n\nDrag the LOOP item from the item toolbar into the SEQUENCE just the way you added the `form_text_display`. New items are inserted below the item that they are dropped on, so if you drop the new LOOP onto the previously created `form_text_display`, it will appear where you want it: after the `form_text_display`. But don't worry if you drop a new item in the wrong place, because you can always re-order things later.\n\nBy itself, a LOOP does not do anything. A LOOP always needs another item to run. Therefore, you have to fill the new LOOP item with another item. (If you view the loop item, you will also see a warning: 'No item selected'.) Drag a SEQUENCE item from the item toolbar onto the LOOP item. A pop-up menu will appear, asking you whether you want to insert the SEQUENCE after or into the LOOP item. Select 'Insert into new_loop'. (We will get back to this in Step 2.)\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__What is a LOOP item?__ -- A LOOP is an item that adds structure to your experiment. It repeatedly runs another item, typically a SEQUENCE. A LOOP is also the place where you will usually define your independent variables, that is, those variables that you manipulate in your experiment.\n\n__What is a SEQUENCE item?__ -- A SEQUENCE item also adds structure to your experiment. As the name suggests, a SEQUENCE runs multiple other items one after another.\n\n__The LOOP-SEQUENCE structure__ -- You often want to repeat a sequence of events. To do this, you will need a LOOP item that contains a SEQUENCE item. By itself, a SEQUENCE does not repeat. It simply starts with the first item and ends with the last item. By 'wrapping' a LOOP item around the SEQUENCE, you can repeat the SEQUENCE multiple times. For example, a single trial usually corresponds to a single SEQUENCE called *trial_sequence*. A LOOP (often called *block_loop*) around this *trial_sequence* would then constitute a single block of trials. Similarly, but at another level of the experiment, a SEQUENCE (often called *block_sequence*) may contain a single block of trials, followed by a FEEDBACK display. A *practice_phase* LOOP around this 'block' SEQUENCE would then constitute the practice phase of the experiment. This may seem a bit abstract right now, but as you follow this tutorial, you will become familiar with the use of LOOPs and SEQUENCEs.\n\n__Tip__ -- For more information about SEQUENCEs and LOOPs, see:\n\n- <https://osdoc.cogsci.nl/4.0/manual/structure/loop>\n- <https://osdoc.cogsci.nl/4.0/manual/structure/sequence>\n\n</div>\n\n__Append a new form_text_display item for the end-of-practice message__\n\nAfter the practice phase, we want to inform the participant that the real experiment will begin. For this we need another `form_text_display`. Go back to the *experiment* SEQUENCE, and drag a `form_text_display` from the item toolbar onto the LOOP item. The same pop-up menu will appear as before. This time, select 'Insert after new_loop'. (We will get back to this in Step 12.)\n\n<div class='info-box' markdown='1'>\n\n__Tip__ -- Don't worry if you have accidentally changed a LOOP's item to run. You can undo this easily by clicking the 'Undo' button in the toolbar (`Ctrl+Shift+Z`).\n\n</div>\n\n__Append a new loop item, containing the previously created sequence, for the experimental phase__\n\nWe need a LOOP item for the experimental phase, just like for the practice phase. Therefore, drag a LOOP from the item toolbar menu onto *_form_text_display*.\n\nThe newly created LOOP (called *new_loop_1*) is empty, and should be filled with a SEQUENCE, just like the LOOP we created before. However, because the trials of the practice and experimental phase are identical, they can use the same SEQUENCE. Therefore, instead of dragging a new SEQUENCE from the item toolbar, you can re-use the *existing* one (i.e. create a linked copy).\n\nTo do this, right-click on the previously created *new_sequence*, and select 'Copy (linked)'. Now, right-click on *new_loop_1* and select 'Paste'. In the pop-up menu that appears, select 'Insert into new_loop 1'.\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__Tip__ \u2014 There is an important distinction between *linked* and *unlinked* copies. If you create a linked copy of an item, you create another occurrence of the same item. Therefore, if you modify the original item, the linked copy will change as well. In contrast, if you create an unlinked copy of an item, the copy will be initially look identical (except for its name), but you can edit the original without affecting the unlinked copy, and vice versa.\n\n</div>\n\n__Append a new form_text_display item, for the goodbye message__\n\nWhen the experiment is finished, we should say goodbye to the participant. For this we need another `form_text_display` item. Go back to the *experiment* SEQUENCE, and drag a `form_text_display` from the item toolbar onto *new_loop_1*. In the pop-up menu that appears, select 'Insert after new_loop_1'. (We will get back to this in Step 12.)\n\n__Give the new items sensible names__\n\nBy default, new items have names like *new_sequence* and *new_form_text_display_2*. It is good practice to give items sensible names. This makes it much easier to understand the structure of the experiment. If you want, you can also add a description to each item. Item names must consist of alphanumeric characters and/or underscores.\n\n- Select *new_form_text_display* in the overview area, double-click on its label in the top of the tab area and rename the item to *instructions*. (Overview-area shortcut: `F2`)\n- Rename *new_loop* to *practice_loop*.\n- Rename *new_sequence* to *block_sequence*. Because you have re-used this item in *new_loop_1*, the name automatically changes there as well. (This illustrates why it is efficient to create linked copies whenever this is possible.)\n- Rename *new_form_text_display_1* to *end_of_practice*.\n- Rename *new_loop_1* to *experimental_loop*.\n- Rename *new_form_text_display_2* to *end_of_experiment*.\n\n__Give the whole experiment a sensible name__\n\nThe experiment in its entirety also has a title and a description. Click on 'New experiment' in the overview area. You can rename the experiment in the same way as you renamed its items. The title currently is 'New experiment'. Rename the experiment to 'Tutorial: Gaze cuing'. Unlike item names, the experiment title may contain spaces etc.\n\nThe overview area of your experiment now looks like [Figure 5](#FigStep1). This would be a good time to save your experiment (shortcut: `Ctrl+S`).\n\n\n![/pages/tutorials/img/beginner/step1.png](/4.0/pages/tutorials/img/beginner/step1.png)\n\n__Figure 5.__ The overview area at the end of the step 1.\n{: .fig-caption #FigStep1}\n\n\n\n\n## Step 2: Create the block sequence\n\nClick on *block_sequence* in the overview. At the moment this SEQUENCE is empty. We want *block sequence* to consist of a block of trials, followed by a  FEEDBACK display. For this we need to do the following:\n\n__Append a reset_feedback item to reset the feedback variables__\n\nWe don't want our feedback to be confounded by key presses that participants have made during the instruction phase or previous blocks of trials. Therefore, we start each block of trials by resetting the feedback variables. To do this we need a `reset_feedback` item. Grab `reset_feedback` from the item toolbar (under 'Response collection') and drag it onto *block_sequence*.\n\n__Append a new loop, containing a new sequence, for a block of trials__\n\nFor a single trial we need a SEQUENCE. For a block of trials, we need to repeat this SEQUENCE multiple times. Therefore, for a block of trials we need to wrap a LOOP around a SEQUENCE. Drag a LOOP from the item toolbar onto *new_reset_feedback*. Next, drag a SEQUENCE from the item toolbar onto the newly created LOOP, and select 'Insert into new_loop' in the pop-up menu that appears. (We will get back to this in Step 3.)\n\n__Append a feedback item__\n\nAfter every block of trials we want to give feedback to the participant, so that the participant knows how well he/ she is doing. For this we need a FEEDBACK item. Drag a FEEDBACK from the item toolbar onto *new_loop*, and select 'Insert after loop' in the pop-up menu that appears. (We will get back to this in Step 10.)\n\n__Give the new items sensible names__\n\nRename: (See Step 1 if you don't remember how to do this.)\n\n- *new_loop* to *block_loop*\n- *new_sequence* to *trial_sequence*\n- *new_reset_feedback* to *reset_feedback*\n- *new_feedback* to *feedback*\n\nThe overview of your experiment now looks like [Figure 6](#FigStep2). Remember to save your experiment regularly.\n\n\n![/pages/tutorials/img/beginner/step2.png](/4.0/pages/tutorials/img/beginner/step2.png)\n\n__Figure 6.__ The overview area at the end of Step 2.\n{: .fig-caption #FigStep2}\n\n\n\n## Step 3: Fill the block loop with independent variables\n\nAs the name suggests, *block_loop* corresponds to a single block of trials. In the previous step we created the *block_loop*, but we still need to define the independent variables that will be varied within the block. Our experiment has three independent variables:\n\n- __gaze_cue__ can be 'left' or 'right'.\n- __target_pos__ (the position of the target) can be '-300' or '300'. These values reflect the X-coordinate of the target in pixels (0 = center). Using the coordinates directly, rather than 'left' and 'right', will be convenient when we create the target displays (see Step 5).\n- __target_letter__ (the target letter) can be 'F' or 'H'.\n\nTherefore, our experiment has 2 x 2 x 2 = 8 levels. Although 8 levels is not that many (most experiments will have more), we don't need to enter all possible combinations by hand. Click on *block_loop* in the overview to open its tab. Now click on the 'Full-factorial design' button. In the variable wizard, you simply define all variables by typing the name in the first row and the levels in the rows below the name (see [Figure 7](#FigVariableWizard)). If you select 'Ok', you will see that *block_loop* has been filled with all 8 possible combinations.\n\n\n![/pages/tutorials/img/beginner/variable-wizard.png](/4.0/pages/tutorials/img/beginner/variable-wizard.png)\n\n__Figure 7.__ The loop variable wizard in Step 3.\n{: .fig-caption #FigVariableWizard}\n\n\n\nIn the resulting loop table, each row corresponds to one run of *trial_sequence*. Because, in our case, one run of *trial_sequence* corresponds to one trial, each row in our loop table corresponds to one trial. Each column corresponds to one variable, which can have a different value on each trial.\n\nBut we are not done yet. We need to add three more variables: the location of the distractor, the correct response, and the congruency.\n\n- __dist_pos__ -- On the first row of the first empty column, enter 'dist_pos'. This automatically adds a new experimental variable named 'dist_pos'. In the rows below, enter '300' wherever 'target_pos' is -300, and '-300' wherever 'target_pos' is 300. In other words, the target and the distractor should be positioned opposite from each other.\n- __correct_response__ -- Create another variable, in another empty column, with the name 'correct_response'. Set 'correct_response' to 'z' where 'target_letter' is 'F', and to 'm' where 'target_letter' is 'H'. This means that the participant should press the 'z' key if she sees an 'F' and the 'm' key if she sees an 'H'. (Feel free to choose different keys if 'z' and 'm' are awkward on your keyboard layout; for example, 'w' and 'n' are better on AZERTY keyboards.)\n- __congruency__ -- Create another variable with the name 'congruency'. Set 'congruency' to 'congruent' where 'target_pos' is '-300' and 'gaze_cue' is 'left', and where 'target_pos' is '300' and 'gaze_cue' is 'right'. In other words, a trial is congruent if the face looks at the target. Set 'congruency' to 'incronguent' for the trials on which the face looks at the distractor. The 'congruency' variable is not necessary to run the experiment; however, it is useful for analyzing the data later on.\n\nWe need to do one last thing. 'Repeat' is currently set to '1.00'. This means that each cycle will be executed once. So the block now consists of 8 trials, which is a bit short. A reasonable length for a block of trials is 24, so set 'Repeat' to 3.00 (3 repeats x 8 cycles = 24 trials). You don't need to change 'Order', because 'random' is exactly what we want.\n\nThe *block_loop* now looks like [Figure 8](#FigStep3). Remember to save your experiment regularly.\n\n\n![/pages/tutorials/img/beginner/step3.png](/4.0/pages/tutorials/img/beginner/step3.png)\n\n__Figure 8.__ The *block_loop* at the end of Step 3.\n{: .fig-caption #FigStep3}\n\n\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__Tip__ -- You can prepare your loop table in your favorite spreadsheet program and copy-paste it into the LOOP variable table.\n\n__Tip__ -- You can specify your loop table in a separate file (in `.xlsx` or `.csv`) format, and use this file directly. To do so, select 'file' under 'Source'.\n\n__Tip__ -- You can set 'Repeat' to a non-integer number. For example, by setting 'Repeat' to '0.5', only half the trials (randomly selected) are executed.\n\n</div>\n\n## Step 4: Add images and sound files to the file pool\n\nFor our stimuli, we will use images from file. In addition, we will play a sound if the participant makes an error. For this we need a sound file.\n\nYou can download the required files here (in most webbrowsers you can right-click the links and choose 'Save Link As' or a similar option):\n\n- [gaze_neutral.png](/img/beginner-tutorial/gaze_neutral.png)\n- [gaze_left.png](/img/beginner-tutorial/gaze_left.png)\n- [gaze_right.png](/img/beginner-tutorial/gaze_right.png)\n- [incorrect.ogg](/img/beginner-tutorial/incorrect.ogg)\n\nAfter you have downloaded these files (to your desktop, for example), you can add them to the file pool. If the file pool is not already visible (by default on the right side of the window), click on the 'Show file pool' button in the main toolbar (shortcut: `Ctrl+P`). The easiest way to add the four files to the file pool is to drag them from the desktop (or wherever you have downloaded the files to) into the file pool. Alternatively, you can click on the '+' button in the file pool and add files using the file select dialog that appears. The file pool will be automatically saved with your experiment.\n\nYour file pool now looks like [Figure 9](#FigStep4). Remember to save your experiment regularly.\n\n\n![/pages/tutorials/img/beginner/step4.png](/4.0/pages/tutorials/img/beginner/step4.png)\n\n__Figure 9.__ The file pool at the end of Step 4.\n{: .fig-caption #FigStep4}\n\n\n\n", "url": "https://osdoc.cogsci.nl/4.0/tutorials/beginner", "title": "Beginner tutorial: gaze cuing"}
{"content": "## Step 5: Fill the trial sequence with items\n\nA trial in our experiment looks as follows:\n\n1. __Fixation dot__ -- 750 ms, SKETCHPAD item\n2. __Neutral gaze__ -- 750 ms, SKETCHPAD item\n3. __Gaze cue__ -- 500 ms, SKETCHPAD item\n4. __Target__  -- 0 ms, SKETCHPAD item\n5. __Response collection__ \t-- KEYBOARD_RESPONSE item\n6. __Play a sound if response was incorrect__ --  SAMPLER item\n7. __Log response to file__ -- LOGGER item\n\nClick on *trial_sequence* in the overview to open the *trial_sequence* tab. Pick up a SKETCHPAD from the item toolbar and drag it into the *trial_sequence*. Repeat this three more times, so that *trial_sequence* contains four SKETCHPADs. Next, select and append a KEYBOARD_RESPONSE item, a SAMPLER item, and a LOGGER item.\n\nAgain, we will rename the new items, to make sure that the *trial_sequence* is easy to understand. Rename:\n\n- *new_sketchpad* to *fixation_dot*\n- *new_sketchpad_1* to *neutral_gaze*\n- *new_sketchpad_2* to *gaze_cue*\n- *new_sketchpad_3* to *target*\n- *new_keyboard_response* to *keyboard_response*\n- *new_sampler* to *incorrect_sound*\n- *new_logger* to *logger*\n\nBy default, items are always executed, which is indicated by the run-if expression `True`. However, we want to change this for the *incorrect_sound* item, which should only be executed if an error was made. To do this, we need to change the 'Run if' expression to `correct == 0` in the *trial_sequence* tab. This works, because the *keyboard_response* item automatically creates a `correct` variable, which is set to `1` (correct), `0` (incorrect), or `undefined` (this relies on the `correct_response` variable that was defined in Step 3). The double equals sign is Python syntax and indicates that you want to compare whether the two things are equal to each other, in this case whether the variable `correct` is equal to 0. To change a run-if expression, double click on it (shortcut: `F3`).\n\nThe *trial_sequence* now looks like [Figure 10](#FigStep5).\n\n\n![/pages/tutorials/img/beginner/step5.png](/4.0/pages/tutorials/img/beginner/step5.png)\n\n__Figure 10.__ The *trial_sequence* at the end of Step 5.\n{: .fig-caption #FigStep5}\n\n\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__What is a SKETCHPAD item?__ -- A SKETCHPAD is used to present visual stimuli: text, geometric shapes, fixation dots, Gabor patches, etc. You can draw on the SKETCHPAD using the built-in drawing tools.\n\n__What is a KEYBOARD_RESPONSE item?__ -- A KEYBOARD_RESPONSE item collects a single participant's response from the keyboard.\n\n__What is a SAMPLER item?__ -- A SAMPLER item plays a sound from a sound file.\n\n__What is a LOGGER item?__ -- A LOGGER item writes data to the log file. This is very important: If you forget to include a LOGGER item, no data will be logged during the experiment!\n\n__Tip__ -- Variables and conditional \"if\" expressions are very powerful! To learn more about them, see:\n\n- <https://osdoc.cogsci.nl/4.0/manual/variables>\n\n</div>\n\n## Step 6: Draw the sketchpad items\n\nThe SKETCHPAD items that we have created in Step 5 are still blank. It's time to do some drawing!\n\n__Set the background color to white__\n\nClick on *fixation_dot* in the overview area to open its tab. The SKETCHPAD is still dark gray, while the images that we have downloaded have a white background. Oops, we forgot to set the background color of the experiment to white (it is dark gray by default)! Click on 'Tutorial: Gaze cuing' in the overview area to open the 'General properties' tab. Change 'Foreground' to 'black' and 'Background' to 'white'.\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__Tip__ -- For more fine-grained control over colors, you can also use the hexadecimal RGB notation (e.g., `#FF000` for red), use various color spaces, or use the color-picker tool. See also:\n\n- <https://osdoc.cogsci.nl/4.0/manual/python/canvas>\n\n</div>\n\n__Draw the fixation dot__\n\nGo back to the *fixation_dot* by clicking on *fixation_dot* in the overview. Now select the fixation-dot element by clicking on the button with the crosshair. If you move your cursor over the sketchpad, you can see the screen coordinates in the top-right. Set the (foreground) color to 'black'. Click on the center of the screen (0, 0) to draw a central fixation dot.\n\nFinally, change the 'Duration' field from 'keypress' to '745', because we want the fixation dot to be presented for 750 ms. Wait ... *why didn't we just specify a duration of 750 ms?* The reason for this is that the actual display-presentation duration is always rounded up to a value that is compatible with your monitor's refresh rate. This may sound complicated, but for most purposes the following rules of thumb are sufficient:\n\n1. Choose a duration that is possible given your monitor's refresh rate. For example, if your monitor's refresh rate is 60 Hz, it means that every frame lasts 16.7 ms (= 1000 ms/60 Hz). Therefore, on a 60 Hz monitor, you should always select a duration that is a multiple of 16.7 ms, such as 16.7, 33.3, 50, 100, etc.\n2. In the duration field of the SKETCHPAD specify a duration that is a few milliseconds less than what you're aiming for. So if you want to present a SKETCHPAD for 50 ms, choose a duration of 45. If you want to present a SKETCHPAD for 1000 ms, choose a duration of 995. Etcetera.\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__Tip__ -- For a detailed discussion of experimental timing, see:\n\n- <https://osdoc.cogsci.nl/4.0/manual/timing>\n\n__Tip__ -- The duration of a SKETCHPAD can be a value in milliseconds, but you can also enter 'keypress' or 'mouseclick' to collect a keyboard press or mouse click respectively. In this case a SKETCHPAD will work much the same as a KEYBOARD_RESPONSE item (but with fewer options).\n\n__Tip__ -- Make sure that the (foreground) color is set to black. Otherwise you will draw white on white and won't see anything!\n\n</div>\n\n__Draw the neutral gaze__\n\nOpen the *neutral_gaze* SKETCHPAD. Now select the image tool by clicking on the button with the mountain-landscape-like icon. Click on the center of the screen (0, 0). The 'Select file from pool' dialog will appear. Select the file `gaze_neutral.png` and click on the 'Select' button. The neutral gaze image will now stare at you from the center of the screen! Finally, like before, change the 'Duration' field from 'keypress' to '745'. (And note again that this means a duration of 750 ms on most monitors!)\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__Tip__ -- OpenSesame can handle a wide variety of image formats. However, some (non-standard) `.bmp` formats are known to cause trouble. If you find that a `.bmp` image is not shown, you can convert it to a different format, such as `.png`. You can convert images easily with free tools such as [GIMP].\n</div>\n\n__Draw the gaze cue__\n\nOpen the *gaze_cue* SKETCHPAD, and again select the image tool. Click on the center of the screen (0, 0) and select the file `gaze_left.png`.\n\nBut we are not done yet! Because the gaze cue should not always be 'left', but should depend on the variable `gaze_cue`, which we have defined in Step 3. However, by drawing the `gaze_left.png` image to the SKETCHPAD, we have generated a script that needs only a tiny modification to make sure that the proper image is shown. Click on the 'Select view' button at the top-right of the *gaze_cue* tab and select 'View script'. You will now see the script that corresponds to the sketchpad that we have just created:\n\n~~~ .python\nset duration keypress\nset description \"Displays stimuli\"\ndraw image center=1 file=\"gaze_left.png\" scale=1 show_if=True x=0 y=0 z_index=0\n~~~\n\nThe only thing that we need to do is replace `gaze_left.png` with `gaze_{gaze_cue}.png`. This means that OpenSesame uses the variable `gaze_cue` (which has the values `left` and `right`) to determine which image should be shown.\n\nWhile we are at it, we might as well change the duration to '495' (rounded up to 500!). The script now looks like this:\n\n~~~ .python\nset duration 495\nset description \"Displays stimuli\"\ndraw image center=1 file=\"gaze_{gaze_cue}.png\" scale=1 show_if=True x=0 y=0 z_index=0\n~~~\n\nClick the 'Apply' button at the top right to apply your changes to the script and return to the regular item controls. OpenSesame will warn you that the image cannot be shown, because it is defined using variables, and a placeholder image will be shown instead. Don't worry, the correct image will be shown during the experiment!\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__Tip__ -- The variable inspector (shortcut: `Ctrl+I`) is a powerful way to find out which variables have been defined in your experiment, and which values they have (see [Figure 11](#FigVariableInspector)). When your experiment is not running, most variables don't have a value yet. But when you run your experiment in a window, while having the variable inspector visible, you can see variables changing in real time. This is very useful for debugging your experiment.\n\n\n![/pages/tutorials/img/beginner/variable-inspector.png](/4.0/pages/tutorials/img/beginner/variable-inspector.png)\n\n__Figure 11.__ The variable inspector is a convenient way to get an overview of the variables that exist in your experiment.\n{: .fig-caption #FigVariableInspector}\n\n\n\n</div>\n\n__Draw the target__\n\nWe want three objects to be part of the target display: the target letter, the distractor letter, and the gaze cue (see [Figure 1](#FigGazeCuing)). As before, we will start by creating a static display using the SKETCHPAD editor. After this, we will only need to make minor changes to the script so that the exact display depends on the variables.\n\nClick on *target* in the overview to open the target tab and like before, draw the `gaze_left.png` image at the center of the screen. Now select the draw text tool by clicking on the button with the 'A' icon. Change the foreground color to 'black' (if it isn't already). The default font size is 18 px, which is a bit small for our purpose, so change the font size to 32 px. Now click on (-320, 0) in the SKETCHPAD (the X-coordinate does not need to be exactly 320, since we will change this to a variable anyway). Enter \"{target_letter}\" in the dialog that appears, to draw the target letter (when drawing text, you can use variables directly). Similarly, click on (320, 0) and draw an 'X' (the distractor is always an 'X').\n\nNow open the script editor by clicking on the 'Select view' button at the top-right of the tab and selecting 'View script'. The script looks like this:\n\n~~~ .python\nset duration keypress\nset duration keypress\nset description \"Displays stimuli\"\ndraw image center=1 file=\"gaze_left.png\" scale=1 show_if=True x=0 y=0 z_index=0\ndraw textline center=1 color=black font_bold=no font_family=mono font_italic=no font_size=32 html=yes show_if=True text=\"{target_letter}\" x=-320 y=0 z_index=0\ndraw textline center=1 color=black font_bold=no font_family=mono font_italic=no font_size=32 html=yes show_if=True text=X x=320 y=0 z_index=0\n~~~\n\nLike before, change `gaze_left.png` to `gaze_{gaze_cue}.png`. We also need to make the position of the target and the distractor depend on the variables `target_pos` and `dist_pos` respectively. To do this, simply change `-320` to `{target_pos}` and `320` to `{dist_pos}`. Make sure that you leave the `0`, which is the Y-coordinate. The script now looks like this:\n\n~~~ .python\nset duration keypress\nset description \"Displays stimuli\"\ndraw image center=1 file=\"gaze_{gaze_cue}.png\" scale=1 show_if=True x=0 y=0 z_index=0\ndraw textline center=1 color=black font_bold=no font_family=mono font_italic=no font_size=32 html=yes show_if=True text=\"{target_letter}\" x={target_pos} y=0 z_index=0\ndraw textline center=1 color=black font_bold=no font_family=mono font_italic=no font_size=32 html=yes show_if=True text=X x={dist_pos} y=0 z_index=0\n~~~\n\nClick on the 'Apply' button to apply the script and go back to the regular item controls.\n\nFinally, set the 'Duration' field to '0'. This does not mean that the target is presented for only 0 ms, but that the experiment will advance to the next item (the *keyboard_response*) right away. Since the *keyboard_response* waits for a response, but doesn't change what's on the screen, the target will remain visible until a response has been given.\n\nRemember to save your experiment regularly.\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__Tip__ -- Each element of a SKETCHPAD has a 'Show if' option, which specifies when the element should be shown. You can use this to hide/ show elements from a SKETCHPAD depending on certain variables, similar to run-if statements in a SEQUENCE.\n\n__Tip__ -- Make sure that the (foreground) color is set to black. Otherwise you will draw white on white and won't see anything!\n\n</div>\n\n", "url": "https://osdoc.cogsci.nl/4.0/tutorials/beginner", "title": "Beginner tutorial: gaze cuing"}
{"content": "## Step 7: Configure the keyboard response item\n\nClick on *keyboard_response* in the overview to open its tab. You see three options: Correct response, Allowed responses, Timeout, and Event type.\n\nWe have already set the `correct_response` variable in Step 3. Unless we explicitly specify a correct response, OpenSesame automatically uses the `correct_response` variable if it is available. Therefore, we don't need to change the 'Correct response' field here.\n\nWe do need to set the allowed responses. Enter 'z;m' in the allowed-responses field (or other keys if you have chosen different response keys). The semicolon is used to separate responses. The KEYBOARD_RESPONSE now only accepts 'z' and 'm' keys. All other key presses are ignored, with the exception of 'escape', which pauses the experiment.\n\nWe also want to set a timeout, which is the maximum interval that the KEYBOARD_RESPONSE waits before deciding that the response is incorrect and setting the 'response' variable to 'None'. '2000' (ms) is a good value.\n\nWe don't need to change the Event type, because we want the participant to respond by pressing a key (keypress, the default) and not by releasing a key (keyrelease).\n\nThe KEYBOARD_RESPONSE now looks like [Figure 12](#FigStep7).\n\n\n![/pages/tutorials/img/beginner/step7.png](/4.0/pages/tutorials/img/beginner/step7.png)\n\n__Figure 12.__ The KEYBOARD_RESPONSE at the end of Step 7.\n{: .fig-caption #FigStep7}\n\n\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__Tip__ -- By default, the KEYBOARD_RESPONSE will use the `correct_response` variable to determine whether a response was correct. But you can use a different variable as well. To do this, enter a variable name between curly braces (`{my_variable}`) in the correct response field.\n\n__Tip__ -- If 'flush pending key presses' is enabled (it is by default), all pending key presses are discarded when the KEYBOARD_RESPONSE item is called. This prevents carry-over effects, which might otherwise occur if the participant accidentally presses a key during a non-response part of the trial.\n\n__Tip__ -- To use special keys, such as '/' or the up-arrow key, you can use key names (e.g., 'up' and 'space') or associated characters (e.g., '/' and ']'). The 'List available keys' button provides an overview of all valid key names.\n\n</div>\n\n## Step 8: Configure the incorrect (sampler) item\n\nThe *incorrect_sound* item doesn't need much work: We only need to select the sound that should be played. Click on *incorrect_sound* in the overview to open its tab. Click on the 'Browse' button and select `incorrect.ogg` from the file pool.\n\nThe sampler now looks like [Figure 13](#FigStep8).\n\n\n![/pages/tutorials/img/beginner/step8.png](/4.0/pages/tutorials/img/beginner/step8.png)\n\n__Figure 13.__ The *incorrect_sound* item at the end of Step 8.\n{: .fig-caption #FigStep8}\n\n\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__Tip__ -- You can use variables to specify which sound should be played by using a variable name between curly braces as (part of) the file name. For example: `{a_word}.ogg`\n\n__Tip__ -- The SAMPLER handles files in `.ogg`, `.mp3`, and `.wav` format. If you have sound files in a different format, [Audacity] is a great free tool to convert sound files (and much more).\n\n</div>\n\n## Step 9: Configure the variable logger\n\nActually, we don't need to configure the variable LOGGER, but let's take a look at it anyway. Click on *logger* in the overview to open its tab. You see that the option 'Automatically log all variables' is selected. This means that OpenSesame logs everything, which is fine.\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__Tip__ -- If you like your log-files clean, you can disable the 'Automatically log all variables' option and manually select variables, either by entering variable names manually ('Add custom variable'), or by dragging variables from the variable inspector into the LOGGER table. You can also leave the 'Automatically log all variables' option enabled and exclude variables that you are not interested in.\n\n__The one tip to rule them all__ -- Always triple-check whether all the necessary variables are logged in your experiment! The best way to check this is to run the experiment and investigate the resulting log files.\n\n</div>\n\n## Step 10: Draw the feedback item\n\nAfter every block of trials, we want to present feedback to the participant to let him/ her know how well he/ she is doing. Therefore, in Step 2, we added a FEEDBACK item, simply named *feedback* to the end of *block_sequence*.\n\nClick on *feedback* in the overview to open its tab, select the draw text tool, change the foreground color to 'black' (if it isn't already), and click at (0, 0). Now enter the following text:\n\n```text\nEnd of block\n\nYour average response time was {avg_rt} ms\nYour accuracy was {acc} %\n\nPress any key to continue\n```\n\nBecause we want the feedback item to remain visible as long as the participant wants (i.e. until he/ she presses a key), we leave 'Duration' field set to 'keypress'.\n\nThe feedback item now looks like [Figure 14](#FigStep_10).\n\n\n![/pages/tutorials/img/beginner/step10.png](/4.0/pages/tutorials/img/beginner/step10.png)\n\n__Figure 14.__ The feedback item at the end of Step 10.\n{: .fig-caption #FigStep_10}\n\n\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__What is a feedback item?__ -- A FEEDBACK item is almost identical to a SKETCHPAD item. The only difference is that a FEEDBACK item is not prepared in advance. This means that you can use it to present feedback, which requires up-to-date information about a participant's response. You should not use FEEDBACK items to present time-critical displays, because the fact that it is not prepared in advance means that its timing properties are not as good as that of the SKETCHPAD item. See also:\n\n- <https://osdoc.cogsci.nl/4.0/manual/stimuli/visual>\n\n__Feedback and variables__ -- Response items automatically keep track of the accuracy and average response time of the participant in the variables 'acc' (synonym: 'accuracy') and 'avg_rt' (synonym: 'average_response_time') respectively. See also:\n\n- <https://osdoc.cogsci.nl/4.0/manual/variables>\n\n__Tip__ -- Make sure that the (foreground) color is set to black. Otherwise you will draw white on white and won't see anything!\n\n</div>\n\n", "url": "https://osdoc.cogsci.nl/4.0/tutorials/beginner", "title": "Beginner tutorial: gaze cuing"}
{"content": "## Step 11: Set the length of the practice phase and experimental phase\n\nWe have previously created the *practice_loop* and *experiment_loop* items, which both call *block_sequence* (i.e., a block of trials). However, right now they call *block_sequence* only once, which means that both the practice and the experimental phase consist of only a single block of trials.\n\nClick on *practice_loop* to open its tab and set 'Repeat' to '2.00'. This means that the practice phase consists of two blocks.\n\nClick on *experimental_loop* to open its tab and set 'Repeat' to '8.00'. This means that the experimental phase consists of eight blocks.\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__Tip__ -- You can create a variable `practice` in both *practice_loop* and *experimental_loop* and set it to 'yes' and 'no' respectively. This is an easy way of keeping track of which trials were part of the practice phase.\n\n</div>\n\n## Step 12: Write the instruction, end_of_practice and end_of_experiment forms\n\nI think you can handle this step your own! Simply open the appropriate items and add some text to present instructions, an end-of-practice message, and an end-of-experiment message.\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__Tip__ -- You can use a subset of HTML tags to format your text. For example, *&lt;b&gt;this will be bold&lt;b&gt;* and *&lt;span color='red'&gt;this will be red&lt;span&gt;*. For more information, see:\n\n- <https://osdoc.cogsci.nl/4.0/manual/stimuli/text>\n\n</div>\n\n## Step 13: Run the experiment!\n\nYou're done! Click on the 'Run in window' (shortcut: `Ctrl+W`) or 'Run fullscreen' (shortcut: `Ctrl+R`) buttons in the toolbar to run your experiment.\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__Tip__ -- A test run is executed even faster by clicking the orange 'Run in window' button (shortcut: `Ctrl+Shift+W`), which doesn't ask you how to save the logfile (and should therefore only be used for testing purposes).\n\n</div>\n\n\n", "url": "https://osdoc.cogsci.nl/4.0/tutorials/beginner", "title": "Beginner tutorial: gaze cuing"}
{"content": "## Understanding errors\n\nBeing able to understand error messages is a crucial skill when working with OpenSeame. After all, a newly built experiment rarely runs immediately without any errors!\n\nLet's say that we made a mistake during one of the steps above. When trying to run the experiment, we get the following error message ([Figure 15](#FigErrorMessage)):\n\n\n![/pages/tutorials/img/beginner/error-message.png](/4.0/pages/tutorials/img/beginner/error-message.png)\n\n__Figure 15.__ An error message in OpenSesame.\n{: .fig-caption #FigErrorMessage}\n\n\n\nThe error message starts with a name, in this case `FStringError`, which indicates the general type of error. This is followed by a short explanatory text, in this case 'Failed to evaluate f-string expression in the following text: gaze_{gaze_ceu}.png`. Even without understanding what an f-string is (it's a string that contains Python code between curly braces), it's clear that there is something wrong with the text '{gaze_ceu}.png'.\n\nThe error message also indicates that the error comes from the prepare phase of the *gaze_cue* item.\n\nFinally, the error message indicates what specifically went wrong when evaluating the text 'gaze_{gaze_ceu}.png': the name 'gaze_ceu' is not defined.\n\nWhile reading the error message carefully, the cause and solution probably already came to your mind: we made a simple spelling mistake in the *gaze_cue* item, writing '{gaze_ceu}' instead of '{gaze_cue}'! And this resulted in an error because there is no variable with the name `gaze_ceu`. This can be easily fixed by opening the script of the *gaze_cue* item and fixing the typo.\n\n\n", "url": "https://osdoc.cogsci.nl/4.0/tutorials/beginner", "title": "Beginner tutorial: gaze cuing"}
{"content": "## Finally: Some general considerations regarding timing and backend selection\n\nIn the 'General properties' tab of the experiment (the tab that you open by clicking on the experiment name), you can select a backend. The backend is the layer of software that controls the display, input devices, sound, etc. Most experiments work with all backends, but there are reasons to prefer one backend over the other, mostly related to timing. Currently there are four backends (depending on your system, not all three may be available):\n\n- __psycho__ -- a hardware-accelerated backend based on PsychoPy [(Peirce, 2007)][references]. This is the default.\n- __xpyriment__ -- a hardware-accelerated backend based on Expyriment [(Krause & Lindeman, 2013)][references]\n- __legacy__ -- a 'safe' backend, based on PyGame. It provides reliable performance on most platforms, but, due to a lack of hardware acceleration, its timing properties are not as good as those of the other backends.\n- __osweb__ -- runs experiments in a browser [(Math\u00f4t & March, 2022)][references].\n\nSee also:\n\n- <https://osdoc.cogsci.nl/4.0/manual/backends>\n- <https://osdoc.cogsci.nl/4.0/manual/timing>\n\n\n## References\n\n<div class='reference' markdown='1'>\n\nBrand, A., & Bradley, M. T. (2011). Assessing the effects of technical variance on the statistical outcomes of web experiments measuring response times. *Social Science Computer Review*. <a href=\"http://dx.doi.org/10.1177/0894439311415604\">doi:10.1177/0894439311415604</a>\n\nDamian, M. F. (2010). Does variability in human performance outweigh imprecision in response devices such as computer keyboards? *Behavior Research Methods*, *42*, 205-211. <a href=\"http://dx.doi.org/10.3758/BRM.42.1.205\">doi:10.3758/BRM.42.1.205</a>\n\nFriesen, C. K., & Kingstone, A. (1998). The eyes have it! Reflexive orienting is triggered by nonpredictive gaze. *Psychonomic Bulletin & Review*, *5*, 490\u2013495. <a href=\"http://dx.doi.org/10.3758/BF03208827\">doi:10.3758/BF03208827</a>\n\nKrause, F., & Lindemann, O. (2013). Expyriment: A Python library for cognitive and neuroscientific experiments. *Behavior Research Methods*. <a href=\"http://dx.doi.org/10.3758/s13428-013-0390-6\">doi:10.3758/s13428-013-0390-6</a>\n\nMath\u00f4t, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences. *Behavior Research Methods*, *44*(2), 314-324. <a href=\"http://dx.doi.org/10.3758/s13428-011-0168-7\">doi:10.3758/s13428-011-0168-7</a>\n\nMath\u00f4t, S., & March, J. (2022). Conducting linguistic experiments online with OpenSesame and OSWeb. *Language Learning*. <a href=\"http://dx.doi.org/10.1111/lang.12509\">doi:10.1111/lang.12509</a>\n\nPeirce, J. W. (2007). PsychoPy: Psychophysics software in Python. *Journal of Neuroscience Methods*, *162*(1-2), 8-13. <a href=\"http://dx.doi.org/10.1016/j.jneumeth.2006.11.017\">doi:10.1016/j.jneumeth.2006.11.017</a>\n\nUlrich, R., & Giray, M. (1989). Time resolution of clocks: Effects on reaction time measurement\u2014Good news for bad clocks. *British Journal of Mathematical and Statistical Psychology*, *42*(1), 1-12. <a href=\"http://dx.doi.org/10.1111/j.2044-8317.1989.tb01111.x\">doi:10.1111/j.2044-8317.1989.tb01111.x</a>\n\n</div>\n\n[references]: #references\n[gpl]: http://www.gnu.org/licenses/gpl-3.0.en.html\n[gimp]: http://www.gimp.org/\n[audacity]: http://audacity.sourceforge.net/\n[python inline scripting]: /python/about", "url": "https://osdoc.cogsci.nl/4.0/tutorials/beginner", "title": "Beginner tutorial: gaze cuing"}
{"content": "This documentation is about the LOOP item in OpenSesame, an open-source tool for creating experiments. The LOOP item serves two main functions: it runs another item multiple times, and it is where you usually define your independent variables for your experiment. \n\nA LOOP is always connected to a single other item that you want to run multiple times. This is usually a SEQUENCE, which runs multiple items in sequence. The LOOP item also allows you to define independent variables through a loop table, where each column represents a variable and each row represents a cycle. \n\nYou can also read independent variables from a file, set the order of cycles, specify a break-if expression to stop the loop before all cycles have been executed, generate a full-factorial design, and add constraints for pseudorandomization. \n\nAdvanced loop operations include the `fullfactorial` instruction that treats the loop table as the input for a full-factorial design, `shuffle` that randomizes the entire table or a specific column, `slice` that selects a portion from the loop, `sort` and `sortby` that sort a column or the entire table, `reverse` that reverses the order of the table or a column, `roll` that shifts the entire table or a column forward or backward, and `weight` that repeats each row by a weighting value specified in a column. \n\nThe LOOP item enables you to preview the loop table to check that the result is as expected. Lastly, the original and transformed LOOP tables, as well as the current row, can be accessed in Python inline script. You can also programmatically define the LOOP table in the Prepare phase of an INLINE_SCRIPT that is before the LOOP.", "url": "https://osdoc.cogsci.nl/4.0/manual/structure/loop", "title": "Looping and independent variables"}
{"content": "title: Advanced_delay\n\nThe `advanced_delay` plug-in delays the experiment for a pre-specified average duration plus a random margin.\n\n- *Duration* is the average duration of the delay in milliseconds.\n- *Jitter* is the size of the variation in the delay in milliseconds.\n- *Jitter mode* is the how the jitter is calculated:\n\t- *Standard deviation* will draw the variation from a Gaussian distribution with Jitter as the standard deviation.\n\t- *Uniform* will draw the variation in duration from a uniform distribution.", "url": "https://osdoc.cogsci.nl/4.0/items/advanced_delay", "title": "Advanced_delay"}
{"content": "The documentation provides information on the use of variables in OpenSesame, a software for designing and conducting psychological experiments. \n\nExperimental variables in OpenSesame can be referred to in the user interface using '{variable_name}' syntax, and are also available as global variables in Python INLINE_SCRIPT or JavaScript INLINE_JAVASCRIPT. They contain values defined in a LOOP item, collected responses, and various properties of the experiment.\n\nThe variable inspector (Ctrl+I) offers an overview of available variables. When the experiment is running, it shows a live overview of variables and their values, which is useful for debugging.\n\nVariables can be defined through the LOOP item. Built-in variables that are always available include experiment variables, item variables, response variables, and feedback variables. Variables can be used throughout the user interface by replacing a value with a variable using the '{variable name}' notation.\n\nOpenSesame also allows the use of Python or JavaScript expressions in the user interface using f-strings or template literals, respectively. In an INLINE_SCRIPT, experimental variables are available as global variables.\n\nConditional statements, or 'if statements', are used to specify conditions under which a particular element is executed. They are regular Python expressions and can be used in 'Run if' statements or 'Show if' fields in SEQUENCE, SKETCHPAD, or FEEDBACK items.\n", "url": "https://osdoc.cogsci.nl/4.0/manual/variables", "title": "Variables"}
{"content": "title: Python-like iterators (pythonic)\n\nThe `pythonic` library provides Python-like functions for iterating over arrays. Available functions are: `range()`, `enumerate()`, `items()`, `zip()`, and `zipLongest()`.\n\n__Example:__\n\nDraw a five by five grid of incrementing numbers:\n\n```js\nlet positions = xy_grid(5, 50)\nconst cnv = Canvas()\nfor (const [i, [x, y]] of enumerate(positions)) {\n    cnv.text({text: i, x: x, y: y})\n}\ncnv.show()\n```\n\nFor an overview, see:\n\n- <https://www.npmjs.com/package/pythonic>", "url": "https://osdoc.cogsci.nl/4.0/manual/javascript/pythonic", "title": "Python-like iterators (pythonic)"}
{"content": "title: Questionnaires in OSWeb\n\n\n## Forms and custom HTML\n\nForms and custom HTML are supported as of OSWeb 1.4\n{:.page-notification}\n\nYou can use the form plugins as described here:\n\n- <https://osdoc.cogsci.nl/4.0/manual/forms/about>\n\nThe FORM_BASE plugin is *not* supported in OSWeb. Instead, you can use the INLINE_HTML item to implement custom HTML forms, as described here:\n\n- <https://osdoc.cogsci.nl/4.0/manual/forms/html>\n\n\n## Linking to a different platform\n\nAs an alternative, you can implement a questionnaire using another platform, such as [LimeSurvey](https://www.limesurvey.org/), and then link to this questionnaire from your OSWeb experiment. The video below shows how to do this in such a way that you can tell afterwards which questionnaire data belongs to which OSWeb data.\n\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe id=\"BeginnerTutorial\" width=\"640\" height=\"360\" src=\"//www.youtube.com/embed/1WvTUQr0JL0\" class=\"video youtube embed-responsive-item\" allowfullscreen>\n</iframe>\n<div class='vid-caption'><strong>Video 1.</strong> Combining OSWeb and LimeSurvey.\n</div>\n</div>\n", "url": "https://osdoc.cogsci.nl/4.0/manual/osweb/questionnaires", "title": "Questionnaires in OSWeb"}
{"content": "title: Repeat_cycle\n\nThis plug-in allows you to repeat cycles from a `loop`. Most commonly, this will be to repeat a trial when a participant made a mistake or was too slow.\n\nFor example, to repeat all trials on which a response was slower than 3000 ms, you can add a `repeat_cycle` item after (typically) the `keyboard_response` and add the following repeat-if expression:\n\n```bash\nresponse_time > 3000\n```\n\nYou can also force a cycle to be repeated by setting the variable `repeat_cycle` to 1 in an `inline_script`, like so:\n\n```python\nrepeat_cycle = 1\n```", "url": "https://osdoc.cogsci.nl/4.0/items/repeat_cycle", "title": "Repeat_cycle"}
{"content": "title: Running experiments online with OSWeb\n\n\n[TOC]\n\n\n## The workflow\n\nFor an introduction to the workflow, see also:\n\nMath\u00f4t, S., & March, J. (2022). Conducting linguistic experiments online with OpenSesame and OSWeb. *Language Learning*. <a href=\"http://dx.doi.org/10.1111/lang.12509\">doi:10.1111/lang.12509</a>\n<br /><small>[Related preprint (not identical to published manuscript)](https://doi.org/10.31234/osf.io/wnryc)</small>\n\n\n### Developing your experiment\n\nFirst, you develop your experiment as you ordinarily would, using the OpenSesame desktop application. Not all functionality is available in online experiments. Notably, you cannot use Python INLINE_SCRIPT items, but have to use JavaScript INLINE_JAVASCRIPT items instead. During the development of your experiment, it is therefore important to check that your experiment is compatible with OSWeb.\n\n- <https://osdoc.cogsci.nl/4.0/manual/osweb/osweb>\n- <https://osdoc.cogsci.nl/4.0/manual/javascript/about>\n\n\n### Uploading your experiment to JATOS\n\nOnce you have developed your experiment, you publish it to JATOS. JATOS is a web server that manages experiments: it allows you to generate links that you can distribute participants, and it stores data that has been collected.\n\nThere is not a single JATOS server. Rather, many institutions maintain their own JATOS server. In addition, <https://mindprobe.eu> is a free JATOS server, sponsored by ESCoP and OpenSesame.\n\n- <https://osdoc.cogsci.nl/4.0/manual/osweb/jatos>\n\n\n### Collecting data\n\nOne you have published your experiment to JATOS, you can start collecting data. You can do this by manually sending links to participants, for example through email. Or you can use a platform for participant recruitment, such as Prolific, Mechanical Turk, or Sona Systems.\n\n- <https://osdoc.cogsci.nl/4.0/manual/osweb/prolific>\n- <https://osdoc.cogsci.nl/4.0/manual/osweb/mturk>\n- <https://osdoc.cogsci.nl/4.0/manual/osweb/sonasystems>\n\n\n### Analyzing data\n\nOnce data collection is finished, you can download the data from JATOS and convert it to `.xlsx` or `.csv` format for further analysis:\n\n- <https://osdoc.cogsci.nl/4.0/manual/osweb/data>\n\n\n## Tutorials\n\n- <https://osdoc.cogsci.nl/4.0/tutorials/intermediate-javascript>\n- <https://osdoc.cogsci.nl/4.0/tutorials/wcst>", "url": "https://osdoc.cogsci.nl/4.0/manual/osweb/workflow", "title": "Running experiments online with OSWeb"}
{"content": "title: Joystick and gamepad\n\nJoysticks and gamepads are supported through the JOYSTICK plugin.\n\n[TOC]\n\n<div class=\"ClassDoc YAMLDoc\" markdown=\"1\">\n\n# instance __joystick__\n\nIf you insert the JOYSTICK plugin at the start of your experiment, a\nJOYSTICK object automatically becomes part of the experiment object\nand can be used within an INLINE_SCRIPT item as `joystick`.\n\n\n\n\n[TOC]\n\n## flush(self)\n\nClears all pending input, not limited to the joystick.\n\n\n\n__Returns__\n\n- True if joyinput was pending (i.e., if there was something to\nflush) and False otherwise.\n\n\n## get_joyaxes(timeout=None)\n\nWaits for joystick axes movement.\n\n\n__Parameters__\n\n- **timeout**: A timeout value in milliseconds or `None` to use default timeout.\n\n__Returns__\n\n- A `(position, timestamp)` tuple. `position` is `None` if a timeout\noccurs. Otherwise, `position` is an `(x, y, z)` tuple.\n\n\n## get_joyballs(timeout=None)\n\nWaits for joystick trackball movement.\n\n\n__Parameters__\n\n- **timeout**: A timeout value in milliseconds or `None` to use default timeout.\n\n__Returns__\n\n- A `(position, timestamp)` tuple. The position is `None` if a\ntimeout occurs.\n\n\n## get_joybutton(joybuttonlist=None, timeout=None)\n\nCollects joystick button input.\n\n\n__Parameters__\n\n- **joybuttonlist**: A list of buttons that are accepted or `None` to default\njoybuttonlist.\n- **timeout**: A timeout value in milliseconds or `None` to use default timeout.\n\n__Returns__\n\n- A (joybutton, timestamp) tuple. The joybutton is `None` if a\ntimeout occurs.\n\n\n## get_joyhats(timeout=None)\n\nWaits for joystick hat movement.\n\n\n__Parameters__\n\n- **timeout**: A timeout value in milliseconds or `None` to use default timeout.\n\n__Returns__\n\n- A `(position, timestamp)` tuple. `position` is `None` if a timeout\noccurs. Otherwise, `position` is an `(x, y)` tuple.\n\n\n## get_joyinput(joybuttonlist=None, timeout=None)\n\nWaits for any joystick input (buttons, axes, hats or balls).\n\n\n__Parameters__\n\n- **joybuttonlist**: A list of buttons that are accepted or `None` to default\njoybuttonlist.\n- **timeout**: A timeout value in milliseconds or `None` to use default timeout.\n\n__Returns__\n\n- A (event, value, timestamp) tuple. The value is `None` if a timeout\noccurs. `event` is one of `None`, 'joybuttonpress',\n'joyballmotion', 'joyaxismotion', or 'joyhatmotion'\n\n\n## input_options(self)\n\nGenerates a list with the number of available buttons, axes, balls\nand hats.\n\n\n\n__Returns__\n\n- A list with number of inputs as: [buttons, axes, balls,\nhats].\n\n\n## set_joybuttonlist(joybuttonlist=None)\n\nSets a list of accepted buttons.\n\n\n__Parameters__\n\n- **joybuttonlist**: A list of buttons that are accepted or `None` to accept all buttons.\n\n\n## set_timeout(timeout=None)\n\nSets a timeout.\n\n\n__Parameters__\n\n- **timeout**: A timeout value in milliseconds or `None` for no timeout.\n\n\n</div>\n\n", "url": "https://osdoc.cogsci.nl/4.0/manual/response/joystick", "title": "Joystick and gamepad"}
{"content": "The API in reference is the Python API for OpenSesame, a software implemented for conducting psychology experiments. The API revolves around the `Mouse` class which is used to collect mouse input. The `Mouse` class does not need to be imported explicitly; its object can be created using the `Mouse()` factory function.\n\nThe initialization of a Mouse can be achieved in the following way:\n\n```python\nmy_mouse = Mouse()\n```\n\nYou can pass optional Response keywords to `Mouse()` to set the default behavior. For example:\n\n```python\nmy_mouse = Mouse(timeout=2000)\n```\n\nThe `**resp_args` is used in functions to accept keyword arguments including `timeout`, `buttonlist`, and `visible`.\n\nButton names/numbers are as follows:\n\n1. Left button\n2. Middle button\n3. Right button\n4. Scroll up\n5. Scroll down\n\nThe coordinates are relative to the center of the display. That is, (0,0) is the center.\n\nThe API provides several functions:\n\n- `flush()`: Clears all pending input. Returns True if a button was clicked and False otherwise.\n- `get_click()`: Collects a mouse click. It returns a tuple (button, position, timestamp) and accepts optional response keywords.\n- `get_click_release()`: Collects a mouse-click release. It returns a tuple (button, position, timestamp) and accepts optional response keywords.\n- `get_pos()`: Returns the current position of the cursor as a tuple (position, timestamp).\n- `get_pressed()`: Returns the current state of the mouse buttons as a tuple of boolean values.\n- `set_pos(pos=(0, 0))`: Sets the position of the mouse cursor.\n- `show_cursor(show=True)`: Changes the visibility of the mouse cursor.\n- `synonyms(button)`: Gives a list of synonyms for a mouse button.", "url": "https://osdoc.cogsci.nl/4.0/manual/python/mouse", "title": "Mouse functions"}
{"content": "title: Touch response\n\nThe `touch_response` plug-in allows you to work with touch responses (or mouse clicks) in an easy way, by dividing the display into rows and columns. Each response is encoded by a single number, which corresponds to the position counting from left-to-right and top-down. For example, if you have specified 2 columns and 3 rows, the display is divided into the following response regions:\n\n```bash\n1\t2\n3\t4\n5\t6\n```\n\nSimilarly, if you have specified 4 columns and 1 row, the display is sliced horizontally into the following response regions:\n\n```bash\n1\t2\t3\t4\n```", "url": "https://osdoc.cogsci.nl/4.0/items/touch_response", "title": "Touch response"}
{"content": "This documentation is about timing in OpenSesame, a tool used to control experimental timing accurately. It delves into various issues related to timing, provides benchmark results, and offers tips for testing your own system. \n\nOpenSesame offers millisecond precision timing, but it doesn't guarantee accurate timing in every experiment. Users are advised to always check the timing in their experiment to ensure it is as intended, by checking the display timestamps reported by OpenSesame. \n\nOther considerations discussed include understanding your monitor, making the refresh deadline, disabling desktop effects, taking into account stimulus-preparation time, and differences between backends. \n\nTo test the precision and accuracy of timing, the documentation recommends presenting displays of different colors in rapid alternation and checking for horizontal lines running across the monitor. \n\nThe document also provides benchmark results and links to other resources for further reading.", "url": "https://osdoc.cogsci.nl/4.0/manual/timing", "title": "Timing"}
{"content": "The document provides a comprehensive overview of using Python in OpenSesame, a tool to build complex experiments using a graphical user interface (GUI). Python is not supported in online experiments; users are advised to use JavaScript instead. The document offers a link to basic Python tutorials and exercises for beginners.\n\nIt further explains how to use Python within the OpenSesame GUI, clarifying that all Python code is executed in a single workspace, meaning variables and modules defined in one script are accessible everywhere. Users can add Python code to their experiments by adding an INLINE_SCRIPT item. Python can also be used in conditional expressions and text strings.\n\nThe document then outlines several useful functions, objects, and classes, such as the `var` object for accessing experimental variables, the `clock` object for time functions, the `log` object for data logging, the `pool` object for accessing the file pool, and several classes for presenting visual stimuli, collecting responses, and sound playback.\n\nLastly, the document mentions alternative modules for display presentation and response collection, such as `psychopy`, `expyriment`, and `pygame`, depending on the backend users are utilizing. The document includes links to more information and resources throughout.", "url": "https://osdoc.cogsci.nl/4.0/manual/python/about", "title": "About Python"}
{"content": "The API described is a Python API for OpenSesame, a software typically used for implementing psychological experiments. The key component of this API is the `clock` object that provides essential time functions and is automatically created when an experiment commences. Importantly, the `clock` object does not require importing as it is an inherent part of the OpenSesame software.\n\nThe available functions and their parameters include:\n\n- `loop_for(ms, throttle=None, t0=None)`: This function is an iterator that loops for a fixed time. The parameters include:\n  - **ms**: The number of milliseconds to loop for.\n  - **throttle**: A period to sleep for in between each iteration.\n  - **t0**: A starting time. If `None`, the starting time is the moment at which the iteration starts.\n\n- `once_in_a_while(ms=1000)`: This function periodically returns `True`. It is useful for executing code that should only be executed once in a while. The only parameter is **ms**, which designates the minimum waiting period.\n\n- `sleep(ms)`: This function simply puts the program to sleep (pauses) for a period. The only parameter is **ms**, indicating the number of milliseconds to sleep for.\n\n- `time()`: This function provides the current timestamp in milliseconds. The absolute meaning of the timestamp depends on the backend. It has no parameters.", "url": "https://osdoc.cogsci.nl/4.0/manual/python/clock", "title": "Clock functions"}
{"content": "The document provides information about using JavaScript with OpenSesame for creating complex experiments. JavaScript is used for experiments that run in a browser with OSWeb. Running experiments on a desktop requires Python instead of JavaScript. JavaScript support on desktop was removed in OpenSesame 4.0 due to its incomplete support and confusion among users. \n\nJavaScript code can be added via an INLINE_JAVASCRIPT item in OpenSesame's GUI. It consists of two tabs: Prepare and Run phase. It is recommended to construct Canvas objects during the Prepare phase for seamless presentation during the Run phase. However, users can execute any JavaScript code during both phases. \n\nThe document also details certain JavaScript functions and objects such as the `persistent` object and the `vars` object, used to preserve objects across scripts and access experimental variables respectively. The `Canvas` class is used to present visual stimuli. \n\nSeveral JavaScript libraries are included by default in OpenSesame such as random functions, color-conversion functions, CSV functions, and Python-like iterators. Additional libraries can also be included via URLs. \n\nFor debugging, users can print to the console with the `console.log()` command. The output will appear in the OpenSesame console when running on desktop, and in the browser console when running in a browser.", "url": "https://osdoc.cogsci.nl/4.0/manual/javascript/about", "title": "About JavaScript"}
{"content": "title: Examples\n\nExample experiments are included with OpenSesame. A list of curated examples is available through Menu \u2192 Tools \u2192 Example experiments. You can also search for publicly available experiments on the OpenScienceFramework by using 'osexp' as search term.\n\n- <https://osf.io/search/?q=osexp>", "url": "https://osdoc.cogsci.nl/4.0/manual/examples", "title": "Examples"}
{"content": "title: CSV functions (csv-parse)\n\nThe synchronous `parse()` function from the `csv-parse` library is available. This allows you to parse CSV-formatted text, for example from a CSV file in the file pool, into an Object.\n\n__Example:__\n\n```js\nconst conditions = csvParse(\n    pool['attentional-capture-jobs.csv'].data,\n    {columns: true}\n)\nfor (const trial of conditions) {\n    console.log(trial.distractor)\n}\n```\n\nFor an overview, see:\n\n- <https://csv.js.org/parse/api/sync/#sync-api>", "url": "https://osdoc.cogsci.nl/4.0/manual/javascript/csv", "title": "CSV functions (csv-parse)"}
{"content": "The Python API for OpenSesame software is used to implement psychology experiments. One of its main features is the `Keyboard` class, which is used to collect keyboard responses. This class does not need to be imported and can be initialized by using the `Keyboard()` factory function. \n\nThis API allows users to specify key names for keyboard inputs, but it is important to note that these key names may vary between backends and can be identified either by character or name, being case-insensitive. \n\nThe API also utilizes `**resp_args`, which are functions that take keyword arguments such as `timeout` and `keylist`. The `timeout` specifies a timeout value in milliseconds, or it can be set to `None` to disable the timeout. The `keylist` specifies a list of keys that are accepted or can also be set to `None` to accept all keys. \n\nThe `Keyboard` class includes the following functions:\n\n- `flush(self)`: This function clears all pending keyboard input and returns True if a key had been pressed and False if not.\n\n- `get_key(*arglist, **kwdict)`: This function collects a single key press and returns a `(key, timestamp)` tuple. `key` is None if a timeout occurs.\n\n- `get_key_release(*arglist, **kwdict)`: Available from version 3.2.0, this function collects a single key release and returns a `(key, timestamp)` tuple. `key` is None if a timeout occurs. \n\n- `get_mods(self)`: This function returns a list of keyboard moderators (e.g., shift, alt, etc.) that are currently pressed.\n\n- `show_virtual_keyboard(visible=True)`: This function shows or hides a virtual keyboard if this is supported by the back-end.\n\n- `synonyms(key)`: This function gives a list of synonyms for a key, either codes or names.\n\n- `valid_keys(self)`: This function tries to guess which key names are accepted by the back-end and returns a list of valid key names.", "url": "https://osdoc.cogsci.nl/4.0/manual/python/keyboard", "title": "Keyboard functions"}
{"content": "title: Downloading and converting data\n\nAfter collecting data with OSWeb through JATOS, you can download and process this data for analysis. To download, navigate to your study within JATOS, click on 'Results', select all Result entries, and then choose 'Export Results \u2192 JATOS Results Archive' (see [Figure 1](#FigJatosExportResults)).\n\n\n![/pages/manual/osweb/img/data/jatos-export-results.png](/4.0/pages/manual/osweb/img/data/jatos-export-results.png)\n\n__Figure 1.__ Procedure for exporting results collected with OSWeb through JATOS.\n{: .fig-caption #FigJatosExportResults}\n\n\n\nThe downloaded file, typically named in the format `jatos_results_<timestamp>.jzip`, contains various folders and files corresponding to metadata and participant data. This format can be difficult to work with directly for data analysis.\n\nTo simplify data analysis, you can convert this file to a more accessible format like `.csv` or `.xlsx`. This conversion can be easily achieved by using the 'Convert OSWeb results to csv/xlsx' option found in the OSWeb extension.", "url": "https://osdoc.cogsci.nl/4.0/manual/osweb/data", "title": "Downloading and converting data"}
{"content": "title: Mouse responses\n\nMouse responses are collected with the MOUSE_RESPONSE item. The MOUSE_RESPONSE is primarily intended to collect individual mouse clicks. If you want to collect mouse-cursor trajectories, take a look at the MOUSETRAP plugins:\n\n- <https://osdoc.cogsci.nl/4.0/manual/mousetracking>\n\n[TOC]\n\n\n## Response variables\n\nThe MOUSE_RESPONSE sets the standard response variables as described here:\n\n- <https://osdoc.cogsci.nl/4.0/manual/variables>\n\n\n## Mouse-button names\n\nMouse buttons have a number (`1`, etc.) as well as a name (`left_button`, etc.). Both can be used to specify correct and allowed responses, but the `response` variable will be set to a number.\n\n- `left_button` corresponds to `1`\n- `middle_button` corresponds to `2`\n- `right_button` corresponds to `3`\n- `scroll_up` corresponds to `4`\n- `scroll_down` corresponds to `5`\n\n\n## Correct response\n\nThe *Correct response* field indicates which response is considered correct. After a correct response, the `correct` variable is automatically set to 1; after an incorrect response or a timeout (i.e. everything else), `correct` is set to 0; if no correct response is specified, `correct` is set to 'undefined'.\n\nYou can indicate the correct response in three main ways:\n\n- *Leave the field empty.* If you leave the *Correct response* field empty, OpenSesame will automatically check if a variable called `correct_response` has been defined, and, if so, use this variable for the correct response.\n- *Enter a literal value.* You can explicitly enter a response, such as 1. This is only useful if the correct response is fixed.\n- *Enter a variable name.* You can enter a variable, such as '{cr}'. In this case, this variable will be used for the correct response.\n\nNote that the correct response refers to which mouse button was clicked, not to which region of interest was clicked (ROI); see the section below for more information about ROIs.\n\n## Allowed responses\n\nThe *Allowed responses* field indicates a list of allowed responses. All other responses will be ignored, except for 'Escape', which will pause the experiment. The allowed responses should be a semicolon-separated list of responses, such as '1;3' to allow the left and right mouse buttons. To accept all responses, leave the *Allowed responses* field empty.\n\nNote that the allowed responses refer to which mouse button can be clicked, not to which region of interest can be clicked (ROI); see the section below for more information about ROIs.\n\n\n## Timeout\n\nThe *Timeout* field indicates a timeout value in milliseconds, or 'infinite' for no timeout. When a timeout occurs, the following happens:\n\n- `response_time` is set to the timeout value, or rather to the time it takes for a timeout to be registered, which may deviate slightly from the timeout value.\n- `response` is set to 'None'. This means that you can specify 'None' for the correct response a timeout should occur; this can be useful, for example, in a go/no-go task, when the participant should withold a response on no-go trials.\n\n\n## Coordinates and regions of interest (ROIs)\n\nThe `cursor_x` and `cursor_y` variables hold the location of the mouse click.\n\nIf you indicate a linked SKETCHPAD, the variable `cursor_roi` will hold a comma-separated list of names of elements that contain the clicked coordinate. In other words, elements on the SKETCHPAD automatically serve as regions of interest for the mouse click.\n\nIf the correctness of a response depends on which ROI was clicked, you cannot use the `correct_response` variable for this, because this currently refers only to which mouse button was clicked. Instead you need to use a simple script.\n\nIn a Python INLINE_SCRIPT you can do this as follows:\n\n```python\nclicked_rois = cursor_roi.split(';')\ncorrect_roi = 'my_roi'\nif correct_roi in clicked_rois:\n    print('correct!')\n    correct = 1\nelse:\n    print('incorrect!')\n    correct = 0\n```\n\nWith OSWeb using a INLINE_JAVASCRIPT you can do this as follows:\n\n```js\nclicked_rois = cursor_roi.split(';')\ncorrect_roi = 'my_roi'\nif (clicked_rois.includes(correct_roi)) {\n    console.log('correct!')\n    correct = 1\n} else {\n    console.log('incorrect!')\n    correct = 0\n}\n```\n\n\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe id=\"VidMouseROI\" width=\"640\" height=\"360\" src=\"//www.youtube.com/embed/21cgX_zHDiA\" class=\"video youtube embed-responsive-item\" allowfullscreen>\n</iframe>\n<div class='vid-caption'><strong>Video 1.</strong> Collecting mouse clicks and using regions of interest.\n</div>\n</div>\n\n\n## Collecting mouse responses in Python\n\nYou can use the `mouse` object to collect mouse responses in Python:\n\n- <https://osdoc.cogsci.nl/4.0/manual/python/mouse>", "url": "https://osdoc.cogsci.nl/4.0/manual/response/mouse", "title": "Mouse responses"}
{"content": "title: About forms\n\nForms are simple interactive displays that can be used to implement questionnaires, instructions, text input displays, etc. You can use forms in four ways.\n\n- Use the form plugins, such as FORM_TEXT_INPUT, which offer ready-made forms. This is the easiest, but least flexible way of using forms. This works both on the desktop and in a browser.\n\t- <https://osdoc.cogsci.nl/4.0/manual/forms/readymade>\n- Define custom forms using OpenSesame script and the form_base plugin. This offers considerable flexibility, and does not require any real programming skills. This only works on the desktop.\n\t- <https://osdoc.cogsci.nl/4.0/manual/forms/custom>\n- Create custom forms using Python inline script. This offers the most flexibility, but requires some knowledge of Python programming. This only works on the desktop.\n\t- <https://osdoc.cogsci.nl/4.0/manual/forms/custom>\n- Create custom forms using HTML code. This only works when running experiments in a browser with OSWeb.\n\t- <https://osdoc.cogsci.nl/4.0/manual/forms/html>\n\n\n![/pages/manual/forms/img/about/about.png](/4.0/pages/manual/forms/img/about/about.png)\n\n__Figure 1.__ An example form.\n{: .fig-caption #FigAbout}\n\n", "url": "https://osdoc.cogsci.nl/4.0/manual/forms/about", "title": "About forms"}
{"content": "title: Custom HTML forms\n\n\nThe INLINE_HTML item allows you to implement forms using custom HTML.\n\n- The `name` attribute of `input` tags corresponds to an experimental variable. Therefore, the text that is entered into the text input of Example 1 will be stored as the experimental variable `text_response`.\n- For `checkbox` and `radio` elements, you can use the `id` attribute to assign a specific value to the associated experimental variable.\n- You can use the `required` attribute to indicate that a form cannot be submitted before a field has been filled out.\n- The form is closed when the participant clicks on an input of type submit.\n- To include images from the file pool in a custom HTML form, first retrieve the URL to the file, assign it to an experimental variable, and then use this variable as the source for the `<img>` tag (see Example 3).\n\n\nExample 1:\n\nA very basic text input form:\n\n```html\n<input type='text' name='text_response'>\n<input type='submit' value='click here to continue'>\n```\n\nExample 2:\n\nA form with multiple radio buttons:\n\n```html\n<p>Please select your age:</p>\n<input type=\"radio\" id=\"age1\" name=\"age\" value=\"30\" required>\n<label for=\"age1\">0 - 30</label><br>\n<input type=\"radio\" id=\"age2\" name=\"age\" value=\"60\">\n<label for=\"age2\">31 - 60</label><br>  \n<input type=\"radio\" id=\"age3\" name=\"age\" value=\"100\">\n<label for=\"age3\">61 - 100</label><br><br>\n<input type=\"submit\" value=\"Submit\">\n```\n\nExample 3:\n\nYou can include variable references (except within `<script>` tags, where curly braces are simply interpreted as part of JavaScript code):\n\n```html\n<p>You age group is {age}</p>\n<input type='submit' value='ok'>\n```\n\nExample 4:\n\nYou can JavaScript through `<script>` tags. For example, you can get an image from the file pool and assign to an initially empty `<img>` tag like this:\n\n```html\n<img id='capybara'>\n<input type='submit' value='ok'>\n\n<script>\ndocument.getElementById('capybara').src = pool['capybara.png'].data.src\n</script>\n```", "url": "https://osdoc.cogsci.nl/4.0/manual/forms/html", "title": "Custom HTML forms"}
{"content": "This document provides an intermediate tutorial on how to use OpenSesame, a program for developing behavioral experiments in psychology, neuroscience, and experimental economics. The tutorial is focused on creating a visual search experiment using JavaScript and assumes previous experience with OpenSesame and JavaScript.\n\nThe tutorial first introduces OpenSesame, including its features and how to download the latest version. It then provides an overview of the tutorial, detailing the experiment that learners will create. The experiment is a visual-search task where participants have to identify a target object among distractors. The experiment design and structure are explained in detail.\n\nThe tutorial then moves on to guide users through the steps of creating the experiment. This includes defining experimental variables, giving instructions to participants, creating the trial sequence, defining the correct responses, and providing feedback to participants. The tutorial also emphasizes the principles of top-down and defensive programming.\n\nThe tutorial ends by guiding users on how to test the experiment and troubleshoot any issues that may arise. It also provides references for further reading. The document is intended to be a comprehensive guide for intermediate users of OpenSesame", "url": "https://osdoc.cogsci.nl/4.0/tutorials/intermediate-javascript", "title": "Intermediate tutorial (JavaScript): visual search"}
{"content": "title: Integration with the Open Science Framework\n\n[TOC]\n\n## About\n\nThe OpenScienceFramework extension connects OpenSesame to the [Open Science Framework](https://osf.io) (OSF), which is a web platform for sharing, connecting, and streamlining scientific workflows. To use this extension, [you need an OSF account](https://osf.io/login/?sign_up=True).\n\nWith the OpenScienceFramework extension, you can:\n\n- Automatically save your experiment to the OSF\n- Automatically upload data to the OSF\n- Open experiments from the OSF\n- Share your experiment and data with other researchers, by giving them access through the OSF\n\n## Logging in to the OSF\n\nTo log into the OSF:\n\n- Create an account on <https://osf.io>. (You cannot create an account from within OpenSesame.)\n- In OpenSesame, click on the log-in button in the main toolbar, and enter your credentials.\n- Once logged in, you can open the OSF Explorer by clicking on your name where the login button used to be, and selecting *Show explorer*. The explorer will show an overview of all your OSF projects, and all repositories/ cloud services that are linked to your projects.\n\n## Linking an experiment to the OSF\n\nIf you link an experiment to the OSF, each time that you save the experiment in OpenSesame, a new version is also uploaded to the OSF.\n\nTo link an experiment:\n\n- Save the experiment on your computer.\n- Open the OSF explorer and select a folder or repository where you would like your experiment to be stored on the OSF. Right-click on this folder and select *Sync experiment to this folder*. The OSF node to which the experiment is linked will be shown at the top of the explorer.\n- The experiment is then uploaded to the selected location.\n- If you check *Always upload experiment on save*, a new version is automatically saved to OSF on each save; if you don't enable this option, you will be asked every time whether or not you want to do this.\n\nTo unlink an experiment:\n\n- Open the OSF explorer, and click the *Unlink* button next to the *Experiment linked to* link.\n\n## Linking data to the OSF\n\nIf you link data to the OSF, each time that data has been collected (normally after every experimental session), this data is also uploaded to the OSF.\n\nTo link data to the OSF:\n\n- Save the experiment on your computer.\n- Open the OSF explorer, right-click on the folder that you want the data to be uploaded to, and select *Sync data to this folder*. The OSF node that the data is linked to will be shown at the top of the explorer.\n- If you check *Always upload collected data*, data files will be automatically saved to OSF after they have been collected; if you don't enable this option, you will be asked every time whether or not you want to do this.\n\nTo unlink data from the OSF:\n\n- Open the OSF explorer, and click the *Unlink* button next to the *Data stored to* link.\n\n## Opening an experiment stored on the OSF\n\nTo open an experiment from the OSF:\n\n- Open the OSF explorer, and find the experiment.\n- Right-click on the experiment and select *Open experiment*.\n- Save the experiment on your computer.\n\n## Handling non-matching versions\n\nIf you open an experiment on your computer that is linked to the OSF, but differs from the version on the OSF, you will be asked what you want to do:\n\n- Use the version from your computer; or\n- Use the version from the OSF. If you choose to use the version from the OSF, it will be downloaded and overwrite the experiment on your computer.\n\n## Installing the OpenScienceFramework extension\n\nThe OpenScienceFramework extension is installed by default in the Windows package of OpenSesame. If the extension is not installed, you can install it as follows:\n\nFrom PyPi:\n\n~~~\npip install opensesame-extension-osf\n~~~\n\nIn an Anaconda environment\n\n~~~\nconda install -c cogsci opensesame-extension-osf\n~~~\n\nThe source code of the extension is available on GitHub:\n\n- <https://github.com/dschreij/opensesame-extension-osf>\n\nAnd for the `python-qosf` module, which is used by the extension:\n\n- <https://github.com/dschreij/python-qosf>", "url": "https://osdoc.cogsci.nl/4.0/manual/osf", "title": "Integration with the Open Science Framework"}
{"content": "This API provides functionality for the Python-based software, OpenSesame, which is utilised for implementing psychological experiments. The API revolves around the `items` object, which doesn't require explicit importation and is automatically created at the commencement of an experiment. The `items` object provides dictionary-like access to the items, allowing for the programmatic execution of items.\n\nThe available functions in this API include:\n\n- `execute(name)`: Executes both the run and prepare phases of an item, updating the item stack accordingly. The `name` parameter refers to the name of the item.\n\n- `new(_type, name=None, script=None, allow_rename=True)`: Creates a new item. The `_type` parameter refers to the type of item, `name` allows for the naming of the item or the assignment of a unique name based on the item type if left as `None`, `script` is for a definition script or a blank item if left as `None`, and `allow_rename` determines whether OpenSesame can use a different name from the one specified under `name`, to avoid duplication.\n\n- `prepare(name)`: Executes the preparation phase of an item, updating the item stack. The `name` parameter refers to the name of the item.\n\n- `run(name)`: Executes the run phase of an item, updating the item stack. The `name` parameter refers to the name of the item.\n\n- `valid_name(item_type, suggestion=None)`: Generates a unique and valid name that resembles the desired name. The `item_type` parameter refers to the type of item, and `suggestion` allows for the naming of the item or the assignment of a name based on the item type if left as `None`.\n  \nMoreover, the API also supports a range of semantics such as checking for the existence of an item, deleting an item, and iterating through all item names.", "url": "https://osdoc.cogsci.nl/4.0/manual/python/items", "title": "Access items"}
{"content": "title: Doing things in sequence\n\nThe SEQUENCE item has two important functions:\n\n- It runs multiple other items one after another.\n- It determines which items should, and which shouldn't, be run.\n\nSEQUENCEs are run from top to bottom; that is, the item at the top is run first. The order of a SEQUENCE is always sequential.\n\n## Run-if expressions\n\nYou can use run-if expressions to determine whether or not a particular item should be run. For example, if you want a display to be presented only if a participant has made an incorrect response, you can set the run-if expressions for that item to:\n\n```python\ncorrect == 0\n```\n\nIf you leave the run-if expressions empty or enter `True`, the item will always be run. Run-if expressions use the same syntax as other conditional expressions. For more information, see:\n\n- <https://osdoc.cogsci.nl/4.0/manual/variables>\n\nRun-if expressions only affect which items are run, not which items are prepared. Phrased differently, the Prepare phase of all items in a SEQUENCE is always executed, regardless of the run-if expressions. See also:\n\n- <https://osdoc.cogsci.nl/4.0/manual/prepare-run>\n\n\n## Disabling items\n\nTo completely disable an item in a SEQUENCE, right-click on it and select 'Disable'. This is mostly useful during development of your experiment, for example to temporarily bypass the instructions.", "url": "https://osdoc.cogsci.nl/4.0/manual/structure/sequence", "title": "Doing things in sequence"}
{"content": "The documentation is about the use of parallel ports in EEG/ERP studies. These studies frequently use triggers to mark significant event timestamps, which are byte signals sent via the parallel port to the EEG device. The guide discusses two methods of sending these triggers: using the `parallel_port_trigger` plugin or using `dportio.dll` in a Python inline Script. The former is a third-party plugin not maintained by OpenSesame and can be installed from PyPi. The latter approach is Windows exclusive and involves adding an INLINE_SCRIPT at the beginning of the experiment.\n\nThe document also provides instructions on how to access the parallel port on Linux and Windows (XP, Vista, and 7). For Linux, the `parport_pc` module is used and the user needs to provide themselves with permissions. For Windows, the DLPortIO driver is required, and the instructions vary depending on the Windows version. For Windows 7, due to strengthened security systems, a helper program called \"Digital Signature Enforcement Overrider\" (DSEO) is needed to install the DLPortIO driver.\n\nThe documentation concludes with recommendations for running experiments, such as starting with a 'zero' trigger and using the [psycho] or [xpyriment] backends for time-critical experiments. It also includes a troubleshooting section with links to relevant forum topics discussing trigger-related issues.\n", "url": "https://osdoc.cogsci.nl/4.0/manual/devices/parallel", "title": "Parallel port (EEG triggers)"}
{"content": "This API is a Python API for OpenSesame, a software used for implementing psychology experiments. The API provides a set of functions which are commonly used in combination with Canvas functions. Importantly, none of the listed functions need to be imported before use.\n\nThe available functions are:\n\n- **Canvas(auto_prepare=True, \\*\\*style_args)**: Creates a new Canvas object.\n- **Experiment(osexp_path=None, log_path='defaultlog.csv', fullscreen=False, subject_nr=0, \\*\\*kwargs)**: Creates a new Experiment object.\n- **Form(\\*args, \\*\\*kwargs)**: Creates a new Form object.\n- **Keyboard(\\*\\*resp_args)**: Creates a new Keyboard object.\n- **Mouse(\\*\\*resp_args)**: Creates a new Mouse object.\n- **Sampler(src, \\*\\*playback_args)**: Creates a new Sampler object.\n- **Synth(osc='sine', freq=440, length=100, attack=0, decay=5)**: Synthesizes a sound and returns it as a Sampler object.\n- **copy_sketchpad(name)**: Returns a copy of a sketchpad's canvas.\n- **pause()**: Pauses the experiment.\n- **register_cleanup_function(fnc)**: Registers a clean-up function which is executed when the experiment ends.\n- **reset_feedback()**: Resets all feedback variables to their initial state.\n- **set_subject_nr(nr)**: Sets the subject number and parity (even/ odd).\n- **sometimes(p=0.5)**: Returns True with a certain probability.\n- **xy_circle(n, rho, phi0=0, pole=(0, 0))**: Generates a list of points (x,y coordinates) in a circle.\n- **xy_distance(x1, y1, x2, y2)**: Gives the distance between two points.\n- **xy_from_polar(rho, phi, pole=(0, 0))**: Converts polar coordinates (distance, angle) to Cartesian coordinates (x, y).\n- **xy_grid(n, spacing, pole=(0, 0))**: Generates a list of points (x,y coordinates) in a grid.\n- **xy_random(n, width, height, min_dist=0, pole=(0, 0))**: Generates a list of random points (x,y coordinates) with a minimum spacing between each pair of points.\n- **xy_to_polar(x, y, pole=(0, 0))**: Converts Cartesian coordinates (x, y) to polar coordinates (distance, angle).", "url": "https://osdoc.cogsci.nl/4.0/manual/python/common", "title": "Common functions"}
{"content": "title: Sound recording\n\n[TOC]\n\n\n## Audio Low Latency plugins\n\nThe Audio Low Latency plugins, developed by Bob Rosbag, are the recommended way to record sound input. The main goal of this set of plugins is to play and record audio with minimal and predictable latencies to achieve a high accuracy and precision. The `PyAlsaAudio` package which uses the Linux ALSA audio system provided the best results within Python. `PortAudio` and `sounddevice` are cross-platform and work on both Windows as Linux.\n\nThe plugins are not installed by default, but can be installed through pip:\n\n```bash\npip install opensesame-plugin-audio-low-latency\n```\n\nSee also:\n\n- <https://pypi.org/project/opensesame-plugin-audio-low-latency/>\n\n\n## Sound recorder plugins\n\nThe sound recorder plugins, developed by Daniel Schreij, are no longer under active development and are therefore no longer recommended. More information about this set of plugins can be found on previous version of this page:\n\n- <https://osdoc.cogsci.nl/3.2/manual/response/soundrecording/>", "url": "https://osdoc.cogsci.nl/4.0/manual/response/soundrecording", "title": "Sound recording"}
{"content": "This API provides functionality for OpenSesame, a software used for implementing psychology experiments. It's a Python API specifically designed to handle experimental variables within the software. OpenSesame 4.0 introduced an update that made all experimental variables available in the Python workspace, eliminating the need for the `var` object. However, the `var` object still exists for those who prefer to use it. \n\nExperimental variables can be referred to in two ways. The preferred method is to reference them as global variables (e.g., `my_var = 10`). Alternatively, they can be referred to as properties of the `var` object (e.g., `var.my_var = 10`). \n\nThe API provides several functions for managing experimental variables:\n\n- `clear(preserve=[])`: This function clears all experimental variables, but can preserve specific variables if their names are provided in the 'preserve' parameter.\n- `get(var, default=None, _eval=True, valid=None)`: This function retrieves the value of a specified experimental variable. It can also return a default value if the variable doesn't exist, and can evaluate the returned value for variable references.\n- `has(var)`: This function checks if a certain experimental variable exists.\n- `inspect(self)`: This function generates a description of all experimental variables, whether they are currently alive or just hypothetical.\n- `items(self)`: This function returns a list of tuples, each consisting of a variable name and its corresponding value.\n- `set(var, val)`: This function assigns a value to a specified experimental variable.\n- `unset(var)`: This function deletes a specified experimental variable.\n- `vars(self)`: This function returns a list of all experimental variables, although it may not be exhaustive due to variables being stored in multiple places.", "url": "https://osdoc.cogsci.nl/4.0/manual/python/var", "title": "Access experimental variables"}
{"content": "The API described is a JavaScript API for OpenSesame, a software designed for implementing psychology experiments. These functions are commonly used in combination with `Canvas` functions. \n\nThe available functions and their parameters are:\n\n- `reset_feedback()`: This function resets all feedback variables to their initial state. It takes no parameters.\n- `set_subject_nr(nr)`: This function sets the subject number and parity (even/ odd). The parameter `nr` represents the subject number.\n- `sometimes([p])`: This function returns true with a certain probability. The parameter `p` represents the probability of returning true and defaults to .5 if not specified.\n- `xy_from_polar(rho, phi, [pole])`: This function converts polar coordinates (distance, angle) to Cartesian coordinates (x, y). The parameters `rho` and `phi` represent the radial coordinate and the angular coordinate, respectively. The optional parameter `pole` represents the reference point and defaults to [0, 0].\n- `xy_to_polar(x, y, [pole])`: This function converts Cartesian coordinates (x, y) to polar coordinates (distance, angle). The parameters `x` and `y` represent the X and Y coordinates, respectively. The optional parameter `pole` represents the reference point and defaults to [0, 0].\n- `xy_distance(x1, y1, x2, y2)`: This function gives the distance between two points. The parameters `x1` and `y1` represent the x and y coordinates of the first point. The parameters `x2` and `y2` represent the x and y coordinates of the second point.\n- `xy_circle(n, rho, [phi0], [pole])`: This function generates a list of points (x,y coordinates) in a circle. The parameters `n` and `rho` represent the number of x,y coordinates to generate and the radial coordinate of the first point, respectively. The optional parameters `phi0` and `pole` represent the angular coordinate for the first coordinate and the reference point, respectively.\n- `xy_grid(n, spacing, [pole])`: This function generates a list of points (x,y coordinates) in a grid. The parameters `n` and `spacing` represent the number of columns and rows and the spacing between cells, respectively. The optional parameter `pole` represents the reference point.\n- `xy_random(n, width, height, [min_dist], [pole])`: This function generates a list of random points (x,y coordinates) with a minimum spacing between each pair of points. The parameters `n`, `width` and `height` represent the number of points to generate, the width of the field with random points and the height of the field with random points, respectively. The optional parameters `min_dist` and `pole` represent the minimum distance between each point and the reference point, respectively.", "url": "https://osdoc.cogsci.nl/4.0/manual/javascript/common", "title": "Common functions"}
{"content": "title: Button box\n\nThere are many different types of button boxes, and they all work in different ways. Therefore, there is no single OpenSesame item that works with all button boxes. (This is different from keyboards, which are standard devices that all work with the KEYBOARD_RESPONSE item.)\n\nCommon types of button boxes:\n\n- Some button boxes *emulate keypresses*. This is easy, because you can use the normal KEYBOARD_RESPONSE item.\n\t- <https://osdoc.cogsci.nl/4.0/manual/response/keyboard>\n- Some button boxes *emulate a joystick*. This is also easy, because you can use the JOYSTICK plugin.\n\t- <https://osdoc.cogsci.nl/4.0/manual/response/joystick>\n- Some button boxes are compatible with the *Serial Response Box* that is developed by Psychology Software Tools. These button boxes are supported by the SRBOX plugin.\n\t- <https://osdoc.cogsci.nl/4.0/manual/response/srbox>\n- Some button boxes have their own Python libaries. In this case, you should be able to find example scripts of how to use the button box in Python, that is, in an OpenSesame INLINE_SCRIPT item.", "url": "https://osdoc.cogsci.nl/4.0/manual/response/buttonbox", "title": "Button box"}
{"content": "The API being described is a Python API for OpenSesame, a software used for implementing psychology experiments. The API's main functionality revolves around the `Canvas` class and its methods, which are used to present visual stimuli. This API does not require the user to import the `Canvas` and Element classes.\n\nA `Canvas` object is typically created using the `Canvas()` factory function. This can be done with or without passing style keywords to `Canvas()`. \n\nDrawing elements onto the `Canvas` can be done in three ways:\n1. Naming an Element interface (e.g., `my_canvas['name'] = FixDot()`)\n2. Adding an Element interface (e.g., `my_canvas += FixDot()`)\n3. Using the function interface (e.g., `my_canvas.fixdot()`), although this method is not preferred.\n\nThe API also allows for modification of named elements. For example, a line element can be added to the canvas, shown, then its color can be changed and shown again, and finally the line can be deleted.\n\nThe `**style_args` parameter in the API allows users to specify various styling options such as color, filling of shapes, line thickness, font family, font size, and more.\n\nColors can be specified in several ways, including color names, hexadecimal strings, RGB strings, and more. For coordinates, x=0 and y=0 is the center of the display. \n\nThe API provides a wide range of functions for creating different visual stimuli such as arrows, circles, lines, rectangles, text, and more, each with their own parameters. Each function is characterized by a specific set of parameters that control its behavior and appearance on the `Canvas`.", "url": "https://osdoc.cogsci.nl/4.0/manual/python/canvas", "title": "Canvas functions"}
{"content": "title: SMI\n\nPyGaze offers *experimental* support for SMI eye trackers. (SMI no longer exists as a company, but its eye trackers are still used in some labs.) For more information, see:\n\n- <https://osdoc.cogsci.nl/4.0/manual/eyetracking/pygaze>", "url": "https://osdoc.cogsci.nl/4.0/manual/eyetracking/smi", "title": "SMI"}
{"content": "title: Form widgets and keywords\n\n\n[TOC]\n\n\n## Screenshot\n\n\n![/pages/manual/forms/img/widgets/widgets.png](/4.0/pages/manual/forms/img/widgets/widgets.png)\n\n__Figure 1.__ A list of available FORM widgets.\n{: .fig-caption #FigWidgets}\n\n\n\n\n## Widgets and keywords\n\nAll keywords are optional, instead otherwise indicated.\n\n### Form\n\nThe `cols` and `rows` keywords can either be single `int` values, in which case they specify the number of equally sized columns and rows, or lists of `int`, in which case they specify the relative sizes of each column and row. For more information about form geometry, see:\n\n- <https://osdoc.cogsci.nl/4.0/manual/forms/custom>\n\nThe `validator` keyword can be used to validate form input. For more information, see:\n\n- <https://osdoc.cogsci.nl/4.0/manual/forms/validation>\n\n(In OpenSesame script, you do not need to explicitly create a form.)\n\nPython script:\n\n~~~ .python\nform = Form(\n    cols=2, rows=2, spacing=10, margins=(100, 100, 100, 100), theme='gray',\n    timeout=None, clicks=False, validator=None\n)\nbutton = Button(text='Ok!')\nform.set_widget(button, (0, 0))\nform._exec()\n~~~\n\n\n### button / Button\n\nOpenSesame script:\n\n~~~python\nwidget 0 0 1 1 button text=\"Click me!\" center=yes frame=yes var=response\n~~~\n\nPython script:\n\n~~~ .python\nform = Form()\nbutton = Button(text='Click me!', frame=True, center=True, var='response')\nform.set_widget(button, (0, 0))\nform._exec()\n~~~\n\n\n### checkbox / Checkbox\n\nIf a group is specified, checking one checkbox from that group will uncheck all other checkboxes from that group. Checkboxes that are part of a group cannot be unchecked, except by clicking on another checkbox in that group.\n\nThe `group` keyword also affects how variables are stored, as described here:\n\n- <https://osdoc.cogsci.nl/4.0/manual/forms/variables>\n\nOpenSesame script:\n\n~~~python\nwidget 0 0 1 1 checkbox group=group text=\"Option 1\"\nwidget 0 1 1 1 checkbox group=group text=\"Option 2\"\n~~~\n\nPython script:\n\n~~~ .python\nform = Form()\ncheckbox1 = Checkbox(text='Option 1', group='group')\ncheckbox2 = Checkbox(text='Option 2', group='group')\nform.set_widget(checkbox1, (0, 0))\nform.set_widget(checkbox2, (0, 1))\nform._exec()\n~~~\n\n\n### image / ImageWidget\n\nThe Python object is called `ImageWidget` to distinguish it from the `Image` canvas element.\n\nOpenSesame script:\n\n~~~python\n# Only path is a required keyword\nwidget 0 0 1 1 image path=\"my_image.png\" adjust=yes frame=no\n~~~\n\nPython script:\n\n~~~ .python\n# Only path is a required keyword\nform = Form()\nimage = ImageWidget(path=pool['my_image.png'], adjust=True, frame=False)\nform.set_widget(image, (0, 0))\nform._exec()\n~~~\n\n\n### image_button / ImageButton\n\nThe `image_id` keyword is used to identify the image button when it is clicked. If no `image_id` is provided, the path to the image is used as id.\n\nOpenSesame script:\n\n~~~python\n# Only path is a required keyword\nwidget 0 0 1 1 image_button path=\"my_image.png\" adjust=yes frame=no image_id=my_image var=response\n~~~\n\nPython script:\n\n~~~ .python\n# Only path is a required keyword\nform = Form()\nimage_button = ImageButton(\n    path=pool['my_image.png'], adjust=True, frame=False,\n    image_id='my_image', var='response'\n)\nform.set_widget(image_button, (0, 0))\nform._exec()\n~~~\n\n\n### label / Label\n\nOpenSesame script:\n\n~~~python\nwidget 0 0 1 1 label text=\"My text\" frame=no center=yes\n~~~\n\nPython script:\n\n~~~ .python\nform = Form()\nlabel = Label(text='My text', frame=False, center=True)\nform.set_widget(label, (0,0))\nform._exec()\n~~~\n\n\n### rating_scale / RatingScale\n\nThe `nodes` keyword can be an `int` or a semicolon-separated list of labels. If `nodes` is an `int`, it specified the number of (unlabeled) nodes.\n\nThe `default` keyword indicates which node number is selected by default, where the first node is 0.\n\nOpenSesame script:\n\n~~~python\nwidget 0 1 1 1 rating_scale var=response nodes=\"Agree;Don't know;Disagree\" click_accepts=no orientation=horizontal var=response default=0\n~~~\n\nPython script:\n\n~~~ .python\nform = Form()\nrating_scale = RatingScale(\n    nodes=['Agree', u\"Don't know\", 'Disagree'], click_accepts=False,\n    orientation='horizontal', var='response', default=0\n)\nform.set_widget(rating_scale, (0, 0))\nform._exec()\n~~~\n\n\n### text_input / TextInput\n\nThe `stub` keyword indicates placeholder text that is shown when no text has been entered. The `key_filter` keyword, available only in Python, specifies a function to filter key presses. This is described in more detail under:\n\n- <https://osdoc.cogsci.nl/4.0/manual/forms/validation>\n\nOpenSesame script:\n\n~~~python\nwidget 0 0 1 1 text_input text=\"Initial text\" frame=yes center=no stub=\"Type here \u2026\" return_accepts=yes var=response\n~~~\n\nPython script:\n\n~~~ .python\nform = Form()\ntext_input = TextInput(\n    text='Initial text', frame=True, center=False, stub='Type here \u2026',\n    return_accepts=True, var='response', key_filter=my_filter_function\n)\nform.set_widget(text_input, (0, 0))\nform._exec()\n~~~", "url": "https://osdoc.cogsci.nl/4.0/manual/forms/widgets", "title": "Form widgets and keywords"}
{"content": "The API described is a Python API for OpenSesame, a software used for implementing psychology experiments. The main functionality revolves around the `responses` object, which is automatically created when an experiment starts and holds the history of all responses collected during the experiment. This object doesn't need to be imported into your Python script.\n\nThe API offers the following functions:\n\n- `add(response=None, correct=None, response_time=None, item=None, feedback=True)`: This function adds a response. The parameters to be added include: 'response' (the response value, for example, 'space' for the spacebar, 0 for joystick button 0, etc.), 'correct' (the correctness of the response), 'response_time' (the response time), 'item' (the item that collected the response), and 'feedback' (indicates whether the response should be included in feedback on accuracy and average response time).\n\n- `clear(self)`: This function clears all responses. It doesn't require any parameters.\n\n- `reset_feedback(self)`: This function sets the feedback status of all responses to False, meaning that only new responses will be included in feedback. It doesn't require any parameters. \n\nThrough the `responses` object, you can loop through all responses, print specific responses, and manipulate the response data in various ways.", "url": "https://osdoc.cogsci.nl/4.0/manual/python/responses", "title": "Access response history"}
{"content": "title: EyeLogic\n\nPyGaze offers *experimental support* for EyeLogic eye trackers as of OpenSesame 3.3.11. For more information, see:\n\n- <https://osdoc.cogsci.nl/4.0/manual/eyetracking/pygaze>\n- <https://www.eyelogicsolutions.com/>", "url": "https://osdoc.cogsci.nl/4.0/manual/eyetracking/eyelogic", "title": "EyeLogic"}
{"content": "This JavaScript API is for OpenSesame, a software used to implement psychology experiments. The API provides a suite of functions associated with the `Canvas` class which is used to present visual stimuli. A `Canvas` object is created using the `Canvas()` factory function without the need for a `new` keyword. In JavaScript `Canvas`, the display center is represented by the coordinates x=0, y=0. \n\n`StyleArgs` are style keywords that can be passed to all functions which accept them. These can also be set as properties of the `Canvas` object. Since JavaScript doesn't support named parameters, parameters are passed as an `Object` with named properties and default values. \n\nHere are the functions available in this API:\n\n- `Canvas.arrow(obj)`: This function draws an arrow, which is a polygon of 7 vertices. Parameters include `sx`, `sy`, `ex`, `ey`, `body_length`, `body_width`, `head_width`, and `styleArgs`.\n\n- `Canvas.clear([styleArgs])`: This function clears the canvas with the current background color.\n\n- `Canvas.circle(obj)`: This function draws a circle. Parameters include `x`, `y`, `r`, and `styleArgs`.\n\n- `Canvas.ellipse(obj)`: This function draws an ellipse. Parameters include `x`, `y`, `w`, `h`, and `styleArgs`.\n\n- `Canvas.fixdot(obj)`: This function draws a fixation dot. Parameters include `x`, `y`, `style`, and `styleArgs`.\n\n- `Canvas.gabor(obj)`: This function draws a gabor patch. Parameters include `x`, `y`, `orient`, `freq`, `env`, `size`, `stdev`, `phase`, `col1`, `col2`, `bgmode`, and `styleArgs`.\n\n- `Canvas.image(obj)`: This function draws an image from a file in the file pool. Parameters include `fname`, `center`, `x`, `y`, `scale`, `rotation`, and `styleArgs`.\n\n- `Canvas.line(obj)`: This function draws a line. Parameters include `sx`, `sy`, `ex`, `ey`, and `styleArgs`.\n\n- `Canvas.noise_patch(obj)`: This function draws a patch of noise, with an envelope. Parameters include `x`, `y`, `env`, `size`, `stdev`, `col1`, `col2`, `bgmode`, and `styleArgs`.\n\n- `Canvas.polygon(obj)`: This function draws a polygon which is defined by a list of vertices. Parameters include `vertices` and `styleArgs`.\n\n- `Canvas.rect(obj)`: This function draws a rectangle. Parameters include `x`, `y`, `w`, `h`, and `styleArgs`.\n\n- `Canvas.show()`: This function shows, or 'flips', the canvas on the screen and returns a timestamp of the time at which the canvas appeared on the screen.\n\n- `Canvas.text(obj)`: This function draws text. Parameters include `text`, `center`, `x`, `y`, and `styleArgs`.", "url": "https://osdoc.cogsci.nl/4.0/manual/javascript/canvas", "title": "Canvas functions"}
{"content": "title: Quest staircase next\n\nProcesses a response and updates the Quest test value.", "url": "https://osdoc.cogsci.nl/4.0/items/quest_staircase_next", "title": "Quest staircase next"}
{"content": "title: Installing packages, plugins, and extensions\n\n\nThis page has moved to:\n\n- <https://rapunzel.cogsci.nl/manual/environment/>", "url": "https://osdoc.cogsci.nl/4.0/manual/environment", "title": "Installing packages, plugins, and extensions"}
{"content": "title: Ambulatory Monitoring System (VU-AMS)\n\nVU-AMS is a third-party plugin, and is not maintained by the OpenSesame team.\n{: .alert .alert-info}\n\n\nThe VU University Ambulatory Monitoring System (VU-AMS) is a device that can be used to measure a variety of factors related to heart rate, respiration, and body movement. The developers offer an OpenSesame template on their website.\n\nFor more information, see:\n\n- <http://www.vu-ams.nl> (product website)\n- <http://www.vu-ams.nl/support/downloads/extras/> (OpenSesame template)", "url": "https://osdoc.cogsci.nl/4.0/manual/devices/vuams", "title": "Ambulatory Monitoring System (VU-AMS)"}
{"content": "title: Keyboard responses\n\nKeyboard responses are collected with the KEYBOARD_RESPONSE item.\n\n[TOC]\n\n\n## Response variables\n\nThe KEYBOARD_RESPONSE sets the standard response variables as described here:\n\n- <https://osdoc.cogsci.nl/4.0/manual/variables>\n\n## Key names\n\nKeys are generally identified by their character and/ or their description (depending on which is applicable). For example:\n\n- The `/` key is named 'slash' and '/'. You can use either of the two names.\n- The `a` is named 'a'.\n- The left-arrow key is named 'left'.\n\nIf you don't know what a particular key is named, you can:\n\n- Click on the 'List available keys' button; or\n- Create a simple experiment in which a KEYBOARD_RESPONSE is immediately followed by a FEEDBACK item with the text '{response}' on it. This will show the name of the previously collected response.\n\n\n## Correct response\n\nThe *Correct response* field indicates which response is considered correct. After a correct response, the `correct` variable is automatically set to 1; after an incorrect response (i.e. everything else), `correct` is set to 0; if no correct response is specified, `correct` is set to 'undefined'.\n\nYou can indicate the correct response in three main ways:\n\n- *Leave the field empty.* If you leave the *Correct response* field empty, OpenSesame will automatically check if a variable called `correct_response` has been defined, and, if so, use this variable for the correct response.\n- *Enter a literal value.* You can explicitly enter a response, such as 'left' in the case of a KEYBOARD_RESPONSE item. This is only useful if the correct response is fixed.\n- *Enter a variable name.* You can enter a variable, such as '{cr}'. In this case, this variable will be used for the correct response.\n\n\n## Allowed responses\n\nThe *Allowed responses* field indicates a list of allowed responses. All other responses will be ignored, except for 'Escape', which will pause the experiment. The allowed responses should be a semicolon-separated list of responses, such as 'a;left;/' for a KEYBOARD_RESPONSE. To accept all responses, leave the *Allowed responses* field empty.\n\n\n## Timeout\n\nThe *Timeout* field indicates a timeout value in milliseconds, or 'infinite' for no timeout. When a timeout occurs, the following happens:\n\n- `response_time` is set to the timeout value, or rather to the time it takes for a timeout to be registered, which may deviate slightly from the timeout value.\n- `response` is set to 'None'. This means that you can specify 'None' for the correct response a timeout should occur; this can be useful, for example, in a go/no-go task, when the participant should withold a response on no-go trials.\n\n\n## Collecting keyboard responses in Python\n\nYou can use the `keyboard` object to collect keyboard responses in Python:\n\n- <https://osdoc.cogsci.nl/4.0/manual/python/keyboard>", "url": "https://osdoc.cogsci.nl/4.0/manual/response/keyboard", "title": "Keyboard responses"}
{"content": "The document provides detailed information about PyGaze, a Python library for eye tracking. PyGaze can be used within OpenSesame using a set of plugins. It supports eye trackers such as EyeLink and EyeTribe, with experimental support for others like EyeLogic, GazePoint/OpenGaze, SMI, and Tobii. It also supports basic eye tracking in online experiments with WebGazer.js, and includes two dummy eye trackers for testing purposes.\n\nThe document provides installation instructions for Windows and Ubuntu, as well as via pip and Anaconda on all platforms. It also lists the available PyGaze plugins and their uses. These include initializing PyGaze, implementing a drift correction procedure, putting PyGaze in and out of recording mode, pausing until an event occurs, and logging experimental variables and arbitrary text.\n\nThe document also provides an example of how to use PyGaze in a Python script and a comprehensive list of functions that can be used once PyGaze is initialized. These functions include calibrating the eye tracking system, closing connection to the tracker, drawing calibration and drift-correction targets, logging messages and variables, starting and stopping recording, and waiting for various eye events like blink start or end, fixation start or end, and saccade start or end.\n\nThe document also provides a citation for PyGaze and a link to the official PyGaze website for more information.", "url": "https://osdoc.cogsci.nl/4.0/manual/eyetracking/pygaze", "title": "PyGaze (eye tracking)"}
{"content": "title: Reset_feedback\n\nThis plug-in has the same effect as presenting a FEEDBACK item with a duration of 0 ms\n{: .page-notification}\n\nIf you do not reset feedback variables, you may confound your feedback with responses that are not relevant to the task. For example, the key presses made during the instruction phase may affect the feedback during the first block of the experiment. Therefore, you will need to reset the feedback variables at appropriate moments.\n\nThis plug-in will reset the following variables to 0:\n\n- `total_response_time`\n- `total_response`\n- `acc`\n- `accuracy`\n- `avg_rt`\n- `average_response_time`", "url": "https://osdoc.cogsci.nl/4.0/items/reset_feedback", "title": "Reset_feedback"}
{"content": "title: Sona Systems\n\n\n[TOC]\n\n\n## About Sona Systems\n\nSona Systems is an online tool that many universities use for recruiting participants, granting course credit to student participants, etc.\n\nSee also:\n\n- <https://www.sona-systems.com/help/integration_test.aspx>\n\n\n## Create a study on JATOS\n\nFirst, import your experiment into JATOS, as described above. Next, go the Worker & Batch Manager, activate the General Multiple Worker, get a URL by clicking on Get Link, and copy it.\n\n\n## Create a study on Sona Systems\n\nNext, create a study on Sona Systems. Insert the JATOS study URL in the field labeled \"Study URL\". This will tell Sona Systems how to start the experiment. Importantly, add the following to the end of the URL (this will pass the participant's Sona ID to your experiment):\n\n```bash\n?SONA_ID=%SURVEY_CODE%  \n```\n\nSona Systems does not use a Redirect URL. This means that Sona Systems will not automatically know whether or not the participant finished the study.\n\n\n## Register the Sona ID in your experiment\n\nEvery participant from Sona is identified by a unique ID. It's important to log this ID in your experiment, because this allows you to tell which participant from Sona corresponds to which entry in the JATOS results. You can do this by adding the script below in the Prepare phase of an `inline_javascript` item at the very start of your experiment.\n\nWhen running the experiment through Sona, this will make the Sona ID available as the experimental variable `sona_participant_id`. When the running the experiment in any other way (e.g. during testing), the variable `sona_participant_id` will be set to -1. \n\n\n```javascript\nif (window.jatos && jatos.urlQueryParameters.SONA_ID) {\n    console.log('Sona information is available')\n    var sona_participant_id = jatos.urlQueryParameters.SONA_ID\n} else {\n    console.log('Sona information is not available (setting value to -1)')\n    var sona_participant_id = -1\n}\nconsole.log('sona_participant_id = ' + sona_participant_id)\n```\n\n\n## Automatically grant credits on study completion\n\nSona Systems provides a completion URL (client-side), which should be called when a study is succesfully completed, so that Sona Systems can grant credit to the participant (see [Figure 1](#FigCompletionURL)).\n\n\n![/pages/manual/osweb/img/sonasystems/completion-url.png](/4.0/pages/manual/osweb/img/sonasystems/completion-url.png)\n\n__Figure 1.__ The completion URL in the Sona Systems study information.\n{: .fig-caption #FigCompletionURL}\n\n\n\nThe completion URL (client side) has three arguments in it:\n\n- `experiment_id` which identifies the study and is the same for all participants\n- `credit_token` which (apparently) changes when you change the study information, but is otherwise the same for all participants\n- `survey_code` which corresponds to the Sona Participant ID, and is therefore different for each participant\n\nCopy the completion URL, and replace the `XXX` by `[SONA_ID]`. Go to Study Properties on JATOS, and insert the resulting URL into the End Redirect URL field.\n\n\n![/pages/manual/osweb/img/sonasystems/end-redirect-url.png](/4.0/pages/manual/osweb/img/sonasystems/end-redirect-url.png)\n\n__Figure 2.__ The end-redirect URL in the JATOS study properties.\n{: .fig-caption #FigEndRedirectURL}\n", "url": "https://osdoc.cogsci.nl/4.0/manual/osweb/sonasystems", "title": "Sona Systems"}
{"content": "title: JATOS\n\n\n[TOC]\n\n\n## Introduction to JATOS\n\n[JATOS](https://www.jatos.org/) is a system for managing online experiments. It allows you to create accounts for experimenters, upload experiments, and generate links that you can distribute to participants. OpenSesame integrates closely with JATOS.\n\nTo access a JATOS server, you have three main options:\n\n- Request a free account on [MindProbe](https://mindprobe.eu/), a public JATOS server sponsored by ESCoP and OpenSesame.\n- se a JATOS server provided by your institution.\n- Download JATOS and install it on your own server.\n\n## Linking OpenSesame with JATOS/MindProbe\n\nOpenSesame requires an API token to access your account on a JATOS server such as MindProbe. Follow these steps to generate an API token:\n\n1. **Log into JATOS.**\n2. **Open your user profile** by clicking on your name located in the top right corner of the page.\n3. **Create an API token** by clicking on 'API tokens' to view all your current tokens, and then click 'New Token'.\n4. **Assign a name to your token**. This name serves as a descriptor indicating its intended use, such as 'OpenSesame integration'.\n5. **Set an expiration for your token**. Tokens default to expire after 30 days, requiring you to generate a new token each month. You can select 'No Expiration' for convenience, but be aware that it is less secure. If someone gains access to a non-expiring token, they can use it indefinitely, or until you revoke the token.\n\n\n![/pages/manual/osweb/img/jatos/api-token.png](/4.0/pages/manual/osweb/img/jatos/api-token.png)\n\n__Figure 1.__ API tokens can be generated within your JATOS user profile.\n{: .fig-caption #FigAPIToken}\n\n\n\nNote: An API token always begins with `jap_`, followed by a series of characters and numbers. Keep your token secure!\n\nOnce you have your API token, open the OSWeb and JATOS control panel in OpenSesame. Enter your API token into the corresponding field and also adjust the JATOS server URL, if necessary.\n\n\n![/pages/manual/osweb/img/jatos/jatos-control-panel.png](/4.0/pages/manual/osweb/img/jatos/jatos-control-panel.png)\n\n__Figure 2.__ Specify the JATOS server and your API token in the OSWeb and JATOS control panel.\n{: .fig-caption #FigJATOSControlPanel}\n\n\n\n\n## Publishing experiments to, and downloading from, JATOS/MindProbe\n\nAfter successfully connecting OpenSesame to JATOS, as explained above, you can publish your experiment to JATOS. To do this, select the 'Publish to JATOS/MindProbe' option from the File menu. Upon initial publication, your experiment will be assigned a unique identifier (UUID) that links it to a study on JATOS.\n\nYou can then visit your JATOS server and observe that the newly published experiment has been added to your list of studies.\n\nFrom that point forward, each time you publish the experiment, the existing JATOS study will be updated with the new version. If you wish to publish the experiment as a completely new study on JATOS, you will need to reset the JATOS UUID via the OSWeb and JATOS control panel.\n\nTo download an experiment from JATOS, select the 'Open from JATOS/MindProbe' option from the File menu. Please note, this function is only applicable if the corresponding JATOS study is compatible with OSWeb 2.", "url": "https://osdoc.cogsci.nl/4.0/manual/osweb/jatos", "title": "JATOS"}
{"content": "title: WebGazer.js\n\nRequires OSWeb v1.4.6.1\n{:.page-notification}\n\n[TOC]\n\n\n## About WebGazer\n\nWebGazer.js is an eye-tracking library written in JavaScript. You can include it with OSWeb to perform eye tracking in online experiments.\n\n- <https://webgazer.cs.brown.edu/>\n\n\n## Including WebGazer.js in the experiment\n\nWebGazer.js is not bundled with OSWeb by default. However, you can include it as an external library by entering a link to `webgazer.js` under External JavaScript libraries. Currently, a functional link is:\n\n```\nhttps://webgazer.cs.brown.edu/webgazer.js\n```\n\nSee also:\n\n- <https://osdoc.cogsci.nl/4.0/manual/osweb/osweb>\n\n\n## Example experiment\n\nBelow you can download an example experiment that uses WebGazer.js. Participants are first asked to click on and look at a set of dots; this will cause WebGazer.js to automatically perform something akin to a calibration procedure. Next, the experiment shows a simple screen to test the accuracy of gaze-position recording. In general, fine-grained eye tracking is not feasible, but you can tell which quadrant of the screen a participant is looking at. To run this experiment, you need include WebGazer.js in the experiment, as described above. \n\n- <https://osdoc.cogsci.nl/4.0/attachments/webgazer.osexp>\n\nYou can also launch the experiment directly in the browser:\n\n- <https://jatos.mindprobe.eu/publix/BowSAFY2VWl>", "url": "https://osdoc.cogsci.nl/4.0/manual/eyetracking/webgazer", "title": "WebGazer.js"}
{"content": "title: Video playback\n\n[TOC]\n\n\n## media_player_mpy plugin\n\nThe MEDIA_PLAYER_MPY plugin is based on MoviePy. It is included by default with the Windows and Mac OS packages of OpenSesame. If it is not installed, you can get it by installing the `opensesame-plugin-media-player-mpy` package, as described here:\n\n- <https://rapunzel.cogsci.nl/manual/environment/>\n\nThe source code is hosted at:\n\n- <https://github.com/dschreij/opensesame-plugin-mediaplayer>\n\n\n## OpenCV\n\nOpenCV is a powerful computer vision library, which contains (among many other things) routines for reading video files.\n\n- <http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html>\n\nThe following example shows how to play back a video file, while drawing a red square on top of the video. This example assumes that you're using the legacy backend.\n\n~~~ .python\nimport cv2\nimport numpy\nimport pygame\n# Full path to the video file in file pool\npath = pool['myvideo.avi']\n# Open the video\nvideo = cv2.VideoCapture(path)\n# A loop to play the video file. This can also be a while loop until a key\n# is pressed. etc.\nfor i in range(100):\n    # Get a frame\n    retval, frame = video.read()\n    # Rotate it, because for some reason it otherwise appears flipped.\n    frame = numpy.rot90(frame)\n    # The video uses BGR colors and PyGame needs RGB\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    # Create a PyGame surface\n    surf = pygame.surfarray.make_surface(frame)\n    # Now you can draw whatever you want onto the PyGame surface!\n    pygame.draw.rect(surf, (255,0,0), (100, 100, 200, 200))\n    # Show the PyGame surface!\n    exp.surface.blit(surf, (0, 0))\n    pygame.display.flip()\n~~~", "url": "https://osdoc.cogsci.nl/4.0/manual/stimuli/video", "title": "Video playback"}
{"content": "title: Doing things in parallel\n\n\nCoroutines run multiple items in parallel\u2014or, to be more exact, they run items in rapid alternation in a way that looks parallel. Not all items support coroutines.\n\n\n[TOC]\n\n\n## Using coroutines\n\nYou can use coroutines through the COROUTINES plugin (see [Figure 1](#FigCoroutinesInterface)).\n\n\n\n![/pages/manual/structure/img/coroutines/FigCoroutinesInterface.png](/4.0/pages/manual/structure/img/coroutines/FigCoroutinesInterface.png)\n\n__Figure 1.__ The interface of the coroutines plugin.\n{: .fig-caption #FigCoroutinesInterface}\n\n\n\n\nAs you can see, the COROUTINES plugin looks similar to the SEQUENCE item, but has a few extra options:\n\n- *Duration* indicates the total duration of the coroutines.\n- *End after item (optional)* indicates that the coroutines should end when a specific item has ended. This allows you, for example, to indicate that the coroutines should end when a key press has been collected, by selecting a KEYBOARD_RESPONSE item here.\n- Each item has a *Start time*. Most items also have an *End time*. The end time does not apply to one-shot items; for example, SKETCHPADs show a display and terminate immediately, so they have no end time.\n\nSpecifically, the example from [Figure 1](#FigCoroutinesInterface) (from the stop-signal-task example) does the following:\n\n- It shows a target display immediately.\n- If the `stop_after` variable is not empty, it shows the stop_signal display after an interval specified by the `stop_after` variable.\n- During the entire (2000 ms) interval, a keyboard response is collected.\n\nThe temporal flow is controlled by the COROUTINES plugin. Therefore, the timeout and duration values specified in the items are not used. For example, in [Figure 1](#FigCoroutinesInterface), the KEYBOARD_RESPONSE will run for 2000 ms, regardless of the timeout that is specified in the item.\n\n\n## Supported items\n\nCurrently, the following items are supported (this list may not be exhaustive):\n\n- FEEDBACK\n- INLINE_SCRIPT\n- KEYBOARD_RESPONSE\n- LOGGER\n- MOUSE_RESPONSE\n- SAMPLER\n- SYNTH\n- SKETCHPAD\n\n\n## Using inline_script items in coroutines\n\nWhen you use an INLINE_SCRIPT item in a COROUTINES, the Run phase works a little differently from what you might be used to. Specifically, the Run phase is executed on every iteration of the COROUTINES. In addition, the Run phase should only contain code that takes very little time to execute; this is because time-consuming operations will block the COROUTINES, thus interfering with the timing of other items in the COROUTINES as well. To end the COROUTINES, you can raise an `AbortCoroutines()` exception.\n\nFor example, say that you have a COROUTINES with two KEYBOARD_RESPONSE items, *kb1* and *kb2*, and you want to run the COROUTINES until two key presses have been collected, with a timeout of 5000 ms. You could then create the following COROUTINES structure:\n\n\n\n![/pages/manual/structure/img/coroutines/FigCoroutinesTwoResponses.png](/4.0/pages/manual/structure/img/coroutines/FigCoroutinesTwoResponses.png)\n\n__Figure 2.__ A coroutines that collects two keypress responses\n{: .fig-caption #FigCoroutinesTwoResponses}\n\n\n\nThe *check_responses* INLINE_SCRIPT would then first set both responses variables to an empty string in the Prepare phase:\n\n```python\n# This is executed at the start of the coroutines\nresponse_kb1 = ''\nresponse_kb2 = ''\n```\n\nAnd then, in the Run phase, check if both variables have been set, and abort the coroutines if this is the case:\n\n```python\n# Values that are not an empty string are True for Python\n# This code will be executed many times!\nif response_kb1 and response_kb2:\n    raise AbortCoroutines()\n```\n\n## Run-if expressions\n\nThe behavior of run-if expressions in COROUTINES is a bit different from that in SEQUENCE items. Specifically, run-if expressions in COROUTINES are evaluated during the prepare phase. See also:\n\n- <https://osdoc.cogsci.nl/4.0/manual/prepare-run>", "url": "https://osdoc.cogsci.nl/4.0/manual/structure/coroutines", "title": "Doing things in parallel"}
{"content": "title: Serial port\n\nPySerial is an easy to use Python library for serial port communications, which is bundled with all OpenSesame packages. For more information, see:\n\n- <http://pyserial.sourceforge.net/>", "url": "https://osdoc.cogsci.nl/4.0/manual/devices/serial", "title": "Serial port"}
{"content": "title: Text\n\n[TOC]\n\n## How can I present text?\n\nThe most common way to show text is using a SKETCHPAD or FEEDBACK item. These allow you to enter text and other visual stimuli. For a questionnaire-like way to show text, you can use [forms](<https://osdoc.cogsci.nl/4.0/manual/forms/about>).\n\n\n## HTML formatting\n\nYou can use a HTML tags, which you can simply insert into your text. You can use these tags everywhere: In SKETCHPAD items, in INLINE_SCRIPTs (provided you use the `Canvas` class), in forms, etc.\n\nExample:\n\n~~~ .html\nOpenSesame supports a sub-set of HTML tags:\n- <b>Bold face</b>\n- <i>Italic</i>\n- <u>Underline</u>\n\nIn addition, you can pass 'color', 'size', and 'style' as keywords to a 'span' tag:\n- <span style='color:red;'>Color</span>\n- <span style='font-size:32px;'>Font size</span>\n- <span style='font-family:serif;'>Font style</span>\n\nFinally, you can force newlines with the 'br' tag:\nLine 1<br>Line 2\n~~~\n\n\n## Variables and inline Python\n\nYou can embed variables in text using the `{...}` syntax. For example, the following:\n\n~~~ .python\nThe subject number is {subject_nr}\n~~~\n\n... might evaluate to (for subject 1):\n\n~~~ .python\nThe subject number is 1\n~~~\n\nYou can also embed Python expression. For example, the following:\n\n~~~ .python\nThe subject number modulo five is {subject_nr % 5}\n~~~\n\n... might evaluate to (for subject 7)\n\n~~~ .python\nThe subject number modulo five is 2\n~~~\n\n\n## Fonts\n\n### Default fonts\n\nYou can select one of the default fonts from the font-selection dialogs ([Figure 1](#FigFontSelect)). These fonts are included with OpenSesame and your experiment will therefore be fully portable when you use them.\n\n\n![/pages/manual/stimuli/img/text/font-selection-dialog.png](/4.0/pages/manual/stimuli/img/text/font-selection-dialog.png)\n\n__Figure 1.__ A number of default fonts, which are bundled with OpenSesame, can be selected through the font-selection dialogs.\n{: .fig-caption #FigFontSelect}\n\n\n\nThe fonts have been renamed for clarity, but correspond to the following open-source fonts:\n\n|__Name in OpenSesame__\t\t|__Actual font__\t\t|\n|---------------------------|-----------------------|\n|`sans`\t\t\t\t\t\t|Droid Sans\t\t\t\t|\n|`serif`\t\t\t\t\t|Droid Serif\t\t\t|\n|`mono`\t\t\t\t\t\t|Droid Sans Mono\t\t|\n|`chinese-japanese-korean`\t|WenQuanYi Micro Hei\t|\n|`arabic`\t\t\t\t\t|Droid Arabic Naskh\t\t|\n|`hebrew`\t\t\t\t\t|Droid Sans Hebrew\t\t|\n|`hindi`\t\t\t\t\t|Lohit Hindi\t\t\t|\n\n### Selecting a custom font through the font-selection dialog\n\nIf you select 'other ...' in the font selection dialog, you can select any font that is available on your operating system. If you do this, your experiment is no longer fully portable, and will require that the selected font is installed on the system that you run your experiment on.\n\n### Placing a custom font in the file pool\n\nAnother way to use a custom font is to put a font file in the file pool. For example, if you place the font file `inconsolata.ttf` in the file pool, you can use this font in a SKETCHPAD item, like so:\n\n\tdraw textline 0.0 0.0 \"This will be inconsolata\" font_family=\"inconsolata\"\n\nNote that the font file must be a truetype `.ttf` file.", "url": "https://osdoc.cogsci.nl/4.0/manual/stimuli/text", "title": "Text"}
{"content": "title: Using the form plugins\n\nA number of commonly used forms are available as ready-made plugins. These allow you to use common forms, without any need for scripting.\n\n- FORM_CONSENT is a simple digital consent form (disclaimer: some journals may require *written* consent)\n- FORM_MULTIPLE_CHOICE allows you to present multiple choice questions\n- FORM_TEXT_DISPLAY is a simple text display that you can use to show instructions etc.\n- FORM_TEXT_INPUT is a simple text input display that allows you to ask a question and collect a multi-character response from the participant\n\nThe FORM_BASE plugin is special. It allows you to define custom forms using OpenSesame script, as described here:\n\n- <https://osdoc.cogsci.nl/4.0/manual/forms/custom>\n\n\n![/pages/manual/forms/img/readymade/form-plugins.png](/4.0/pages/manual/forms/img/readymade/form-plugins.png)\n\n__Figure 1.__ The FORM plugins in the item toolbar.\n{: .fig-caption #FigFormPlugins}\n\n", "url": "https://osdoc.cogsci.nl/4.0/manual/forms/readymade", "title": "Using the form plugins"}
{"content": "title: Prolific\n\n\n[TOC]\n\n\n## About Prolific\n\n[Prolific](https://prolific.co/) is a commercial tool for recruiting participants for research. To run OSWeb experiments on Prolific, you need to follow the steps explained below.\n\nSee also:\n\n- <http://www.jatos.org/Use-Prolific.html>\n\n\n## Create a study on JATOS\n\nFirst, import your experiment into JATOS, as described above. Next, go the Worker & Batch Manager, activate the General Multiple Worker, get a URL by clicking on Get Link, and copy it ([Figure 1](#FigJatosURL)).\n\n\n\n![/pages/manual/osweb/img/prolific/jatos-url.png](/4.0/pages/manual/osweb/img/prolific/jatos-url.png)\n\n__Figure 1.__ Get a study URL from JATOS.\n{: .fig-caption #FigJatosURL}\n\n\n\n\n\n## Create a study on Prolific\n\nNext, create a study on Prolific. Under Study Details ([Figure 2](#FigProlific)), insert the JATOS study URL in the field labeled \"What is the URL of your study?\". This will tell Prolific how to start the experiment. Importantly, add the following to the end of the URL (this will pass important information from Prolific to your experiment):\n\n\n```bash\n&PROLIFIC_PID={{%PROLIFIC_PID%}}&STUDY_ID={{%STUDY_ID%}}&SESSION_ID={{%SESSION_ID%}}\n```\n\n\nWhen the experiment is finished, Prolific needs to know about it. For this purpose, Prolific uses an End Redirect URL, which is listed in the field labeled \"To prove that participants have completed your study \u2026\". Copy this End Redirect URL. Also check the box labeled \"I've set up my study to redirect to this url at the end\".\n\n\n![/pages/manual/osweb/img/prolific/prolific.png](/4.0/pages/manual/osweb/img/prolific/prolific.png)\n\n__Figure 2.__ Study details on Prolific.\n{: .fig-caption #FigProlific}\n\n\n\n\n\n## Set an End Redirect URL in JATOS\n\nNow go back to JATOS, and open the Properties of your study ([Figure 3](#FigJatosProperties)). There, paste the End Redirect URL that you have copied from Prolific in the field labeled \"End Redirect URL\". This will tell JATOS that the participant should be redirected back to Prolific when the experiment is finished, so that Prolific knows that the participant completed the experiment.\n\n\n\n![/pages/manual/osweb/img/prolific/jatos-properties.png](/4.0/pages/manual/osweb/img/prolific/jatos-properties.png)\n\n__Figure 3.__ Set the End Redirect URL in JATOS.\n{: .fig-caption #FigJatosProperties}\n\n\n\n\n## Register Prolific information in your experiment\n\nEvery participant from Prolific is identified by a unique ID. It's important to log this ID in your experiment, because this allows you to tell which participant from Prolific corresponds to which entry in the JATOS results. You can do this by adding the script below in the Prepare phase of an `inline_javascript` item at the very start of your experiment.\n\nWhen running the experiment through Prolific, this will make the Prolific ID available as the experimental variable `prolific_participant_id`. When the running the experiment in any other way (e.g. during testing), the variable `prolific_participant_id` will be set to -1. The same logic applied to the Prolific Study ID (`prolific_study_id`) and the Prolific Session ID (`prolific_session_id`).\n\n\n```javascript\nif (window.jatos && jatos.urlQueryParameters.PROLIFIC_PID) {\n    console.log('Prolific information is available')\n    var prolific_participant_id = jatos.urlQueryParameters.PROLIFIC_PID\n    var prolific_study_id = jatos.urlQueryParameters.STUDY_ID\n    var prolific_session_id = jatos.urlQueryParameters.SESSION_ID\n} else {\n    console.log('Prolific information is not available (setting values to -1)')\n    var prolific_participant_id = -1\n    var prolific_study_id = -1\n    var prolific_session_id = -1\n}\nconsole.log('prolific_participant_id = ' + prolific_participant_id)\nconsole.log('prolific_study_id = ' + prolific_study_id)\nconsole.log('prolific_session_id = ' + prolific_session_id)\n```\n\n\n## Test the study\n\nGo back to the Study Details page on Prolific. At the bottom of the page, there is a Preview button. This allows you to test the experiment by acting as a participant yourself. Don't forget to check the JATOS results to make sure that the experiment has successfully finished, and that all the necessary information (including the Prolific information) has been logged!", "url": "https://osdoc.cogsci.nl/4.0/manual/osweb/prolific", "title": "Prolific"}
{"content": "title: Eyelink\n\n[TOC]\n\n## About EyeLink\n\nThe Eyelink series of eye trackers, produced by SR Research, are one of the most commonly used eye trackers in psychological research. SR Research provides Python bindings for the Eyelink (called PyLink), which are used by PyGaze. The license of PyLink is incompatible with the license used by OpenSesame. For that reason, PyLink is not included in the default distribution of OpenSesame, and needs to be installed separately.\n\n\n## Windows\n\n### Installing the EyeLink Developers Kit\n\nThe Eyelink Developers Kit (sometimes called Display Software) provides the libraries that are required to communicate with the Eyelink PC. You can find it here (free registration required):\n\n- <https://www.sr-research.com/support/thread-13.html>\n\nIf you extract the `.zip`, and then run the `.exe` installer, the EyeLink display will be installed in one of the following folders (depending on your version of Windows:\n\n```\nC:\\Program Files\\SR Research\\EyeLink\\\nC:\\Program Files (x86)\\SR Research\\EyeLink\n```\n\nIn this folder, there is a `libs` subfolder, which you need to add to the system Path (this may have been added to the path automatically, but check to make sure). You can do this by opening \"My Computer\", clicking on \"View system information\", opening the \"Advanced\" tab, clicking on \"Environment Variables\" and appending `;C:\\Program Files\\SR Research\\EyeLink\\libs` or (depending on your system) `;C:\\Program Files (x86)\\SR Research\\EyeLink\\libs` to the Path variable (under System variables).\n\n\n### Installing OpenSesame with PyLink\n\nPyLink is the Python library for EyeLink support. PyLink can be installed from the SR Research PyPi repository through `pip install`:\n\n```\npip install --index-url=https://pypi.sr-research.com sr-research-pylink\n```\n\nYou can find more information about PyLink on the SR Research forum (free registration required):\n\n- <https://www.sr-research.com/support/thread-8291.html>\n\n\n## Ubuntu\n\nThe EyeLink display software can be installed directly from a repository. This also installs PyLink and various convenient tools, such ast the `edf2asc` converter.\n\n```bash\nsudo add-apt-repository 'deb [arch=amd64] https://apt.sr-research.com SRResearch main'\nsudo apt-key adv --fetch-keys https://apt.sr-research.com/SRResearch_key\nsudo apt-get update\nsudo apt-get install eyelink-display-software\n```\n\nFor more information, please visit:\n\n- <https://www.sr-support.com/thread-13.html>\n\n\n## PyGaze\n\nAfter you have install the EyeLink display software and PyLink per the instructions above, you can use the EyeLink with PyGaze! See:\n\n- <https://osdoc.cogsci.nl/4.0/manual/eyetracking/pygaze>", "url": "https://osdoc.cogsci.nl/4.0/manual/eyetracking/eyelink", "title": "Eyelink"}
{"content": "The backend of OpenSesame is the software layer that manages input (like keyboard and mouse) and output (like display and sound playback). OpenSesame is backend-independent, allowing the user to choose the backend that should be used. Currently, there are four backends: legacy, psycho, xpyriment, and osweb.\n\nThe differences between the backends are usually technical, and they all work in similar ways when using the graphical user interface. However, different backends may be preferred depending on the requirements of the experiment. For example, if one wants to run the experiment in a browser, the osweb backend should be used. Backends also differ in temporal precision, stimulus preparation time, and the specific functionalities they offer when writing Python code.\n\nThe backend can be selected in the general properties of the experiment. Also, the six different backends (canvas, keyboard, mouse, sampler, color, and clock) can be mixed and matched depending on needs.\n\nThe xpyriment backend, built on Expyriment, is lightweight and has excellent timing properties. The psycho backend, built on PsychoPy, is hardware-accelerated and useful for creating complex visual stimuli. The legacy backend, built on PyGame, is reliable and well-supported on a wide range of platforms, but lacks hardware acceleration. The osweb backend allows experiments to run in a browser.", "url": "https://osdoc.cogsci.nl/4.0/manual/backends", "title": "Backends"}
{"content": "title: Logging and reading data files\n\nAlways triple check whether your data has been logged correctly before running your experiment!\n{: .page-notification}\n\n[TOC]\n\n\n## Using the logger item\n\nOpenSesame will not log your data automatically. Instead, you need to insert a LOGGER item, typically at the end of your trial sequence.\n\n\n![/pages/manual/img/logging/logger.png](/4.0/pages/manual/img/logging/logger.png)\n\n__Figure 1.__ The LOGGER item.\n{: .fig-caption #FigLogger}\n\n\n\nThe simplest way to use the LOGGER is by leaving the option 'Automatically log all variables' enabled. That way, all variables that OpenSesame knows about are written the log file, except for those that are explicitly excluded (see below).\n\nYou can explicitly *include* which variables you want to log. The main reason for doing so is when you find that some variables are missing (because OpenSesame did not automatically detect them), or if you have disabled the option 'Automatically log all variables', \n\nYou can also explicitly exclude certain variables from the log file. The main reason for doing so is to keep the log files clean by excluding variables that are generally not useful.\n\nIn general, you should create only one logger item, and reuse that LOGGER at different locations in your experiment if necessary (i.e. use linked copies of the same LOGGER item). If you create multiple LOGGERs (rather than using a single LOGGER multiple times), they will all write to the same log file, and the result will be a mess!\n\n## Using Python inline script\n\nYou can write to the log file using the `log` object:\n\n~~~ .python\nlog.write('This will be written to the log file!')\n~~~\n\nFor more information, see:\n\n- <https://osdoc.cogsci.nl/4.0/manual/python/log>\n\nYou should generally not write to the log file directly and use a LOGGER item at the same time; doing so will result in messy log files.\n\n## Format of the data files\n\nIf you have used the standard LOGGER item, data files are in the following format format (simply standard csv):\n\n- plain-text\n- comma-separated\n- double-quoted (literal double-quotes are escaped with backward slashes)\n- unix-style line endings\n- UTF-8 encoded\n- column names on the first row\n\n## Reading and processing data files\n\n### In Python with pandas or DataMatrix\n\nIn Python, you can use [pandas](http://pandas.pydata.org/) to read csv files.\n\n```python\nimport pandas\ndf = pandas.read_csv('subject-1.csv')\nprint(df)\n```\n\nOr [DataMatrix](https://datamatrix.cogsci.nl/):\n\n```python\nfrom datamatrix import io\ndm = io.readtxt('subject-1.csv')\nprint(dm)\n```\n\n", "url": "https://osdoc.cogsci.nl/4.0/manual/logging", "title": "Logging and reading data files"}
{"content": "### In R\n\nIn R, you can simply use the `read.csv()` function to read a single data file.\n\n~~~ .R\ndf = read.csv('subject-1.csv', encoding = 'UTF-8')\nhead(df)\n~~~\n\nIn addition, you can use the `read_opensesame()` function from the [readbulk](https://github.com/pascalkieslich/readbulk) package to easily read and merge multiple data files into one large data frame. The package is available on CRAN and can be installed via `install.packages('readbulk')`.\n\n~~~ .R\n# Read and merge all data files stored in the folder 'raw_data'\nlibrary(readbulk)\ndf = read_opensesame('raw_data')\n~~~\n\n### In JASP\n\n[JASP](http://jasp-stats.org/), an open-source statistics package, opens csv files straight away.\n\n### In LibreOffice Calc\n\nIf you open a csv file in LibreOffice Calc, you have to indicate the exact data format, as indicated in [Figure 2](#FigLibreOffice). (The default settings are often correct.)\n\n\n![/pages/manual/img/logging/libreoffice.png](/4.0/pages/manual/img/logging/libreoffice.png)\n\n__Figure 2.__ \n{: .fig-caption #FigLibreOffice}\n\n\n\n### In Microsoft Excel\n\nIn Microsoft Excel, you need to use the Text Import Wizard.\n\n### Merging multiple data files into one large file\n\nFor some purposes, such as using pivot tables, it may be convenient to merge all data files into one large file. With Python DataMatrix, you can do this with the following script:\n\n```python\nimport os\nfrom datamatrix import DataMatrix, io, operations as ops\n\n# Change this to the folder that contains the .csv files\nSRC_FOLDER = 'student_data'\n# Change this to a list of column names that you want to keep\nCOLUMNS_TO_KEEP = [\n    'RT_search',\n    'load',\n    'memory_resp'\n]\n\n\ndm = DataMatrix()\nfor basename in os.listdir(SRC_FOLDER):\n    path = os.path.join(SRC_FOLDER, basename)\n    print('Reading {}'.format(path))\n    dm <<= ops.keep_only(io.readtxt(path), *COLUMNS_TO_KEEP)\nio.writetxt(dm, 'merged-data.csv')\n```\n\n\n## Logging in OSWeb\n\nWhen you run an experiment in a browser with OSWeb, logging works differently from when you run an experiment on the desktop.\n\nSpecifically, when you launch an OSWeb experiment directly from within OpenSesame, the log file is downloaded at the end of the experiment. This log file is in `.json` format. When you launch an OSWeb experiment from JATOS, there is no log file as such, but rather all data is sent to JATOS from where it can be downloaded.\n\nSee also:\n\n- <https://osdoc.cogsci.nl/4.0/manual/osweb/workflow>\n\n\n\n[libreoffice]: http://www.libreoffice.org/\n[openoffice]: http://www.openoffice.org/\n[gnumeric]: http://projects.gnome.org/gnumeric/\n[log-func]: /python/inline-script/#inline_script.log\n[codecs]: http://docs.python.org/2/library/codecs.html\n[ppa]: https://launchpad.net/~smathot/+archive/cogscinl/", "url": "https://osdoc.cogsci.nl/4.0/manual/logging", "title": "Logging and reading data files"}
{"content": "The API described is a Python API for OpenSesame, a software used for implementing psychology experiments. It revolves around the `Sampler` class, which provides functionality to play sound samples. Importantly, the `Sampler` class does not require importing. \n\nTo initialize a `Sampler`, you use the `Sampler()` factory function, which takes the full path to a sound file as the first argument. Optionally, you can pass Playback keywords to `Sampler()` to set the default behavior, such as volume. \n\nFunctions that accept `**playback_args` take several keyword arguments including volume, pitch, pan, duration, fade_in, and block. These arguments specify characteristics of the sound playback such as the volume between `0.0` (silent) and `1.0` (maximum), the pitch (or playback speed), the panning (left or right), duration of the sound in milliseconds, fade-in time, and whether the experiment should block (`True`) during playback or not (`False`). \n\nThe API supports sound files in `.wav`, `.mp3`, and `.ogg` format. \n\nThe available functions and their parameters are:\n\n- `close_sound(experiment)`: Closes the mixer after the experiment is finished. The parameter is the experiment object.\n- `init_sound(experiment)`: Initializes the pygame mixer before the experiment begins. The parameter is the experiment object.\n- `is_playing(self)`: Checks if a sound is currently playing. This function returns True if a sound is playing, False if not.\n- `pause(self)`: Pauses playback if any.\n- `play(*arglist, **kwdict)`: Plays the sound. Optional playback keywords will be used for this call to `Sampler.play()`. This does not affect subsequent operations.\n- `resume(self)`: Resumes playback if any.\n- `set_config(**cfg)`: Updates the configurables. The parameter is the to-be-updated configurables.\n- `stop(self)`: Stops the currently playing sound if any.\n- `wait(self)`: Blocks until the sound has finished playing or returns right away if no sound is playing.", "url": "https://osdoc.cogsci.nl/4.0/manual/python/sampler", "title": "Sampler functions"}
{"content": "title: Form variables\n\n[TOC]\n\n## About form variables\n\nWhen you present a form with multiple `checkbox`es, you generally want to know which `checkbox` the user has checked. Similarly, when you present a form with two `button`s, you want to know which `button` the user has clicked. This information is available through variables that are automatically set when the user interacts with a form. You can specify yourself which response variables should be used. How this is done depends on how you have created your form.\n\n### In ready-made form plugins\n\nWhen you use one of the ready-made form plugins, such as FORM_TEXT_INPUT, you can specify the name of the response variable directly in the plugin controls.\n\n### In custom forms\n\nYou can use the `var` keyword to indicate which variable should be used. For example, the following OpenSesame script, which you can enter into a FORM_BASE plugin, indicates that the response from a `text_input` widget should be stored in a variable called `my_response_var`:\n\n```python\nwidget 0 0 1 1 text_input var=my_response_var\n```\n\nThe equivalent Python code is:\n\n~~~ .python\nmy_widget = TextInput(var='my_response_var')\n~~~\n\nSee also:\n\n- <https://osdoc.cogsci.nl/4.0/manual/forms/widgets>\n\n## Widget-specific information\n\nEach widget uses its response variable in a slightly different way.\n\n### button\n\nThe `button` widget sets the response variable to 'yes' if it has been clicked and to 'no' if it has not.\n\n### checkbox\n\nThe `checkbox` widget sets the response variable to a semicolon-separated list of the text on all checkboxes that have been checked (for that variable), or 'no' if no `checkbox` has been checked (for that variable). This sounds a bit complicated, so let's see a few examples.\n\n```python\nwidget 0 0 1 1 checkbox group=\"1\" text=\"A\" var=\"my_response_var\"\nwidget 1 0 1 1 checkbox group=\"1\" text=\"B\" var=\"my_response_var\"\nwidget 1 1 1 1 button text=\"Next\"\n```\n\nHere there are two `checkbox`es with the text 'A' and 'B'. Both part of the same group, called '1'. Both have the same response variable, called `my_response_var`. If 'A' is checked, `my_response_var` will be 'A'. If 'B' is checked, `my_response_var` will be 'B'. If neither is checked, `my_response_var` will be 'no'. Note that only one `checkbox` in the same group can be checked, so `my_response_var` will *never* be 'A;B' in this example.\n\nNow let's consider the same script, with the sole difference that the two `checkbox`es are not part of a group:\n\n```python\nwidget 0 0 1 1 checkbox text=\"A\" var=\"my_response_var\"\nwidget 1 0 1 1 checkbox text=\"B\" var=\"my_response_var\"\nwidget 1 1 1 1 button text=\"Next\"\n```\n\nIn this case, the situation is much like described above, with the exception that both `checkbox`es can be checked at the same time, in which case `my_response_var` will be set to 'A;B'.\n\nYou cannot use the same response variable for `checkbox`es in different groups.\n\n### image\n\nVariables are not applicable to the `image` widget.\n\n### image_button\n\nThe `image_button` widget sets the response variable to 'yes' if it has been clicked and to 'no' if it has not.\n\n### label\n\nVariables are not applicable to the `label` widget.\n\n### rating_scale\n\nThe `rating_scale` widget sets the response variable to the number of the option that has been clicked, where '0' is the first option (zero-based indexing). If no option has been selected, the response variable is set to 'None'.\n\n### text_input\n\nThe `text_input` widget sets the response variable to the entered text.", "url": "https://osdoc.cogsci.nl/4.0/manual/forms/variables", "title": "Form variables"}
{"content": "The documentation discusses how to create custom forms using OpenSesame script and Python. Forms are composed of widgets such as buttons, labels, and text input fields, arranged into a grid with a specified geometry. The form's geometry is determined by properties such as margins, spacing, rows, and columns. Users can create custom forms either by using the FORM_BASE item with OpenSesame script or with Python in an INLINE_SCRIPT item. The documentation provides a detailed guide on how to use the script editor to generate a form, including setting margins, spacing, rows, and columns, and defining widgets. \n\nThemes are also available for forms, with two options: 'gray' or 'plain'. Non-interactive forms, without an input field or button, can be created by setting `only_render` to \"yes\" in OpenSesame script, or by calling `form.render()` in Python INLINE_SCRIPT. \n\nThe document provides examples of creating forms both in OpenSesame script and Python, including a questionnaire with three rating scales and a 'next' button. Finally, it provides links for further information on available widgets, keywords, and input validation.", "url": "https://osdoc.cogsci.nl/4.0/manual/forms/custom", "title": "Creating custom forms"}
{"content": "title: Color conversion functions (color-convert)\n\n\nThe `color-convert` library is available as `convert`. It provides convenient high level functions for converting from one color specification to another.\n\n__Example:__\n\n```js\nconsole.log('The RGB values for blue are ' + convert.keyword.rgb('blue'))\n```\n\nFor an overview, see:\n\n- <https://www.npmjs.com/package/color-convert>", "url": "https://osdoc.cogsci.nl/4.0/manual/javascript/color-convert", "title": "Color conversion functions (color-convert)"}
{"content": "title: EyeTribe\n\nThe EyeTribe is supported through PyGaze. For more information, see:\n\n- <https://osdoc.cogsci.nl/4.0/manual/eyetracking/pygaze>", "url": "https://osdoc.cogsci.nl/4.0/manual/eyetracking/eyetribe", "title": "EyeTribe"}
{"content": "title: Runtime for Android\n\n\n__Important note:__ The OpenSesame runtime for Android is based on software by others that is no longer developed. As a result, we are unable to make sure that the runtime works with recent versions of Android. Windows 10 tablets with Intel processors are a good alternative.\n{: .alert .alert-warning}\n\n\n[TOC]\n\n\n## OpenSesame runtime for Android\n\n### Download\n\nYou can download the OpenSesame runtime for Android through the Google Play Store:\n\n<a href=\"https://play.google.com/store/apps/details?id=nl.cogsci.opensesame\" style=\"border:none;\">\n  <img alt=\"Get it on Google Play\"\n       src=\"https://developer.android.com/images/brand/en_generic_rgb_wo_45.png\" />\n</a>\n\n### Usage\n\nWhen you start the OpenSesame runtime, you will be asked where your experiments are located. By default, OpenSesame assumes that they are in the `/sdcard/` folder, or (if it exists) in the `/sdcard/Experiments/` folder. If you have no experiments on your device, pressing `enter` will show the example experiments that are bundled with the `.apk`.\n\nThe `Back` button serves the same purpose as the `Escape` key on regular systems, and will exit OpenSesame.\n\n### Supported devices\n\nOpenSesame is developed with the Nexus 4 and 9 as reference devices. In general, any device that runs Android 2.2. 'Froyo' or later appears to work.\n\n### Disabling automatic updates\n\nIf you are using the OpenSesame runtime for Android in a production environment (e.g., while you are running an experiment), it is recommended to disable the Auto-update feature of the Google Play Store, at least for OpenSesame. This will prevent the app from being updated and potentially changing its behavior. In case you need to downgrade to a previous version of the Android runtime, you can find the `.apk` files for previous releases [here](https://github.com/smathot/OpenSesame/releases).\n\n### Automatically start an experiment\n\nIf you want to directly launch a specific experiment when the OpenSesame runtime for Android is started, you can create a file called `opensesame-autorun.yml` in the `/sdcard/` folder of your device. This is a YAML file with the following structure:\n\n~~~\nexperiment: /sdcard/experiments/my_experiment.opensesame\nsubject_nr: 3\nlogfile: /sdcard/data/subject03.csv\n~~~\n\n## Developing experiments for Android\n\n### backend\n\nThe OpenSesame runtime for Android requires the *droid* backend.\n\n", "url": "https://osdoc.cogsci.nl/4.0/manual/android", "title": "Runtime for Android"}
{"content": "### Design tips\n\nImplement most user interactions through the MOUSE_RESPONSE item or TOUCH_RESPONSE plugin. In general, screen touches are registered as mouse clicks. Using keyboard input will work as well, but it will show and hide the virtual keyboard after every key that is entered, which looks messy.\n\nThe resolution for the DROID backend is fixed at 1280x800 (landscape). On Android, your experiment will be automatically scaled up or down depending on the resolution of the device, but the resolution that you design with is always 1280x800.\n\n### Debugging\n\nDebug output is written to `/sdcard/opensesame-debug.txt`.\n\n### Limitations\n\n- The SYNTH item and `openexp.synth` module are not functional.\n- The SAMPLER item and `openexp.sampler` module will ignore panning and pitching.\n\n## Know issue: Frozen or misbehaving virtual keyboard\n\nOn some devices, the default virtual keyboard is unresponsive (i.e. it shows but doesn't respond to taps) or doesn't respond normally. This appears to happen on phones with recent versions of Android. To work around this issue, you can install a third-party keyboard. Keyboards that have been reported to work are:\n\n- [GO Keyboard](https://play.google.com/store/apps/details?id=com.jb.emoji.gokeyboard&hl=en)\n- [Smart Keyboard Trial](https://play.google.com/store/apps/details?id=net.cdeguet.smartkeyboardtrial&hl=en)\n\n## Available Python modules\n\nBelow is a list of Python modules that should be available in the OpenSesame runtime for android. (This list is copied from the pgs4a now-defunct website.)\n\n~~~\npygame\npygame.base\npygame.bufferproxy\npygame.colordict\npygame.color\npygame.compat\npygame.constants\npygame.cursors\npygame.display\npygame.draw\npygame.event\npygame.fastevent\npygame.font\npygame.gfxdraw\npygame.imageext\npygame.image\npygame.joystick\npygame.key\npygame.locals\npygame.mask\npygame.mouse\npygame.overlay\npygame.rect\npygame.rwobject\npygame.sprite\npygame.surface\npygame.surflock\npygame.sysfont\npygame.time\npygame.transform\npygame.version\n_abcoll\nabc\naliases\narray\nast\natexit\nbase64\nbisect\nbinascii\ncalendar\ncmath\ncodecs\ncollections\ncompileall\ncontextlib\ncopy\ncopy_reg\ncStringIO\ncPickle\ndatetime\ndifflib\ndis\ndummy_threading\ndummy_thread\nencodings\nencodings.raw_unicode_escape\nencodings.utf_8\nencodings.zlib_codec\nerrno\nfcntl\nfnmatch\nfunctools\n__future__\ngenericpath\ngetopt\nglob\ngzip\nhashlib\nheapq\nhttplib\ninspect\nitertools\nkeyword\nlinecache\nmath\nmd5\nmimetools\nopcode\noptparse\nos\noperator\nparser\npickle\nplatform\nposix\nposixpath\npprint\npy_compile\npwd\nQueue\nrandom\nrepr\nre\nrfc822\nselect\nsets\nshlex\nshutil\nsite\nsocket\nsre_compile\nsre_constants\nsre_parse\nssl\nstat\nStringIO\nstring\nstruct\nsubprocess\nsymbol\nsymtable\nstrop\ntarfile\ntempfile\ntextwrap\n_threading_local\nthreading\ntime\ntokenize\ntoken\ntraceback\ntypes\nurllib\nurllib2\nurlparse\nUserDict\nwarnings\nweakref\nwebbrowser\nzipfile\nzipimport\nzlib\n~~~\n\n[google-play]: https://play.google.com/store/apps/details?id=nl.cogsci.opensesame\n[forum]: http://forum.cogsci.nl/index.php?p=/discussion/333/a-video-of-opensesame-running-natively-on-android\n[droid]: /backends/droid\n[pgs4a]: http://pygame.renpy.org/", "url": "https://osdoc.cogsci.nl/4.0/manual/android", "title": "Runtime for Android"}
{"content": "The document is a detailed guide about OSWeb, an online runtime for OpenSesame experiments. OSWeb is a JavaScript library that allows OpenSesame experiments to be executed in a web browser. To use OSWeb, the opensesame-extension-osweb package is necessary, which is pre-installed with the Windows and macOS distributions of OpenSesame.\n\nThe document provides steps on how to run an experiment in a web browser using OSWeb. It highlights the process of opening the Experiment Properties, selecting 'In a browser with OSWeb', and running the experiment. It notifies that if the experiment isn't compatible with OSWeb, an error message will pop up detailing the compatibility issues. If there are no compatibility issues, the experiment will run in a new browser window. Once the experiment is finished, the data can be downloaded in .json format and be converted to .xlsx or .csv format for further analysis.\n\nAdditionally, the document details how to access the OSWeb and JATOS control panel from the Tools menu. The panel offers configuration options such as selecting subject numbers, making the browser fullscreen, showing the OSWeb Welcome Screen, bypassing the compatibility check, customizing the welcome text, and specifying external libraries.\n\nThe document also outlines the functionality supported by OSWeb when running an experiment from within OpenSesame and details how to include external JavaScript packages. Lastly, it provides a link for debugging related issues.", "url": "https://osdoc.cogsci.nl/4.0/manual/osweb/osweb", "title": "OSWeb"}
{"content": "The documentation provides an explanation on the concept of counterbalancing in experimental design and how to implement it using OpenSesame. Counterbalancing helps remove confounding factors by assigning slightly different tasks to different participant groups. Two examples of counterbalancing are provided: counterbalancing response rule and rotating stimulus conditions. \n\nThe response rule example suggests assigning 'z' and 'm' keys for verbs and nouns respectively for even participant numbers, and the opposite for odd participant numbers. The rotating stimulus conditions example suggests changing the condition in which each word occurs between participants.\n\nTo implement counterbalancing, the document suggests using the subject number where the parity of the subject number determines the response rule. An alternative method is using Batch Session Data when running an OSWeb experiment hosted on JATOS. This method allows data sharing between experimental sessions and enables distribution of conditions across participants. A template experiment for this method is provided.\n\nThe document also covers the process of setting up conditions in JATOS, monitoring the progress of sessions, and manually adding a new condition if a session is not finished.", "url": "https://osdoc.cogsci.nl/4.0/manual/counterbalancing", "title": "Counterbalancing"}
{"content": "title: Validating form input\n\n\nTo validate a form, pass a function with the `validator` keyword to `Form()`. In the example below, `my_form_validator()` is used in this way. A validator function should not expect any arguments, and should return a `bool` to indicate whether or not the form validates. If the form does not validate, no error message is shown, but the form simply stays open.\n\nIn addition, you can validate (or filter) input to a `TextInput` widget to exclude certain characters as input. To do so, pass a function with the `key_filter` keyword to `TextInput()`. In the example below, `filter_digits()` is used in this way. A key-filter function should accept a single argument, which corresponds to a single key press, and should return a `bool` to indicate whether or not the key is accepted as input.\n\n~~~ .python\ndef my_form_validator():\n    \"\"\"Checks whether both the gender and age fields have been filled out\"\"\"\n    return gender != 'no' and age != ''\n\n\ndef filter_digits(ch):\n    \"\"\"Allows only digit characters as input\"\"\"\n    return ch in '0123456789'\n\n\n# Define all widgets\nbutton_ok = Button(text='Ok')\nlabel_gender= Label('Your gender')\ncheckbox_male = Checkbox(text='Male', group='gender', var='gender')\ncheckbox_female = Checkbox(text='Female', group='gender', var='gender')\nlabel_age = Label('Your age')\n# Specify a key filter so that only digits are accepted as text input\ninput_age = TextInput(stub='Age here \u2026', var='age', key_filter=filter_digits)\n# Build the form. Specify a validator function to make sure that the form is\n# completed.\nmy_form = Form(validator=my_form_validator, rows=[1,1,1], cols=[1,1,1])\nmy_form.set_widget(label_gender, (0, 0))\nmy_form.set_widget(checkbox_male, (1, 0))\nmy_form.set_widget(checkbox_female, (2, 0))\nmy_form.set_widget(label_age, (0, 1))\nmy_form.set_widget(input_age, (1, 1), colspan=2)\nmy_form.set_widget(button_ok, (1, 2))\nmy_form._exec()\n~~~", "url": "https://osdoc.cogsci.nl/4.0/manual/forms/validation", "title": "Validating form input"}
