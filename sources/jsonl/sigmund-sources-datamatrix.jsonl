{"content": "title: datamatrix.convert\n\nA set of functions to convert `DataMatrix` and column objects to and from other data structures, notable `pandas.DataFrame`, `mne.Epochs()`, and JSON strings. This module is typically imported as `cnv` for brevity:\n\n\n\n~~~ .python\nfrom datamatrix import convert as cnv\n\n~~~\n\n\n\n[TOC]\n\n<div class=\" YAMLDoc\" id=\"\" markdown=\"1\">\n\n \n\n<div class=\"FunctionDoc YAMLDoc\" id=\"from_json\" markdown=\"1\">\n\n## function __from\\_json__\\(s\\)\n\n*Requires json_tricks*\n\nCreates a DataMatrix from a `json` string.\n\n__Arguments:__\n\n- `s` -- A json string.\n\t- Type: str\n\n__Returns:__\n\nA DataMatrix.\n\n- Type: DataMatrix.\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"from_mne_epochs\" markdown=\"1\">\n\n## function __from\\_mne\\_epochs__\\(epochs, ch\\_avg=False\\)\n\n[Python MNE](https://mne.tools/) is a library for analysis of\nneurophysiological data.\n\nThis function converts an `mne.Epochs()` object to a multidimensional\ncolumn. The column's metadata is set to the `epochs.info` property. The\nlength of the datamatrix should match the length of the metadata that\nwas passed to `mne.Epochs()`. Rejected epochs result in `nan` values\nin the corresponding rows of the column.\n\nIf channels are averaged, the shape of the resulting column is\ntwo-dimensional, where the first dimension is the number of rows and\nthe second dimension is sample time.\n\nIf channels not are averaged, the shape of the resulting column is\nthree-dimensional, where the first dimension is the number of rows,\nthe second dimension is the channel (which can be referenced by name),\nand the third dimension is sample time.\n\n*Version note:* New in 1.0.0\n\n\n~~~ .python\nimport numpy as np\nimport pickle\nimport mne\nfrom matplotlib import pyplot as plt\nfrom datamatrix import operations as ops, convert as cnv\n\n# First read a simple dataset that contains data from three occipital EEG\n# channels (O1, O2, Oz). `events` is an mne style array with event codes\n# and timestamps. `dm` is a datamatrix with trial information.\nwith open('data/eeg-data.pkl', 'rb') as fd:\n    raw, events, dm = pickle.loads(fd.read())\n# Create an Epochs object and convert it to a multidimensional column (dm.erp)\nepochs = mne.Epochs(raw, events, tmin=-.05, tmax=1.5,\n                    metadata=cnv.to_pandas(dm))\ndm.erp = cnv.from_mne_epochs(epochs)\n# The last dimension corresponds to time, and the `index_names` property\n# contains the timestamps in seconds.\nchannels = dm.erp.index_names[0]\ntimestamps = dm.erp.index_names[1]\n# Split the data by stimulus intensity, and plot the mean signal over time\nplt.title(f'channels = {channels}')\nfor intensity, idm in ops.split(dm.intensity):\n    # Average over trials and channels, but not time\n    plt.plot(timestamps, idm.erp[..., ...], label=str(intensity))\nplt.legend(title='Stimulus intensity')\nplt.xlabel('Time (s)')\nplt.ylabel('Voltage')\nplt.show()\n\n~~~\n\n__Output:__\n\n~~~ .text\nAdding metadata with 2 columns\n400 matching events found\nSetting baseline interval to [-0.048, 0.0] sec\nApplying baseline correction (mode: mean)\n0 projection items activated\nUsing data from preloaded Raw for 400 events and 388 original time points ...\n47 bad epochs dropped\n\n~~~\n\n![](/1.0/img/1.png)\n\n\n__Arguments:__\n\n- `epochs` -- No description\n\t- Type: mne.Epochs\n\n__Keywords:__\n\n- `ch_avg` -- Determines whether the epochs should be averaged across channels or not.\n\t- Type: bool\n\t- Default: False\n\n__Returns:__\n\nNo description\n\n- Type: MultiDimensionalColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"from_mne_tfr\" markdown=\"1\">\n\n", "url": "https://pydatamatrix.eu/1.0/convert", "title": "datamatrix.convert"}
{"content": "## function __from\\_mne\\_tfr__\\(tfr, ch\\_avg=False, freq\\_avg=False\\)\n\n[Python MNE](https://mne.tools/) is a library for analysis of\nneurophysiological data.\n\nThis function converts an `mne.EpochsTFR()` object to a \nmultidimensional column. The column's metadata is set to the\n`tfr.info` property. The length of the datamatrix should match the \nlength of the metadata that was passed to `mne.Epochs()`. Rejected \nepochs result in `nan` values in the corresponding rows of the column.\n\nIf both channels and frequencies are averaged, the shape of the\nresulting column is two-dimensional, where the first dimension is the \nnumber of rows and the second dimension is sample time.\n\nIf only channels are averaged, the shape of the resulting column is\nthree-dimensional, where the first dimension is the number of rows, the\nsecond dimension is frequency, and the third dimension is sample time.\n\nIf only frequencies are averaged, the shape of the resulting column is\nthree-dimensional, where the first dimension is the number of rows, the\nsecond dimension is channel (which can be referenced by name), and the\nthird dimension is sample time.\n\nIf nothing is averaged, the shape of the resulting column is\nfour-dimensional, where the first dimension is the number of rows,\nthe second dimension is the channel (which can be referenced by name),\nthe third dimension is frequency, and the fourth dimension is sample\ntime.\n\n*Version note:* New in 1.0.0\n\n\n~~~ .python\nimport numpy as np\nimport pickle\nimport mne\nfrom mne.time_frequency import tfr_morlet\nfrom matplotlib import pyplot as plt\nfrom datamatrix import operations as ops, convert as cnv\n\n# First read a simple dataset that contains data from three occipital EEG\n# channels (O1, O2, Oz). `events` is an mne style array with event codes\n# and timestamps. `dm` is a datamatrix with trial information.\nwith open('data/eeg-data.pkl', 'rb') as fd:\n    raw, events, dm = pickle.loads(fd.read())\n# Create an Epochs object. From there, create a TFR object. From there, create\n# a multidimensional column.\nepochs = mne.Epochs(raw, events, tmin=-.5, tmax=2, metadata=cnv.to_pandas(dm))\ntfr = tfr_morlet(epochs, freqs=np.arange(4, 30, 1), n_cycles=2,\n                 return_itc=False, average=False)\ntfr.crop(0, 1.5)\ndm.tfr = cnv.from_mne_tfr(tfr)\n# Plot the \nchannels = dm.tfr.index_names[0]\nplt.title(f'channels = {channels}')\n", "url": "https://pydatamatrix.eu/1.0/convert", "title": "datamatrix.convert"}
{"content": "# Average over trials and channels, but not frequency or time.\nplt.imshow(dm.tfr[..., ...], aspect='auto')\nplt.xticks(dm.tfr.index_values[2][::100], dm.tfr.index_names[2][::100])\nplt.yticks(dm.tfr.index_values[1][::4], dm.tfr.index_names[1][::4])\nplt.xlabel('Time (s)')\nplt.ylabel('Frequency (Hz)')\nplt.show()\n\n~~~\n\n__Output:__\n\n~~~ .text\nSetting baseline interval to [-0.5, 0.0] sec\nUsing data from preloaded Raw for 400 events and 626 original time points ...\n50 bad epochs dropped\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n\u001b[32m\u2839\u001b[0m Generating...[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n\u001b[32m\u2827\u001b[0m Generating...[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.7s remaining:    0.0s\n\u001b[32m\u2819\u001b[0m Generating...[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.0s remaining:    0.0s\n\u001b[32m\u2834\u001b[0m Generating...[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.0s finished\n\u001b[32m\u2834\u001b[0m Generating...\n~~~\n\n![](/1.0/img/2.png)\n\n\n__Arguments:__\n\n- `tfr` -- No description\n\t- Type: mne.EpochsTFR\n\n__Keywords:__\n\n- `ch_avg` -- Determines whether the data should be averaged across channels or not.\n\t- Type: bool\n\t- Default: False\n- `freq_avg` -- Determines whether the data should be averaged across frequencies or not.\n\t- Type: bool\n\t- Default: False\n\n__Returns:__\n\nNo description\n\n- Type: MultiDimensionalColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"from_pandas\" markdown=\"1\">\n\n## function __from\\_pandas__\\(df\\)\n\nConverts a pandas DataFrame to a DataMatrix.\n\n__Example:__\n\n\n~~~ .python\nimport pandas as pd\nfrom datamatrix import convert\n\ndf = pd.DataFrame( {'col' : [1,2,3] } )\ndm = convert.from_pandas(df)\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+\n| # | col |\n+---+-----+\n| 0 |  1  |\n| 1 |  2  |\n| 2 |  3  |\n+---+-----+\n\n~~~\n\n\n__Arguments:__\n\n- `df` -- No description\n\t- Type: DataFrame\n\n__Returns:__\n\nNo description\n\n- Type: DataMatrix\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"to_json\" markdown=\"1\">\n\n## function __to\\_json__\\(dm\\)\n\n*Requires json_tricks*\n\nCreates (serializes) a `json` string from a DataMatrix.\n\n__Arguments:__\n\n- `dm` -- A DataMatrix to serialize.\n\t- Type: DataMatrix\n\n__Returns:__\n\nA json string.\n\n- Type: str\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"to_pandas\" markdown=\"1\">\n\n## function __to\\_pandas__\\(obj\\)\n\nConverts a DataMatrix to a pandas DataFrame, or a column to a Series.\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, convert\n\ndm = DataMatrix(length=3)\ndm.col = 1, 2, 3\ndf = convert.to_pandas(dm)\nprint(df)\n\n~~~\n\n__Output:__\n\n~~~ .text\n   col\n0    1\n1    2\n2    3\n\n~~~\n\n\n__Arguments:__\n\n- `obj` -- No description\n\t- Type: DataMatrix, BaseColumn\n\n__Returns:__\n\nNo description\n\n- Type: DataFrame, Series\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"wrap_pandas\" markdown=\"1\">\n\n## function __wrap\\_pandas__\\(fnc\\)\n\nA decorator for pandas functions. It converts a DataMatrix to a DataFrame, passes it to a function, and then converts the returned DataFrame back to a DataMatrix.\n\n__Example:__\n\n~~~ .python\nimport pandas as pd\nfrom datamatrix import convert as cnv\n\npivot_table = cnv.wrap_pandas(pd.pivot_table)\n~~~\n\n__Arguments:__\n\n- `fnc` -- A function that takes a DataFrame as first argument and returns a DataFrame as sole return argument.\n\t- Type: callable\n\n__Returns:__\n\nA function takes a DataMatrix as first argument and returns a DataMatrix as sole return argument.\n\n</div>\n\n</div>\n\n\n", "url": "https://pydatamatrix.eu/1.0/convert", "title": "datamatrix.convert"}
{"content": "title: Memoization (caching)\n\n[TOC]\n\n\n## What is memoization?\n\n[Memoization](https://en.wikipedia.org/wiki/Memoization) is a way to optimize code by storing the return values of functions called with a specific set of arguments. Memoization is a specific type of caching.\n\n\n## When (not) to memoize?\n\nMemoization is only valid for functions that are *referentially transparent*: functions that always return the same result for the same set of arguments, and that do not affect the state of the program.\n\nTherefore, you should *not* memoize a function that returns random numbers, because it will end up returning the same set of random numbers over and over again. And you should also *not* memoize a function that depends on the state of the program, for example because it relies on the command-line arguments that were passed to a script.\n\nBut you *could* memoize a function that performs some time consuming operation that is always done in exactly the same way, such as a function that performs time-consuming operations on a large dataset.\n\n\n## Examples\n\n### Basic memoization\n\nMemoization is done with the `memoize` decorator, which is part of [`datamatrix.functional`](<https://pydatamatrix.eu/1.0/functional>). Let's take a time-consuming function that determines the highest prime number below a certain value, and measure the performance improvement that memoization gives us when we call the function twice with same argument.\n\n\n~~~ .python\nimport time\nfrom itertools import dropwhile\nfrom datamatrix import functional as fnc\n\n\n@fnc.memoize\ndef prime_below(x):\n\n\t\"\"\"Returns the highest prime that is lower than X.\"\"\"\n\n\tprint('Calculating the highest prime number below %d' % x)\n\treturn next(\n\t\tdropwhile(\n\t\t\tlambda x: any(not x % i for i in range(x-1, 2, -1)),\n\t\t\trange(x-1, 0, -1)\n\t\t\t)\n\t\t)\n\n\nt0 = time.time()\nprime_below(10000)\nt1 = time.time()\nprime_below(10000)\nt2 = time.time()\n\nprint('Fresh: %.2f ms' % (1000*(t1-t0)))\nprint('Memoized: %.2f ms' % (1000*(t2-t1)))\n\n~~~\n\n__Output:__\n\n~~~ .text\nCalculating the highest prime number below 10000\nFresh: 10.69 ms\nMemoized: 0.24 ms\n\n~~~\n\n\n\n", "url": "https://pydatamatrix.eu/1.0/memoization", "title": "Memoization (caching)"}
{"content": "### Chaining memoized functions and lazy evaluation\n\nWhen you call a function, Python automatically evaluates the function arguments. This happens even if a function has been memoized. In some cases, this is undesirable because evaluating the arguments may be time-consuming in itself, for example because one of the arguments is a call to another time-consuming function.\n\nIdeally, evaluation of the arguments occurs only when the memoized function actually needs to be executed. To approximate this behavior in Python, the `memoize` decorator accepts the `lazy` keyword. When `lazy=True` is specified, all callable objects that are passed to the memoized function are evaluated automatically, but *only* when the memoized function is actually executed.\n\n\n~~~ .python\n@fnc.memoize(lazy=True)\ndef prime_below(x):\n\n\tprint('Calculating the highest prime number below %d' % x)\n\treturn next(\n\t\tdropwhile(\n\t\t\tlambda x: any(not x % i for i in range(x-1, 2, -1)),\n\t\t\trange(x-1, 0, -1)\n\t\t\t)\n\t\t)\n\n\ndef thousand():\n\n\tprint('Returning a thousand!')\n\treturn 1000\n\n\nprint(prime_below(thousand))\nprint(prime_below(thousand))\n\n~~~\n\n__Output:__\n\n~~~ .text\nReturning a thousand!\nCalculating the highest prime number below 1000\n997\n997\n\n~~~\n\n\n\nA slightly more complicated situation arises when you want to pass arguments to a function that is itself passed as argument, without evaluating the function. To accomplish this, you can first bind the argument to the function using `functools.partial` and then pass the resulting partial function as an argument. Like so:\n\n\n~~~ .python\nfrom functools import partial\n\nprint(prime_below(partial(prime_below, 1000)))\nprint(prime_below(partial(prime_below, 1000)))\n\n~~~\n\n__Output:__\n\n~~~ .text\nCalculating the highest prime number below 1000\nCalculating the highest prime number below 997\n991\n991\n\n~~~\n\n\nYou can also implement this behavior with the `>>` operator, in which the resulting of one function call is fed into the next function call, etc. The result is a `chain` object that needs to be explicitly called. The `>>` only works\nwith lazy memoization.\n\n\n~~~ .python\nchain = 1000 >> prime_below >> prime_below\nprint(chain())\nprint(chain())\n\n~~~\n\n__Output:__\n\n~~~ .text\n991\n991\n\n~~~\n\n\n\n### Persistent memoization, memoization keys, and cache clearing\n\nIf you pass `persistent=True` to the `memoize` decorator, the cache will be written to disk, by default to a subfolder `.memoize` of the current working directory. The filename will correspond to the memoization key, which by default is derived from the function name and the arguments.\n\nIf you want to change the cache folder, you can either pass a `folder` keyword to the `memoize` decorator, or change the `memoize.folder` class property before applying the `memoize` decorator to any functions.\n\nYou can also specify a custom memoization key through the `key` keyword. If you specify a custom key, `memoize` will no longer distinguish between different arguments (and thus no longer be real `memoization`).\n\nTo re-execute a memoized function, you can clear the memoization cache by calling the `.clear()` method on the memoized function, as shown below. This will clear the cache only for the next function call.\n\n\n\n~~~ .python\n@fnc.memoize(persistent=True, key='custom-key')\ndef prime_below(x):\n\n\tprint('Calculating the highest prime number below %d' % x)\n\treturn next(\n\t\tdropwhile(\n\t\t\tlambda x: any(not x % i for i in range(x-1, 2, -1)),\n\t\t\trange(x-1, 0, -1)\n\t\t\t)\n\t\t)\n\n\nprint(prime_below(1000))\nprint(prime_below(1000))\nprime_below.clear() # Clear the cache\nprint(prime_below(1000))\n\n~~~\n\n__Output:__\n\n~~~ .text\nCalculating the highest prime number below 1000\n997\n997\nCalculating the highest prime number below 1000\n997\n\n~~~\n\n\n\n## Limitations\n\nMemoization only works for functions with:\n\n- Arguments and keywords that:\n\t- Can be serialized by `json_tricks`, which includes simple data types, DataMatrix objects, and numpy array; or\n\t- Are callable, which includes regular functions, `lambda` expressions, `partial` objects, and `memoize` objects.\n- Return values that can be pickled.\n", "url": "https://pydatamatrix.eu/1.0/memoization", "title": "Memoization (caching)"}
{"content": "title: Page not found\n\nThe page that you requested does not exist. Perhaps it has moved.\n\nTo find the information you're looking for:\n\n- Browse the menu at the top of this page; or\n- Search using the search bar at the left of this page.\n", "url": "https://pydatamatrix.eu/1.0/notfound", "title": "Page not found"}
{"content": "title: DataMatrix\n\n`DataMatrix` is an intuitive Python library for working with column-based, time-series, and multidimensional data. It's a light-weight and easy-to-use alternative to `pandas`.\n\n<div class=\"btn-group\" role=\"group\" aria-label=\"...\">\n  <a role=\"button\" class=\"btn btn-success\" href=\"https://pydatamatrix.eu/1.0/install\">\n\t\t<span class=\"glyphicon glyphicon-download\" aria-hidden=\"true\"></span>\n\t\tInstall\n\t </a>\n  <a role=\"button\" class=\"btn btn-success\" href=\"https://pydatamatrix.eu/1.0/basic\">\n  <span class=\"glyphicon glyphicon-education\" aria-hidden=\"true\"></span>\n  \tBasic use\n  </a>\n  <a role=\"button\" class=\"btn btn-success\" href=\"https://professional.cogsci.nl/\">\n  <span class=\"glyphicon glyphicon-comment\" aria-hidden=\"true\"></span>\n  Get support</a>\n</div>\n\n\n## Features\n\n- [An intuitive syntax](<https://pydatamatrix.eu/1.0/basic>) that makes your code easy to read\n- Mix tabular data with [time series](<https://pydatamatrix.eu/1.0/series>) and [multidimensional data](%link:multidimensional) in a single data structure\n- Support for [large data](<https://pydatamatrix.eu/1.0/largedata>) by intelligent (and automatic) offloading of data to disk when memory is running low\n- Advanced [memoization (caching)](<https://pydatamatrix.eu/1.0/memoization>)\n- Requires only the Python standard libraries (but you can use `numpy` to improve performance)\n- Compatible with your favorite data-science libraries:\n    - `seaborn` and `matplotlib` for [plotting](https://pythontutorials.eu/numerical/plotting)\n    - `scipy`, `statsmodels`, and `pingouin` for [statistics](https://pythontutorials.eu/numerical/statistics)\n    - `mne` for analysis of electroencephalographic (EEG) and magnetoencephalographic (MEG) data\n    - [Convert](<https://pydatamatrix.eu/1.0/convert>) to and from `pandas.DataFrame`\n    - Looks pretty inside a Jupyter Notebook\n\n\n## Ultra-short cheat sheet\n\n~~~python\nfrom datamatrix import DataMatrix, io\n# Read a DataMatrix from file\ndm = io.readtxt('data.csv')\n# Create a new DataMatrix\ndm = DataMatrix(length=5)\n# The first two rows\nprint(dm[:2])\n# Create a new column and initialize it with the Fibonacci series\ndm.fibonacci = 0, 1, 1, 2, 3\n# You can also specify column names as if they are dict keys\ndm['fibonacci'] = 0, 1, 1, 2, 3\n# Remove 0 and 3 with a simple selection\ndm = (dm.fibonacci > 0) & (dm.fibonacci < 3)\n# Get a list of indices that match certain criteria\nprint(dm[(dm.fibonacci > 0) & (dm.fibonacci < 3)])\n# Select 1, 1, and 2 by matching any of the values in a set\ndm = dm.fibonacci == {1, 2}\n# Select all odd numbers with a lambda expression\ndm = dm.fibonacci == (lambda x: x % 2)\n# Change all 1s to -1\ndm.fibonacci[dm.fibonacci == 1] = -1\n# The first two cells from the fibonacci column\nprint(dm.fibonacci[:2])\n# Column mean\nprint('Mean: %s' % dm.fibonacci.mean)\n# Multiply all fibonacci cells by 2\ndm.fibonacci_times_two = dm.fibonacci * 2\n# Loop through all rows\nfor row in dm:\n    print(row.fibonacci) # get the fibonacci cell from the row\n# Loop through all columns\nfor colname, col in dm.columns:\n    for cell in col: # Loop through all cells in the column\n        print(cell) # do something with the cell\n# Or just see which columns exist\nprint(dm.column_names)\n~~~\n", "url": "https://pydatamatrix.eu/1.0/index", "title": "DataMatrix"}
{"content": "title: Basic use\n\n\n[TOC]\n\n\n## Ultra-short cheat sheet\n\n~~~python\nfrom datamatrix import DataMatrix, io\n# Read a DataMatrix from file\ndm = io.readtxt('data.csv')\n# Create a new DataMatrix\ndm = DataMatrix(length=5)\n# The first two rows\nprint(dm[:2])\n# Create a new column and initialize it with the Fibonacci series\ndm.fibonacci = 0, 1, 1, 2, 3\n# You can also specify column names as if they are dict keys\ndm['fibonacci'] = 0, 1, 1, 2, 3\n# Remove 0 and 3 with a simple selection\ndm = (dm.fibonacci > 0) & (dm.fibonacci < 3)\n# Get a list of indices that match certain criteria\nprint(dm[(dm.fibonacci > 0) & (dm.fibonacci < 3)])\n# Select 1, 1, and 2 by matching any of the values in a set\ndm = dm.fibonacci == {1, 2}\n# Select all odd numbers with a lambda expression\ndm = dm.fibonacci == (lambda x: x % 2)\n# Change all 1s to -1\ndm.fibonacci[dm.fibonacci == 1] = -1\n# The first two cells from the fibonacci column\nprint(dm.fibonacci[:2])\n# Column mean\nprint(dm.fibonacci[...])\n# Multiply all fibonacci cells by 2\ndm.fibonacci_times_two = dm.fibonacci * 2\n# Loop through all rows\nfor row in dm:\n    print(row.fibonacci) # get the fibonacci cell from the row\n# Loop through all columns\nfor colname, col in dm.columns:\n    for cell in col: # Loop through all cells in the column\n        print(cell) # do something with the cell\n# Or just see which columns exist\nprint(dm.column_names)\n~~~\n\n__Important note:__ Because of a limitation (or feature, if you will) of the Python language, the behavior of `and`, `or`, and chained (`x < y < z`) comparisons cannot be modified. These therefore do not work with `DataMatrix` objects as you would expect them to:\n\n~~~python\n# INCORRECT: The following does *not* work as expected\ndm = dm.fibonacci > 0 and dm.fibonacci < 3\n# INCORRECT: The following does *not* work as expected\ndm = 0 < dm.fibonacci < 3\n# CORRECT: Use the '&' operator\ndm = (dm.fibonacci > 0) & (dm.fibonacci < 3)\n~~~\n\nSlightly longer cheat sheet:\n\n\n## Creating a DataMatrix\n\nCreate a new `DataMatrix` object with a length (number of rows) of 2, and add a column (named `col`). By default, the column is of the `MixedColumn` type, which can store numeric, string, and `None` data.\n\n\n\n~~~ .python\nimport sys\nfrom datamatrix import DataMatrix, __version__\ndm = DataMatrix(length=2)\ndm.col = '\u263a'\nprint('DataMatrix v{} on Python {}\\n'.format(__version__, sys.version))\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\nDataMatrix v1.0.3 on Python 3.10.5 | packaged by conda-forge | (main, Jun 14 2022, 07:04:59) [GCC 10.3.0]\n\n+---+-----+\n| # | col |\n+---+-----+\n| 0 |  \u263a  |\n| 1 |  \u263a  |\n+---+-----+\n\n~~~\n\n\n\nYou can change the length of the `DataMatrix` later on. If you reduce the length, data will be lost. If you increase the length, empty cells (by default containing empty strings) will be added.\n\n\n\n~~~ .python\ndm.length = 3\n\n~~~\n\n\n\n## Reading and writing files\n\nYou can read and write files with functions from the `datamatrix.io` module. The main supported file types are `csv` and `xlsx`.\n\n\n\n~~~ .python\nfrom datamatrix import io\n\ndm = DataMatrix(length=3)\ndm.col = 1, 2, 3\n# Write to disk\nio.writetxt(dm, 'my_datamatrix.csv')\nio.writexlsx(dm, 'my_datamatrix.xlsx')\n# And read it back from disk!\ndm = io.readtxt('my_datamatrix.csv')\ndm = io.readxlsx('my_datamatrix.xlsx')\n\n~~~\n\n\n\nMultidimensional columns cannot be saved to `csv` or `xlsx` format but instead need to be saved to a custom binary format.\n\n```\nfrom datamatrix import MultiDimensionalColumn\ndm.mdim_col = MultiDimensionalColumn(shape=2)\n# Write to disk\nio.writebin(dm, 'my_datamatrix.dm')\n# And read it back from disk!\ndm = io.readbin('my_datamatrix.dm')\n```\n\n\n## Stacking (vertically concatenating) DataMatrix objects\n\nYou can stack two `DataMatrix` objects using the `<<` operator. Matching columns will be combined. (Note that row 2 is empty. This is because we have increased the length of `dm` in the previous step, causing an empty row to be added.)\n\n\n\n\n~~~ .python\ndm2 = DataMatrix(length=2)\ndm2.col = '\u263a'\ndm2.col2 = 10, 20\ndm3 = dm << dm2\nprint(dm3)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+------+\n| # | col | col2 |\n+---+-----+------+\n| 0 |  1  |      |\n| 1 |  2  |      |\n| 2 |  3  |      |\n| 3 |  \u263a  |  10  |\n| 4 |  \u263a  |  20  |\n+---+-----+------+\n\n~~~\n\n\n\nPro-tip: To stack three or more `DataMatrix` objects, using [the `stack()` function from the `operations` module](%url:operations) is faster than iteratively using the `<<` operator.\n\n\n\n~~~ .python\nfrom datamatrix import operations as ops\ndm4 = ops.stack(dm, dm2, dm3)\n\n~~~\n\n\n\n## Working with columns\n\n### Referring to columns\n\nYou can refer to columns in two ways: as keys in a `dict` or as properties. The two notations are identical for most purposes. The main reason to use a `dict` style is when the name of the column is itself variable. Otherwise, the property style is recommended for clarity.\n\n\n\n~~~ .python\ndm['col']  # dict style\ndm.col     # property style\n\n~~~\n\n\n\n### Creating columns\n\nBy assigning a value to a non-existing colum, a new column is created and initialized to this value.\n\n\n\n~~~ .python\ndm.col = 'Another value'\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+---------------+\n| # |      col      |\n+---+---------------+\n| 0 | Another value |\n| 1 | Another value |\n| 2 | Another value |\n+---+---------------+\n\n~~~\n\n\n\n\n### Renaming columns\n\n\n\n\n~~~ .python\ndm.rename('col', 'col2')\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+---------------+\n| # |      col2     |\n+---+---------------+\n| 0 | Another value |\n| 1 | Another value |\n| 2 | Another value |\n+---+---------------+\n\n~~~\n\n\n\n### Deleting columns\n\nYou can delete a column using the `del` keyword:\n\n\n\n\n~~~ .python\ndm.col = 'x'\ndel dm.col2\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+\n| # | col |\n+---+-----+\n| 0 |  x  |\n| 1 |  x  |\n| 2 |  x  |\n+---+-----+\n\n~~~\n\n\n\n### Column types\n\nThere are five column types:\n\n- `MixedColumn` is the default column type. This can contain numbers (`int` and `float`), strings (`str`), and `None` values. This column type is flexible but not very fast because it is (mostly) implemented in pure Python, rather than using `numpy`, which is the basis for the other columns. The default value for empty cells is an empty string.\n- `FloatColumn` contains `float` numbers. The default value for empty cells is `NAN`.\n- `IntColumn` contains `int` numbers. (This does not include `INF`, and `NAN`, which are of type `float` in Python.) The default value for empty cells is 0.\n- `MultiDimensionalColumn` contains higher-dimensional `float` arrays. This allows you to mix higher-dimensional data, such as time series or images, with regular one-dimensional data. The default value for empty cells is `NAN`.\n- `SeriesColumn` is identical to a two-dimensional `MultiDimensionalColumn`.\n\nWhen you create a `DataMatrix`, you can indicate a default column type.\n\n\n\n~~~ .python\n# Create IntColumns by default\ndm = DataMatrix(length=2, default_col_type=int)\ndm.i = 1, 2  # This is an IntColumn\n\n~~~\n\n\n\nYou can also explicitly indicate the column type when creating a new column. \n\n\n\n~~~ .python\ndm.f = float  # This creates an empty (`NAN`-filled) FloatColumn\ndm.i = int    # This creates an empty (0-filled) IntColumn\n\n~~~\n\n\n\nTo create a `MultiDimensionalColumn` you need to import the column type and specify a shape:\n\n\n\n~~~ .python\nfrom datamatrix import MultiDimensionalColumn\ndm.mdim_col = MultiDimensionalColumn(shape=(2, 3))\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+---+-----------------+\n| # |  f  | i |     mdim_col    |\n+---+-----+---+-----------------+\n| 0 | nan | 0 |  [[nan nan nan] |\n|   |     |   |  [nan nan nan]] |\n| 1 | nan | 0 |  [[nan nan nan] |\n|   |     |   |  [nan nan nan]] |\n+---+-----+---+-----------------+\n\n~~~\n\n\n\nYou can also specify named dimensions. For example, `('x', 'y')` creates a dimension of size 2 where index 0 can be referred to as 'x' and index 1 can be referred to as 'y':\n\n\n\n~~~ .python\ndm.mdim_col = MultiDimensionalColumn(shape=(('x', 'y'), 3))\n\n~~~\n\n\n\n\n### Column properties\n\nBasic numerical properties, such as the mean, can be accessed directly. For this purpose, only numerical, non-`NAN` values are taken into account.\n\n\n\n\n~~~ .python\ndm = DataMatrix(length=3)\ndm.col = 1, 2, 'not a number'\n# Numeric descriptives\nprint('mean: %s' % dm.col.mean)  #  or dm.col[...]\nprint('median: %s' % dm.col.median)\nprint('standard deviation: %s' % dm.col.std)\nprint('sum: %s' % dm.col.sum)\nprint('min: %s' % dm.col.min)\nprint('max: %s' % dm.col.max)\n# Other properties\nprint('unique values: %s' % dm.col.unique)\nprint('number of unique values: %s' % dm.col.count)\nprint('column name: %s' % dm.col.name)\n\n~~~\n\n__Output:__\n\n~~~ .text\nmean: 1.5\nmedian: 1.5\nstandard deviation: 0.7071067811865476\nsum: 3.0\nmin: 1.0\nmax: 2.0\nunique values: [1, 2, 'not a number']\nnumber of unique values: 3\ncolumn name: col\n\n~~~\n\n\n\nThe `shape` property indicates the number and sizes of the dimensions of the column. For regular columns, the shape is a tuple containing only the length of the datamatrix (the number of rows). For multidimensional columns, the shape is a tuple containing the length of the datamatrix and the shape of cells as specified through the `shape` keyword.\n\n\n\n~~~ .python\nprint(dm.col.shape)\ndm.mdim_col = MultiDimensionalColumn(shape=(2, 4))\nprint(dm.mdim_col.shape)\n\n~~~\n\n__Output:__\n\n~~~ .text\n(3,)\n(3, 2, 4)\n\n~~~\n\n\n\nThe `loaded` property indicates whether a column is currently stored in memory, or whether it is offloaded to disk. This is mainly relevant for multidimensional columns, which are [automatically offloaded to disk when memory runs low](<https://pydatamatrix.eu/1.0/largedata>).\n\n\n\n~~~ .python\nprint(dm.mdim_col.loaded)\n\n~~~\n\n__Output:__\n\n~~~ .text\nTrue\n\n~~~\n\n\n\n\n## Assigning\n\n### Assigning by index, multiple indices, or slice\n\nYou can assign a single value to one or more cells in various ways.\n\n\n\n~~~ .python\ndm = DataMatrix(length=4)\n# Create a new columm\ndm.col = ''\n# By index: assign to a single cell (at row 1)\ndm.col[1] = ':-)'\n# By a tuple (or other iterable) of multiple indices:\n# assign to cells at rows 0 and 2\ndm.col[0, 2] = ':P'\n# By slice: assign from row 1 until the end\ndm.col[2:] = ':D'\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+\n| # | col |\n+---+-----+\n| 0 |  :P |\n| 1 | :-) |\n| 2 |  :D |\n| 3 |  :D |\n+---+-----+\n\n~~~\n\n\n\nYou can also assign multiple values at once, provided that the to-be-assigned sequence is of the correct length.\n\n\n\n~~~ .python\n# Assign to the full column\ndm.col = 1, 2, 3, 4\n# Assign to two cells\ndm.col[0, 2] = 'a', 'b'\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+\n| # | col |\n+---+-----+\n| 0 |  a  |\n| 1 |  2  |\n| 2 |  b  |\n| 3 |  4  |\n+---+-----+\n\n~~~\n\n\n\n\n### Assigning to cells that match a selection criterion\n\nAs will be described in more detail later on, comparing a column to a value gives a new `DataMatrix` that contains only the matching rows. This subsetted `DataMatrix` can in turn be used to assign to the matching rows of the original `DataMatrix`. This sounds a bit abstract but is very easy in practice:\n\n\n\n~~~ .python\ndm.col[1:] = ':D'\ndm.is_happy = 'no'\ndm.is_happy[dm.col == ':D'] = 'yes'\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+----------+\n| # | col | is_happy |\n+---+-----+----------+\n| 0 |  a  |    no    |\n| 1 |  :D |   yes    |\n| 2 |  :D |   yes    |\n| 3 |  :D |   yes    |\n+---+-----+----------+\n\n~~~\n\n\n\n### Assigning to multidimensional columns\n\nAssigning to multidimensional columns works much the same as assigning to regular columns. The main differences are that there are multiple dimensions, and that dimensions can be named.\n\n\n\n\n~~~ .python\ndm = DataMatrix(length=2)\ndm.mdim_col = MultiDimensionalColumn(shape=(('x', 'y'), 3))\n# Set all values to a single value\ndm.mdim_col = 1\n# Set all last dimensions to a single array of shape 3\ndm.mdim_col = [ 1,  2,  3]\n# Set all rows to a single array of shape (2, 3)\ndm.mdim_col = [[ 1,  2,  3],\n               [ 4,  5,  6]]\n", "url": "https://pydatamatrix.eu/1.0/basic", "title": "Basic use"}
{"content": "# Set the column to an array of shape (2, 3, 3)\ndm.mdim_col = [[[ 1,  2,  3],\n                [ 4,  5,  6]],\n               [[ 7,  8,  9],\n                [10, 11, 12]]]\n\n~~~\n\n\n\nTo assign to dimensions by name:\n\n\n\n~~~ .python\ndm.mdim_col[:, 'x'] = 1, 2, 3  # identical to assigning to dm.mdim_col[:, 0]\ndm.mdim_col[:, 'y'] = 4, 5, 6  # identical to assigning to dm.mdim_col[:, 1]\n\n~~~\n\n\n\n*Pro-tip:* When assigning an array-like object to a multidimensional column, the shape of the to-be-assigned array needs to match the final part of the shape of the column. This means that you can assign a (2, 3) array to a (2, 2, 3) column in which case all rows (the first dimension) are set to the array. shape However, you *cannot* assign a (2, 2) array to a (2, 2, 3) column.\n\n## Accessing\n\n### Accessing by index, multiple indices, or slice\n\n\n\n~~~ .python\ndm = DataMatrix(length=4)\n# Create a new column\ndm.col = 'a', 'b', 'c', 'd'\n# By index: select a single cell (at row 1).\nprint(dm.col[1])\n# By a tuple (or other iterable) of multiple indices:\n# select cells at rows 0 and 2. This gives a new column.\nprint(dm.col[0, 2])\n# By slice: assign from row 1 until the end. This gives a new column.\nprint(dm.col[2:])\n\n~~~\n\n__Output:__\n\n~~~ .text\nb\ncol['a', 'c']\ncol['c', 'd']\n\n~~~\n\n\n\n\n### Accessing and averaging (ellipsis averaging) multidimensional columns\n\nAccessing multidimensional columns works much the same as accessing regular columns. The main differences are that there are multiple dimensions, and that dimensions can be named.\n\n\n\n~~~ .python\ndm = DataMatrix(length=2)\ndm.mdim_col = MultiDimensionalColumn(shape=(('x', 'y'), 3))\ndm.mdim_col = [[[ 1,  2,  3],\n                [ 4,  5,  6]],\n               [[ 7,  8,  9],\n                [10, 11, 12]]]\n# From all rows, get index 1 (named 'y') from the second dimension and index 2 from the third dimension.\nprint(dm.mdim_col[:, 'y', 2])\n\n~~~\n\n__Output:__\n\n~~~ .text\ncol[ 6. 12.]\n\n~~~\n\n\n\nYou can select the average of a column using the ellipsis (`...`) index. For regular columns, this is indentical to accessing the `mean` property:\n\n\n\n~~~ .python\ndm.col = 1, 2\nprint(dm.col[...])  # identical to `dm.col.mean`\n\n~~~\n\n__Output:__\n\n~~~ .text\n1.5\n\n~~~\n\n\n\nEllipsis averaging (`...`) is especially useful when working with multidimensional data, in which case it allows you to average over specific dimensions. As long as you don't average over the first dimension, which corresponds to the rows of the `DataMatrix`, the result is a new column.\n\n\n\n\n~~~ .python\n# Averaging over the third dimension gives a column of shape (2, 2)\ndm.avg3 = dm.mdim_col[:, :, ...]\n# Average over the second dimension gives a colum of shape (2, 3)\ndm.avg2 = dm.mdim_col[:, ...]\n# Averaging over the second and third dimensions gives a `FloatColumn`.\ndm.avg23 = dm.mdim_col[:, ..., ...]\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+------------------+-------+-----------+-----+-----------------+\n| # |       avg2       | avg23 |    avg3   | col |     mdim_col    |\n+---+------------------+-------+-----------+-----+-----------------+\n| 0 |  [2.5 3.5 4.5]   |  3.5  |  [2. 5.]  |  1  |   [[1. 2. 3.]   |\n|   |                  |       |           |     |    [4. 5. 6.]]  |\n| 1 | [ 8.5  9.5 10.5] |  9.5  | [ 8. 11.] |  2  |  [[ 7.  8.  9.] |\n|   |                  |       |           |     |  [10. 11. 12.]] |\n+---+------------------+-------+-----------+-----+-----------------+\n\n~~~\n\n\n\nWhen averaging over the first dimension, which corresponds to the rows of the `DataMatrix`, the result is either an array or (if all dimensions are averaged) a float:\n\n\n\n~~~ .python\n# Averaging over the rows gives an array of shape (2, 3)\nprint(dm.mdim_col[...])\n# Averaging over all dimensions gives a float\nprint(dm.mdim_col[..., ..., ...])\n\n~~~\n\n__Output:__\n\n~~~ .text\n[[4. 5. 6.]\n [7. 8. 9.]]\n6.5\n\n~~~\n\n\n\n\n## Selecting\n\n### Selecting by column values\n\nYou can select by directly comparing columns to values. This returns a new `DataMatrix` object with only the selected rows.\n\n\n\n\n~~~ .python\ndm = DataMatrix(length=10)\ndm.col = range(10)\ndm_subset = dm.col > 5\nprint(dm_subset)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+\n| # | col |\n+---+-----+\n| 6 |  6  |\n| 7 |  7  |\n| 8 |  8  |\n| 9 |  9  |\n+---+-----+\n\n~~~\n\n\n\n### Selecting by multiple criteria with `|` (or), `&` (and), and `^` (xor)\n\nYou can select by multiple criteria using the `|` (or), `&` (and), and `^` (xor) operators (but not the actual words 'and' and 'or'). Note the parentheses, which are necessary because `|`, `&`, and `^` have priority over other operators.\n\n\n\n\n~~~ .python\ndm_subset = (dm.col < 1) | (dm.col > 8)\nprint(dm_subset)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+\n| # | col |\n+---+-----+\n| 0 |  0  |\n| 9 |  9  |\n+---+-----+\n\n~~~\n\n\n\n\n\n\n~~~ .python\ndm_subset = (dm.col > 1) & (dm.col < 8)\nprint(dm_subset)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+\n| # | col |\n+---+-----+\n| 2 |  2  |\n| 3 |  3  |\n| 4 |  4  |\n| 5 |  5  |\n| 6 |  6  |\n| 7 |  7  |\n+---+-----+\n\n~~~\n\n\n\n### Selecting by multiple criteria by comparing to a set `{}`\n\nIf you want to check whether column values are identical to, or different from, a set of test values, you can compare the column to a `set` object. (This is considerably faster than comparing the column values to each of the test values separately, and then merging the result using `&` or `|`.)\n\n\n\n\n~~~ .python\ndm_subset = dm.col == {1, 3, 5, 7}\nprint(dm_subset)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+\n| # | col |\n+---+-----+\n| 1 |  1  |\n| 3 |  3  |\n| 5 |  5  |\n| 7 |  7  |\n+---+-----+\n\n~~~\n\n\n\n### Selecting (filtering) with a function or lambda expression\n\nYou can also use a function or `lambda` expression to select column values. The function must take a single argument and its return value determines whether the column value is selected. This is analogous to the classic `filter()` function.\n\n\n\n\n~~~ .python\ndm_subset = dm.col == (lambda x: x % 2)\nprint(dm_subset)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+\n| # | col |\n+---+-----+\n| 1 |  1  |\n| 3 |  3  |\n| 5 |  5  |\n| 7 |  7  |\n| 9 |  9  |\n+---+-----+\n\n~~~\n\n\n\n", "url": "https://pydatamatrix.eu/1.0/basic", "title": "Basic use"}
{"content": "### Selecting values that match another column (or sequence)\n\nYou can also select by comparing a column to a sequence, in which case a row-by-row comparison is done. This requires that the sequence has the same length as the column, is not a `set` object (because `set` objects are treated as described above).\n\n\n\n\n~~~ .python\ndm = DataMatrix(length=4)\ndm.col = 'a', 'b', 'c', 'd'\ndm_subset = dm.col == ['a', 'b', 'x', 'y']\nprint(dm_subset)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+\n| # | col |\n+---+-----+\n| 0 |  a  |\n| 1 |  b  |\n+---+-----+\n\n~~~\n\n\n\n### Selecting values by type\n\nWhen a column contains values of different types, you can also select values by type:\n\n\n\n\n~~~ .python\ndm = DataMatrix(length=4)\ndm.col = 'a', 1, 'c', 2\ndm_subset = dm.col == int\nprint(dm_subset)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+\n| # | col |\n+---+-----+\n| 1 |  1  |\n| 3 |  2  |\n+---+-----+\n\n~~~\n\n\n\n### Getting indices for rows that match selection criteria ('where')\n\nYou can get the indices for rows that match certain selection criteria by slicing a `DataMatrix` with a subset of itself. This is similar to the `numpy.where()` function.\n\n\n\n~~~ .python\ndm = DataMatrix(length=4)\ndm.col = 1, 2, 3, 4\nindices = dm[(dm.col > 1) & (dm.col < 4)]\nprint(indices)\n\n~~~\n\n__Output:__\n\n~~~ .text\n[1, 2]\n\n~~~\n\n\n\n### Selecting a subset of columns\n\nYou can select a subset of columns by passing the columns as an index to `dm[]`. Columns can be specified by name ('col3') or by object (`dm.col1`).\n\n\n\n~~~ .python\ndm = DataMatrix(length=4)\ndm.col1 = '\u263a'\ndm.col2 = 'a'\ndm.col3 = 1\ndm_subset = dm[dm.col1, 'col3']\nprint(dm_subset)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+------+------+\n| # | col1 | col3 |\n+---+------+------+\n| 0 |  \u263a   |  1   |\n| 1 |  \u263a   |  1   |\n| 2 |  \u263a   |  1   |\n| 3 |  \u263a   |  1   |\n+---+------+------+\n\n~~~\n\n\n\n\n## Element-wise column operations\n\n### Multiplication, addition, etc.\n\nYou can apply basic mathematical operations on all cells in a column simultaneously. Cells with non-numeric values are ignored, except by the `+` operator, which then results in concatenation.\n\n\n\n~~~ .python\ndm = DataMatrix(length=3)\ndm.col = 0, 'a', 20\ndm.col2 = dm.col * .5\ndm.col3 = dm.col + 10\ndm.col4 = dm.col - 10\ndm.col5 = dm.col / 50\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+------+------+------+------+\n| # | col | col2 | col3 | col4 | col5 |\n+---+-----+------+------+------+------+\n| 0 |  0  | 0.0  |  10  | -10  | 0.0  |\n| 1 |  a  |  a   | a10  |  a   |  a   |\n| 2 |  20 | 10.0 |  30  |  10  | 0.4  |\n+---+-----+------+------+------+------+\n\n~~~\n\n\n\n### Applying (mapping) a function or lambda expression\n\nYou can apply a function or `lambda` expression to all cells in a column simultaneously with the `@` operator. This analogous to the classic `map()` function.\n\n\n\n\n~~~ .python\ndm = DataMatrix(length=3)\ndm.col = 0, 1, 2\ndm.col2 = dm.col @ (lambda x: x*2)\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+------+\n| # | col | col2 |\n+---+-----+------+\n| 0 |  0  |  0   |\n| 1 |  1  |  2   |\n| 2 |  2  |  4   |\n+---+-----+------+\n\n~~~\n\n\n\n", "url": "https://pydatamatrix.eu/1.0/basic", "title": "Basic use"}
{"content": "## Iterating over rows, columns, and cells (for loops)\n\nBy iterating directly over a `DataMatrix` object, you get successive `Row` objects. From a `Row` object, you can directly access cells.\n\n\n\n\n~~~ .python\ndm.col = 'a', 'b', 'c'\nfor row in dm:\n    print(row)\n    print(row.col)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+------+-------+\n| Name | Value |\n+------+-------+\n| col  |   a   |\n| col2 |   0   |\n+------+-------+\na\n+------+-------+\n| Name | Value |\n+------+-------+\n| col  |   b   |\n| col2 |   2   |\n+------+-------+\nb\n+------+-------+\n| Name | Value |\n+------+-------+\n| col  |   c   |\n| col2 |   4   |\n+------+-------+\nc\n\n~~~\n\n\n\nBy iterating over `DataMatrix.columns`, you get successive `(column_name, column)` tuples.\n\n\n\n\n~~~ .python\nfor colname, col in dm.columns:\n    print('%s = %s' % (colname, col))\n\n~~~\n\n__Output:__\n\n~~~ .text\ncol = col['a', 'b', 'c']\ncol2 = col[0, 2, 4]\n\n~~~\n\n\n\nBy iterating over a column, you get successive cells:\n\n\n\n\n~~~ .python\nfor cell in dm.col:\n    print(cell)\n\n~~~\n\n__Output:__\n\n~~~ .text\na\nb\nc\n\n~~~\n\n\n\nBy iterating over a `Row` object, you get (`column_name, cell`) tuples:\n\n\n\n\n~~~ .python\nrow = dm[0] # Get the first row\nfor colname, cell in row:\n    print('%s = %s' % (colname, cell))\n\n~~~\n\n__Output:__\n\n~~~ .text\ncol = a\ncol2 = 0\n\n~~~\n\n\n\nThe `column_names` property gives a sorted list of all column names (without the corresponding column objects):\n\n\n\n\n~~~ .python\nprint(dm.column_names)\n\n~~~\n\n__Output:__\n\n~~~ .text\n['col', 'col2']\n\n~~~\n\n\n\n\n## Miscellanous notes\n\n### Type conversion and character encoding\n\nFor `MixedColumn`:\n\n- The strings 'nan', 'inf', and '-inf' are converted to the corresponding `float` values (`NAN`, `INF`, and `-INF`).\n- Byte-string values (`bytes`) are automatically converted to `str` assuming `utf-8` encoding.\n- Trying to assign an unsupported type results in a `TypeError`.\n- The string 'None' is *not* converted to the type `None`.\n\n\nFor `FloatColumn`:\n\n- The strings 'nan', 'inf', and '-inf' are converted to the corresponding `float` values (`NAN`, `INF`, and `-INF`).\n- Unsupported types are converted to `NAN`. A warning is shown.\n\n\nFor `IntColumn`:\n\n- Trying to assign non-`int` values results in a `TypeError`.\n\n\n### NAN and INF values\n\nYou have to take special care when working with `nan` data. In general, `nan` is not equal to anything else, not even to itself: `nan != nan`. You can see this behavior when selecting data from a `FloatColumn` with `nan` values in it.\n\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, FloatColumn, NAN\ndm = DataMatrix(length=3)\ndm.f = FloatColumn\ndm.f = 0, NAN, 1\ndm = dm.f == [0, NAN, 1]\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+\n| # |  f  |\n+---+-----+\n| 0 | 0.0 |\n| 2 | 1.0 |\n+---+-----+\n\n~~~\n\n\n\nHowever, for convenience, you can select all `nan` values by comparing a `FloatColumn` to a single `nan` value:\n\n\n\n~~~ .python\ndm = DataMatrix(length=3)\ndm.f = FloatColumn\ndm.f = 0, NAN, 1\nprint(dm.f == NAN)\nprint('NaN values')\nprint('Non-NaN values')\nprint(dm.f != NAN)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+\n| # |  f  |\n+---+-----+\n| 1 | nan |\n+---+-----+\nNaN values\nNon-NaN values\n+---+-----+\n| # |  f  |\n+---+-----+\n| 0 | 0.0 |\n| 2 | 1.0 |\n+---+-----+\n\n~~~\n\n\n", "url": "https://pydatamatrix.eu/1.0/basic", "title": "Basic use"}
{"content": "title: Analyzing eye-movement data\n\n[TOC]\n\n## About this tutorial\n\nWe're going to analyze pupil-size data from an auditory-working-memory experiment. This data is taken from [Math\u00f4t (2018)](#references), and you can find the data and experimental materials [here](https://github.com/smathot/pupillometry_review).\n\nIn this experiment, the participant first hears a series of digits; we will refer to this period as the `sounds` trace. The number of digits (set size) varies: 3, 5, or 7. Next, there is a retention interval during which the participant keeps the digits in memory; we will refer to this period as the `retention` trace. Finally, the participant enters the response.\n\nWe will analyze pupil size during the `sounds` and `retention` traces as a function of set size. As reported by [Kahneman and Beatty (1966)](#references), and as we will also see during this tutorial, the size of the pupil increases with set size.\n\nThis tutorial makes use of the [`eyelinkparser` module](https://github.com/smathot/python-eyelinkparser), which can be installed with pip:\n\n~~~bash\npip install eyelinkparser\n~~~\n\nThe data has been collected with an EyeLink 1000 eye tracker.\n\n\n## Designing an experiment for easy analysis\n\nEyeLink data files (and data files for most other eye trackers) correspond to an event log; that is, each line corresponds to some event. These events can be gaze samples, saccade onsets, user messages, etc.\n\nFor example, a `start_trial` user message followed by four gaze samples might look like this:\n\n~~~text\nMSG\t451224 start_trial\n451224\t  517.6\t  388.9\t 1691.0\t...\n451225\t  517.5\t  389.1\t 1690.0\t...\n451226\t  517.3\t  388.9\t 1692.0\t...\n451227\t  517.1\t  388.7\t 1693.0\t...\n~~~\n\nWhen designing your experiment, it's important to send user messages in such a way that your analysis software, in this case `eyelinkparser`, knows how to interpret them. If you do, then data analysis will be easy, because you will not have to write a custom script to parse the data file from the ground up.\n\nIf you use [OpenSesame/ PyGaze](http://osdoc.cogsci.nl), most of these messages, with the exception of phase messages, will by default be sent in the below format automatically.\n\n\n### Trials\n\nThe following messages indicate the start and end of a trial. The `trialid` argument is optional.\n\n\tstart_trial [trialid]\n\tend_trial\n\n### Variables\n\nThe following message indicates a variable and a value. For example, `var response_time 645` would tell `eyelinkparser` that the variable `response_time` has the value 645 on that particular trial.\n\n\tvar [name] [value]\n\n### Phases\n\nPhases are named periods of continuous data. Defining phases during the experiment is the easiest way to segment your data into different epochs for analysis.\n\nThe following messages indicate the start and end of a phase. A phase is automatically ended when a new phase is started.\n\n\tstart_phase [name]\n\tend_phase [name]\n\nFor each phase, four columns of type `SeriesColumn` will be created with information about fixations:\n\n- `fixxlist_[phase name]` is a series of X coordinates\n- `fixylist_[phase name]` is a series of Y coordinates\n- `fixstlist_[phase name]` is a series of fixation start times\n- `fixetlist_[phase name]` is a series of fixation end times\n- `blinkstlist_[phase name]` is a series of blink start times\n- `blinketlist_[phase name]` is a series of blink end times\n\n\nAdditionally, four columns will be created with information about individual gaze samples:\n\n- `xtrace_[phase name]` is a series of X coordinates\n- `ytrace_[phase name]` is a series of Y coordinates\n- `ttrace_[phase name]` is a series of time stamps\n- `ptrace_[phase name]` is a series of pupil sizes\n\n\n## Analyzing data\n\n", "url": "https://pydatamatrix.eu/1.0/eyelinkparser", "title": "Analyzing eye-movement data"}
{"content": "### Parsing\n\nWe first define a function to parse the EyeLink data; that is, we read the data files, which are in `.asc` text format, into a `DataMatrix` object.\n\nWe define a `get_data()` function that is decorated with `@fnc.memoize()` such that parsing is not redone unnecessarily (see [memoization](<https://pydatamatrix.eu/1.0/memoization>)).\n\n\n\n~~~ .python\nfrom datamatrix import (\n  operations as ops,\n  functional as fnc,\n  series as srs\n)\nfrom eyelinkparser import parse, defaulttraceprocessor\n\n\n@fnc.memoize(persistent=True)\ndef get_data():\n\n    # The heavy lifting is done by eyelinkparser.parse()\n    dm = parse(\n        folder='data',           # Folder with .asc files\n        traceprocessor=defaulttraceprocessor(\n          blinkreconstruct=True, # Interpolate pupil size during blinks\n          downsample=10,         # Reduce sampling rate to 100 Hz,\n          mode='advanced'        # Use the new 'advanced' algorithm\n        )\n    )\n    # To save memory, we keep only a subset of relevant columns.\n    dm = dm[dm.set_size, dm.correct, dm.ptrace_sounds, dm.ptrace_retention, \n            dm.fixxlist_retention, dm.fixylist_retention]\n    return dm\n\n~~~\n\n\n\nWe now call this function to get the data as a a `DataMatrix`. If you want to clear the cache, you can call `get_data.clear()` first.\n\nLet's also print out the `DataMatrix` to get some idea of what our data structure looks like. As you can see, traces are stored as [series](https://pythontutorials.eu/numerical/time-series/), which is convenient for further analysis.\n\n\n\n~~~ .python\ndm = get_data()\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n........../home/sebastiaan/anaconda3/envs/pydata/lib/python3.10/site-packages/datamatrix/series.py:1643: \nRuntimeWarning: Mean of empty slice\n  return fnc(a.reshape(-1, by), axis=1)\n\u001b[32m\u2834\u001b[0m Generating....................................................................+----+---------+-------------------------------+-------------------------------+-----------------------------------+-----------------------------------+----------+\n| #  | correct |       fixxlist_retention      |       fixylist_retention      |          ptrace_retention         |           ptrace_sounds           | set_size |\n+----+---------+-------------------------------+-------------------------------+-----------------------------------+-----------------------------------+----------+\n| 0  |    1    | [525.9 529.7 ...   nan   nan] | [382.9 379.8 ...   nan   nan] | [1810.  1810.3 ... 1902.9 1908.2] | [1655.6 1657.1 ...    nan    nan] |    5     |\n| 1  |    1    | [528.9 527.7 ...   nan   nan] | [386.4 385.8 ...   nan   nan] | [1644.6 1648.7 ... 1675.4 1673.4] | [1717.3 1718.5 ...    nan    nan] |    3     |\n| 2  |    1    | [521.4 523.  ...   nan   nan] | [377.3 367.9 ...   nan   nan] | [1787.  1791.4 ... 1719.9 1719.3] | [1471.8 1470.4 ... 1783.2 1785.3] |    7     |\n| 3  |    1    | [523.2 513.9 ...   nan   nan] | [382.6 379.7 ...   nan   nan] | [1773.5 1774.8 ... 1755.7 1752.5] | [1857.7 1859.1 ...    nan    nan] |    3     |\n| 4  |    1    | [522.1 531.6 ...   nan   nan] | [373.5 376.1 ...   nan   nan] | [1939.2 1939.6 ... 1801.3 1802.7] | [1847.2 1849.6 ... 1940.  1938.7] |    7     |\n| 5  |    1    | [521.  526.9 ...   nan   nan] | [379.1 384.2 ...   nan   nan] | [1867.4 1867.9 ... 1765.6 1768.4] | [1615.9 1619.3 ... 1872.6 1870.6] |    7     |\n| 6  |    1    | [520.8 532.5 ...   nan   nan] | [387.5 377.8 ...   nan   nan] | [1675.2 1674.1 ... 1585.  1581.4] | [1826.5 1821.8 ...    nan    nan] |    5     |\n| 7  |    1    | [527.5 527.  ...   nan   nan] | [396.  390.3 ...   nan   nan] | [1766.5 1766.7 ... 1685.7 1682.2] | [1728.6 1726.7 ...    nan    nan] |    3     |\n| 8  |    1    | [518.4 526.1 ...   nan   nan] | [391.8 388.3 ...   nan   nan] | [1733.3 1736.7 ... 1733.2 1730.2] | [1636.  1637.6 ... 1730.3 1729.3] |    7     |\n| 9  |    0    | [528.2 541.3 ...   nan   nan] | [382.7 386.9 ...   nan   nan] | [1692.  1692.8 ... 1655.5    nan] | [1533.  1534.5 ... 1688.  1693.6] |    7     |\n| 10 |    1    | [528.4 525.9 ...   nan   nan] | [388.3 378.2 ...   nan   nan] | [1602.2 1601.3 ... 1574.1 1575.9] | [1789.2 1793.1 ... 1599.6 1602.1] |    7     |\n| 11 |    0    | [525.  519.7 ...   nan   nan] | [386.7 383.8 ...   nan   nan] | [1742.5 1740.1 ... 1567.1 1559.5] | [1696.8 1700.6 ... 1747.5 1746.8] |    7     |\n| 12 |    1    | [503.1 514.6 ...   nan   nan] | [376.2 377.3 ...   nan   nan] | [1668.9 1668.8 ... 1556.1    nan] | [1667.6 1667.6 ...    nan    nan] |    5     |\n| 13 |    1    | [517.2 511.3 ...   nan   nan] | [387.1 386.  ...   nan   nan] | [1609.5 1602.3 ... 1590.9 1584.3] | [1479.8 1477.9 ... 1614.1 1609.2] |    7     |\n| 14 |    1    | [507.6 502.5 ...   nan   nan] | [388.4 386.2 ...   nan   nan] | [1475.4 1473.7 ... 1484.6 1490.7] | [1481.9 1477.6 ...    nan    nan] |    3     |\n| 15 |    1    | [508.6 507.9 ...   nan   nan] | [387.  382.8 ...   nan   nan] | [1494.8 1494.2 ... 1522.9 1522.1] | [1522.4 1518.7 ... 1496.1 1495.4] |    7     |\n| 16 |    1    | [498.6 509.6 ...   nan   nan] | [386.1 389.7 ...   nan   nan] | [1550.4 1550.3 ... 1390.8 1385.9] | [1651.6 1649.1 ...    nan    nan] |    3     |\n| 17 |    1    | [510.2 500.9 ...   nan   nan] | [387.  380.8 ...   nan   nan] | [1557.6 1559.  ... 1532.2 1532. ] | [1537.8 1534.  ...    nan    nan] |    3     |\n| 18 |    1    | [521.7 512.4 ...   nan   nan] | [382.8 382.5 ...   nan   nan] | [1523.1 1521.9 ... 1409.5 1409.6] | [1471.8 1472.6 ...    nan    nan] |    5     |\n| 19 |    1    | [517.4 515.4 ...   nan   nan] | [386.5 380.5 ...   nan   nan] | [1500.1 1502.  ... 1500.5 1498.4] | [1530.4 1531.  ...    nan    nan] |    5     |\n+----+---------+-------------------------------+-------------------------------+-----------------------------------+-----------------------------------+----------+\n(+ 55 rows not shown)\n\n~~~\n\n\n\n\n### Preprocessing\n\nNext, we do some preprocessing of the pupil-size data.\n\nWe are interested in two traces, `sounds` and `retention`. The length of `sounds` varies, depending on how many digits were played back. The shorter traces are padded with `nan` values at the end. We therefore apply `srs.endlock()` to move the `nan` padding to the beginning of the trace.\n\nTo get some idea of what this means, let's plot pupil size during the `sounds` trace for the first 5 trials, both with and without applying `srs.endlock()`.\n\n\n\n~~~ .python\nfrom matplotlib import pyplot as plt\nfrom datamatrix import series as srs\n\nplt.figure()\nplt.subplot(211)\nplt.title('NANs at the end')\nfor pupil in dm.ptrace_sounds[:5]:\n    plt.plot(pupil)\nplt.subplot(212)\nplt.title('NANs at the start')\nfor pupil in srs.endlock(dm.ptrace_sounds[:5]):\n    plt.plot(pupil)\nplt.show()\n\n~~~\n\n![](/1.0/img/3.png)\n\n\n\nNext, we concatenate the (end-locked) `sounds` and `retention` traces, and save the result as a series called `pupil`.\n\n\n\n~~~ .python\ndm.pupil = srs.concatenate(\n    srs.endlock(dm.ptrace_sounds),\n    dm.ptrace_retention\n)\n\n~~~\n\n\n\nWe then perform baseline correction. As a baseline, we use the first two samples of the `sounds` trace. (This trace still has the `nan` padding at the end.)\n\n\n\n~~~ .python\ndm.pupil = srs.baseline(\n    series=dm.pupil,\n    baseline=dm.ptrace_sounds,\n    bl_start=0,\n    bl_end=2\n)\n\n~~~\n\n\n\nAnd we explicitly set the depth of the `pupil` trace to 1200, which given our original 1000 Hz signal, downsampled 10 \u00d7, corresponds to 12 s.\n\n\n\n~~~ .python\ndm.pupil.depth = 1200\n\n~~~\n\n\n\n\n### Analyzing pupil size\n\nAnd now we plot the pupil traces for each of the three set sizes!\n\n\n\n~~~ .python\nimport numpy as np\n\n\ndef plot_series(x, s, color, label):\n\n    se = s.std / np.sqrt(len(s))\n    plt.fill_between(x, s.mean-se, s.mean+se, color=color, alpha=.25)\n    plt.plot(x, s.mean, color=color, label=label)\n\n\nx = np.linspace(-7, 5, 1200)\ndm3, dm5, dm7 = ops.split(dm.set_size, 3, 5, 7)\n\nplt.figure()\nplt.xlim(-7, 5)\nplt.ylim(-150, 150)\nplt.axvline(0, linestyle=':', color='black')\nplt.axhline(1, linestyle=':', color='black')\nplot_series(x, dm3.pupil, color='green', label='3 (N=%d)' % len(dm3))\nplot_series(x, dm5.pupil, color='blue', label='5 (N=%d)' % len(dm5))\nplot_series(x, dm7.pupil, color='red', label='7 (N=%d)' % len(dm7))\nplt.ylabel('Pupil size (norm)')\nplt.xlabel('Time relative to onset retention interval (s)')\nplt.legend(frameon=False, title='Memory load')\nplt.show()\n\n~~~\n\n__Output:__\n\n~~~ .text\n/home/sebastiaan/anaconda3/envs/pydata/lib/python3.10/site-packages/numpy/lib/nanfunctions.py:1878: \nRuntimeWarning: Degrees of freedom <= 0 for slice.\n  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\u001b[32m\u2826\u001b[0m Generating.../home/sebastiaan/anaconda3/envs/pydata/lib/python3.10/site-packages/datamatrix/_datamatrix/_multidimension\nalcolumn.py:204: RuntimeWarning: Mean of empty slice\n  return nanmean(self._seq, axis=0)\n\u001b[32m\u2826\u001b[0m Generating...\n~~~\n\n![](/1.0/img/4.png)\n\n\n\nAnd a beautiful replication of [Kahneman & Beatty (1966)](#references)!\n\n\n### Analyzing fixations\n\nNow let's look at fixations during the `retention` phase. To get an idea of how the data is structured, we print out the x, y coordinates of all fixations of the first two trials.\n\n\n\n~~~ .python\nfor i, row in zip(range(2), dm):\n    print('Trial %d' % i)\n    for x, y in zip(\n        row.fixxlist_retention,\n        row.fixylist_retention\n    ):\n        print('\\t', x, y)\n\n~~~\n\n__Output:__\n\n~~~ .text\nTrial 0\n\t 525.9 382.9\n\t 529.7 379.8\n\t 531.2 378.7\n\t 526.2 381.9\n\t 529.1 388.7\n\t nan nan\n\t nan nan\n\t nan nan\n\t nan nan\n\t nan nan\n\t nan nan\n\t nan nan\n\t nan nan\nTrial 1\n\t 528.9 386.4\n\t 527.7 385.8\n\t 530.8 383.9\n\t 528.7 381.6\n\t 530.0 379.0\n\t nan nan\n\t nan nan\n\t nan nan\n\t nan nan\n\t nan nan\n\t nan nan\n\t nan nan\n\t nan nan\n\n~~~\n\n\n\nA common way to plot fixation distributions is as a heatmap. To do this, we need to create numpy arrays from the `fixxlist_retention` and `fixylist_retention` columns. This will result in two 2D arrays, whereas `plt.hexbin()` expects two 1D arrays. So we additionally flatten the arrays.\n\nThe resulting heatmap clearly shows that fixations are clustered around the display center (512, 384), just as you would expect from an experiment in which the participant needs to maintain central fixation.\n\n\n\n~~~ .python\nimport numpy as np\n\nx = np.array(dm.fixxlist_retention)\ny = np.array(dm.fixylist_retention)\nx = x.flatten()\ny = y.flatten()\nplt.hexbin(x, y, gridsize=25)\nplt.show()\n\n~~~\n\n![](/1.0/img/5.png)\n\n\n\n## References\n\n- Kahneman, D., & Beatty, J. (1966). Pupil diameter and load on memory. *Science*, 154(3756), 1583\u20131585. <https://doi.org/10.1126/science.154.3756.1583>\n- Math\u00f4t, S., (2018). Pupillometry: Psychology, Physiology, and Function. *Journal of Cognition*. 1(1), p.16. <https://doi.org/10.5334/joc.18>\n", "url": "https://pydatamatrix.eu/1.0/eyelinkparser", "title": "Analyzing eye-movement data"}
{"content": "title: Working with series\n\nThis page has been deprecated. For information about series columns, see:\n\n- <https://pydatamatrix.eu/1.0/basic>\n- <https://pythontutorials.eu/numerical/time-series/>\n", "url": "https://pydatamatrix.eu/1.0/series-tutorial", "title": "Working with series"}
{"content": "title: Install\n\n\n## Dependencies\n\n`DataMatrix` requires only the Python standard library. That is, you can use it without installing any additional Python packages (although the pip and conda packages install some of the optional dependencies by default). Python 3.7 and higher are supported.\n\nThe following packages are required for extra functionality:\n\n- `numpy` and `scipy` for using the `FloatColumn`, `IntColumn`, `SeriesColumn`, `MultiDimensionalColumn` objects\n- `pandas` for conversion to and from `pandas.DataFrame`\n- `mne` for conversion to and from `mne.Epochs` and `mne.TFR`\n- `fastnumbers` for improved performance\n- `prettytable` for creating a text representation of a DataMatrix (e.g. to print it out)\n- `openpyxl` for reading and writing `.xlsx` files\n- `json_tricks` for hashing, serialization to and from `json`, and memoization (caching)\n- `tomlkit` for reading configuration from `pyproject.toml`\n- `psutil` for dynamic loading of large data\n\n\n## Installation\n\n### PyPi (pip install)\n\n~~~bash\npip install datamatrix\n~~~\n\n\n### Anaconda\n\n~~~bash\nconda install -c conda-forge datamatrix\n~~~\n\n\n### Ubuntu\n\n~~~bash\nsudo add-apt-repository ppa:smathot/cogscinl  # for stable releases\nsudo add-apt-repository ppa:smathot/rapunzel  # for development releases\nsudo apt-get update\nsudo apt install python3-datamatrix\n~~~\n\n\n## Source code\n\n- <https://github.com/open-cogsci/python-datamatrix>\n", "url": "https://pydatamatrix.eu/1.0/install", "title": "Install"}
{"content": "title: Working with large data (dynamic loading)\n\n\n[TOC]\n\n\n## Dynamic loading of large data\n\nWhen working with large datasets, especially those containing multidimensional data, the available memory easily becomes a limiting factor. For example, a multidimensional column of shape `(2000, 500, 500)` takes 3.7 Gb of memory.\n\nDataMatrix automatically offloads multidimensional columns to disk when memory is running low. Let's see how this works by creating a DataMatrix with a single column of shape `(2000, 500, 500)`. (The first dimension corresponds to the length of the `DataMatrix`.) On its own, this column easily fits in memory, and we can use the `loaded` property to verify that the column has indeed been loaded into memory.\n\n\n~~~python\nfrom datamatrix import DataMatrix, MultiDimensionalColumn\n\ndm = DataMatrix(length=2000)\ndm.large_data1 = MultiDimensionalColumn(shape=(500, 500))\nprint(f'large_data1 loaded: {dm.large_data1.loaded}')\n~~~\n\n__Output:__\n\n~~~\nlarge_data1 loaded: True\n~~~\n\n\nHowever, if we add another column of the same size, memory starts to run low. Therefore, the old column (`large_data1`) is offloaded to disk, while the newly created column (`large_data2`) is held in memory. This happens automatically. \n\n\n~~~python\ndm.large_data2 = MultiDimensionalColumn(shape=(500, 500))\nprint(f'large_data1 loaded: {dm.large_data1.loaded}')\nprint(f'large_data2 loaded: {dm.large_data2.loaded}')\n~~~\n\n__Output:__\n\n~~~\nlarge_data1 loaded: False\nlarge_data2 loaded: True\n~~~\n\n\nDataMatrix tries to keep the most recently used columns in memory, and offloads the least recently used columns to disk. Therefore, if we assign the value 0 to `large_data1`, this column gets loaded into memory, while `large_data2` is offloaded to disk.\n\n\n~~~python\nimport numpy as np\n\ndm.large_data1 = 0\nprint(f'large_data1 loaded: {dm.large_data1.loaded}')\nprint(f'large_data2 loaded: {dm.large_data2.loaded}')\n~~~\n\n__Output:__\n\n~~~\nlarge_data1 loaded: True\nlarge_data2 loaded: False\n~~~\n\n\nYou can also manually force columns to be loaded into memory or offloaded to disk by changing the `loaded` property.\n\n\n~~~python\ndm.large_data1.loaded = False\nprint(f'large_data1 loaded: {dm.large_data1.loaded}')\nprint(f'large_data2 loaded: {dm.large_data2.loaded}')\n~~~\n\n__Output:__\n\n~~~\nlarge_data1 loaded: False\nlarge_data2 loaded: False\n~~~\n\n## Individual column sizes should not exceed available memory\n\nDynamic loading works best when columns do not, by themselves, exceed the available memory, even though the total size of the `DataMatrix` may exceed the available memory. For example, dynamic loading works well for a 16 Gb system when working with a `DataMatrix` that consists of 3 multidimensional columns of 8 Gb. Here, the total size of the `DataMatrix` is 3 \u00d7 4 = 24 Gb, which exceeds the 16 Gb of available memory; however, each column on its own is only 8 Gb, which does not exceed the available memory.\n\nIt is aso possible (though not recommended) to create columns that, by themselves, exceed the available memory, such as a 24 Gb column on a 16 Gb system. However, many numerical operations, such as taking the mean or standard deviation, will cause all data to be loaded into memory, thus causing Python to crash due to insufficient memory.\n\n\n## Implementation details\n\nWhen a column is offloaded to disk, a `numpy.memmap` object is created instead of a regular `numpy.ndarray`. This object is mapped onto a hidden temporary file in the current working directory. Depending on the operating system, this temporary file is either invisible (unlinked) or has the extension `.memmap`.\n\nSee also:\n\n- <https://numpy.org/doc/stable/reference/generated/numpy.memmap.html>\n", "url": "https://pydatamatrix.eu/1.0/largedata", "title": "Working with large data (dynamic loading)"}
{"content": "title: datamatrix.operations\n\nA set of common operations that can be apply to columns and `DataMatrix` objects. This module is typically imported as `ops` for brevity:\n\n\n\n~~~ .python\nfrom datamatrix import operations as ops\n\n~~~\n\n\n\n\n[TOC]\n\n<div class=\" YAMLDoc\" id=\"\" markdown=\"1\">\n\n \n\n<div class=\"FunctionDoc YAMLDoc\" id=\"auto_type\" markdown=\"1\">\n\n## function __auto\\_type__\\(dm\\)\n\n*Requires fastnumbers*\n\nConverts all columns of type MixedColumn to IntColumn if all values are\ninteger numbers, or FloatColumn if all values are non-integer numbers.\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, operations as ops\n\ndm = DataMatrix(length=5)\ndm.A = 'a'\ndm.B = 1\ndm.C = 1.1\ndm_new = ops.auto_type(dm)\nprint('dm_new.A: %s' % type(dm_new.A))\nprint('dm_new.B: %s' % type(dm_new.B))\nprint('dm_new.C: %s' % type(dm_new.C))\n\n~~~\n\n__Output:__\n\n~~~ .text\ndm_new.A: <class 'datamatrix._datamatrix._mixedcolumn.MixedColumn'>\ndm_new.B: <class 'datamatrix._datamatrix._numericcolumn.IntColumn'>\ndm_new.C: <class 'datamatrix._datamatrix._numericcolumn.FloatColumn'>\n\n~~~\n\n\n__Arguments:__\n\n- `dm` -- No description\n\t- Type: DataMatrix\n\n__Returns:__\n\nNo description\n\n- Type: DataMatrix\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"bin_split\" markdown=\"1\">\n\n## function __bin\\_split__\\(col, bins\\)\n\nSplits a DataMatrix into bins; that is, the DataMatrix is first sorted\nby a column, and then split into equal-size (or roughly equal-size)\nbins.\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, operations as ops\n\ndm = DataMatrix(length=5)\ndm.A = 1, 0, 3, 2, 4\ndm.B = 'a', 'b', 'c', 'd', 'e'\nfor bin, dm in enumerate(ops.bin_split(dm.A, bins=3)):\n   print('bin %d' % bin)\n   print(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\nbin 0\n+---+---+---+\n| # | A | B |\n+---+---+---+\n| 1 | 0 | b |\n+---+---+---+\nbin 1\n+---+---+---+\n| # | A | B |\n+---+---+---+\n| 0 | 1 | a |\n| 3 | 2 | d |\n+---+---+---+\nbin 2\n+---+---+---+\n| # | A | B |\n+---+---+---+\n| 2 | 3 | c |\n| 4 | 4 | e |\n+---+---+---+\n\n~~~\n\n\n__Arguments:__\n\n- `col` -- The column to split by.\n\t- Type: BaseColumn\n- `bins` -- The number of bins.\n\t- Type: int\n\n__Returns:__\n\nA generator that iterates over the bins.\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"fullfactorial\" markdown=\"1\">\n\n## function __fullfactorial__\\(dm, ignore=u''\\)\n\n*Requires numpy*\n\nCreates a new DataMatrix that uses a specified DataMatrix as the base\nof a full-factorial design. That is, each value of every row is \ncombined with each value from every other row. For example:\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, operations as ops\n\ndm = DataMatrix(length=2)\ndm.A = 'x', 'y'\ndm.B = 3, 4\ndm = ops.fullfactorial(dm)\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+---+---+\n| # | A | B |\n+---+---+---+\n| 0 | x | 3 |\n| 1 | y | 3 |\n| 2 | x | 4 |\n| 3 | y | 4 |\n+---+---+---+\n\n~~~\n\n\n__Arguments:__\n\n- `dm` -- The source DataMatrix.\n\t- Type: DataMatrix\n\n__Keywords:__\n\n- `ignore` -- A value that should be ignored.\n\t- Default: ''\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"group\" markdown=\"1\">\n\n## function __group__\\(dm, by\\)\n\n*Requires numpy*\n\nGroups the DataMatrix by unique values in a set of grouping columns.\nGrouped columns are stored as SeriesColumns. The columns that are\ngrouped should contain numeric values. The order in which groups appear\nin the grouped DataMatrix is unpredictable.\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, operations as ops\n\ndm = DataMatrix(length=4)\ndm.A = 'x', 'x', 'y', 'y'\ndm.B = 0, 1, 2, 3\nprint('Original:')\nprint(dm)\ndm = ops.group(dm, by=dm.A)\nprint('Grouped by A:')\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\nOriginal:\n+---+---+---+\n| # | A | B |\n+---+---+---+\n| 0 | x | 0 |\n| 1 | x | 1 |\n| 2 | y | 2 |\n| 3 | y | 3 |\n+---+---+---+\nGrouped by A:\n+---+---+---------+\n| # | A |    B    |\n+---+---+---------+\n| 0 | y | [2. 3.] |\n| 1 | x | [0. 1.] |\n+---+---+---------+\n\n~~~\n\n\n__Arguments:__\n\n- `dm` -- The DataMatrix to group.\n\t- Type: DataMatrix\n- `by` -- A column or list of columns to group by.\n\t- Type: BaseColumn, list\n\n__Returns:__\n\nA grouped DataMatrix.\n\n- Type: DataMatrix\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"keep_only\" markdown=\"1\">\n\n## function __keep\\_only__\\(dm, \\*cols\\)\n\nRemoves all columns from the DataMatrix, except those listed in `cols`.\n\n*Version note:* As of 0.11.0, the preferred way to select a subset of\ncolumns is using the `dm = dm[('col1', 'col2')]` notation.\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, operations as ops\n\ndm = DataMatrix(length=5)\ndm.A = 'a', 'b', 'c', 'd', 'e'\ndm.B = range(5)\ndm.C = range(5, 10)\ndm_new = ops.keep_only(dm, dm.A, dm.C)\nprint(dm_new)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+---+---+\n| # | A | C |\n+---+---+---+\n| 0 | a | 5 |\n| 1 | b | 6 |\n| 2 | c | 7 |\n| 3 | d | 8 |\n| 4 | e | 9 |\n+---+---+---+\n\n~~~\n\n\n__Arguments:__\n\n- `dm` -- No description\n\t- Type: DataMatrix\n\n__Argument list:__\n\n- `*cols`: A list of column names, or column objects.\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"pivot_table\" markdown=\"1\">\n\n## function __pivot\\_table__\\(dm, values, index, columns, \\*args, \\*\\*kwargs\\)\n\n*Requires pandas*\n\n*Version note:* New in 0.14.1\n\nCreates a pivot table where rows correspond to levels of `index`,\ncolumns correspond to levels of `columns`, and cells contain aggregate\nvalues of `values`.\n\nA typical use for a pivot table is to create a summary report for a\ndata set. For example, in an experiment where reaction times of human\nparticipants were measured on a large number of trials under different\nconditions, each row might correspond to one participant, each column\nto an experimental condition (or a combination of experimental\nconditions), and cells might contain mean reaction times.\n\nThis function is a wrapper around the `pandas.pivot_table()`. For an\noverview of possible `*args` and `**kwargs`, see\n[this page](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html).\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import operations as ops, io\n\ndm = io.readtxt('data/fratescu-replication-data-exp1.csv')\npm = ops.pivot_table(dm, values=dm.RT_search, index=dm.subject_nr,\n                     columns=dm.load)\nprint(pm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+----+--------------------+--------------------+\n| #  |         1          |         2          |\n+----+--------------------+--------------------+\n| 0  |  691.393451812936  | 678.3091036076119  |\n| 1  | 1037.4137452306413 | 1076.5579254730912 |\n| 2  | 725.8907459323649  | 740.7180629199368  |\n| 3  | 690.0324213757542  | 663.2912040537004  |\n| 4  | 1061.9616479996344 | 1066.694913085751  |\n| 5  | 878.9107412950773  | 868.7606042917906  |\n| 6  | 772.3190416083047  | 751.7079807753719  |\n| 7  | 640.5894986370438  | 620.1758912269404  |\n| 8  | 591.1702219508884  | 576.4774491644316  |\n| 9  | 610.0829479542426  | 582.0857663440086  |\n| 10 | 912.6923951234676  | 885.8144986324572  |\n| 11 | 776.5285874867564  | 744.9990142569052  |\n| 12 | 811.9071031332232  | 808.8067775165715  |\n| 13 | 763.8125378568926  |  756.239461402817  |\n| 14 | 629.1304692714401  | 614.8002285032511  |\n| 15 | 1138.8041812832648 | 1099.0619141121608 |\n| 16 | 669.6717745408761  | 665.5764135306341  |\n| 17 |  667.380042786298  | 654.8964957059492  |\n| 18 | 696.0044456339372  | 682.9299482924577  |\n| 19 | 703.5121217687149  | 688.2862053908701  |\n+----+--------------------+--------------------+\n(+ 36 rows not shown)\n\n~~~\n\n\n__Arguments:__\n\n- `dm` -- The source DataMatrix.\n\t- Type: DataMatrix\n- `values` -- A column or list of columns to aggregate.\n\t- Type: BaseColumn, str, list\n- `index` -- A column or list of columns to separate rows by.\n\t- Type: BaseColumn, str, list\n- `columns` -- A column or list of columns to separate columns by.\n\t- Type: BaseColumn, str, list\n\n__Argument list:__\n\n- `*args`: No description.\n\n__Keyword dict:__\n\n- `**kwargs`: No description.\n\n__Returns:__\n\nNo description\n\n- Type: DataMatrix\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"random_sample\" markdown=\"1\">\n\n## function __random\\_sample__\\(obj, k\\)\n\n*New in v0.11.0*\n\nTakes a random sample of `k` rows from a DataMatrix or column. The\norder of the rows in the returned DataMatrix is random.\n\n__Example:__\n\n```python\nfrom datamatrix import DataMatrix, operations as ops\n\ndm = DataMatrix(length=5)\ndm.A = 'a', 'b', 'c', 'd', 'e'\ndm = ops.random_sample(dm, k=3)\nprint(dm)\n```\n\n__Arguments:__\n\n- `obj` -- No description\n\t- Type: DataMatrix, BaseColumn\n- `k` -- No description\n\t- Type: int\n\n__Returns:__\n\nA random sample from a DataMatrix or column.\n\n- Type: DataMatrix, BaseColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"replace\" markdown=\"1\">\n\n", "url": "https://pydatamatrix.eu/1.0/operations", "title": "datamatrix.operations"}
{"content": "## function __replace__\\(col, mappings=\\{\\}\\)\n\nReplaces values in a column by other values.\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, operations as ops\n\ndm = DataMatrix(length=3)\ndm.old = 0, 1, 2\ndm.new = ops.replace(dm.old, {0 : 'a', 2 : 'c'})\nprint(dm_new)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+---+---+\n| # | A | C |\n+---+---+---+\n| 0 | a | 5 |\n| 1 | b | 6 |\n| 2 | c | 7 |\n| 3 | d | 8 |\n| 4 | e | 9 |\n+---+---+---+\n\n~~~\n\n\n__Arguments:__\n\n- `col` -- The column to weight by.\n\t- Type: BaseColumn\n\n__Keywords:__\n\n- `mappings` -- A dict where old values are keys and new values are values.\n\t- Type: dict\n\t- Default: {}\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"shuffle\" markdown=\"1\">\n\n## function __shuffle__\\(obj\\)\n\nShuffles a DataMatrix or a column. If a DataMatrix is shuffled, the\norder of the rows is shuffled, but values that were in the same row\nwill stay in the same row.\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, operations as ops\n\ndm = DataMatrix(length=5)\ndm.A = 'a', 'b', 'c', 'd', 'e'\ndm.B = ops.shuffle(dm.A)\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+---+---+\n| # | A | B |\n+---+---+---+\n| 0 | a | d |\n| 1 | b | b |\n| 2 | c | c |\n| 3 | d | e |\n| 4 | e | a |\n+---+---+---+\n\n~~~\n\n\n__Arguments:__\n\n- `obj` -- No description\n\t- Type: DataMatrix, BaseColumn\n\n__Returns:__\n\nThe shuffled DataMatrix or column.\n\n- Type: DataMatrix, BaseColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"shuffle_horiz\" markdown=\"1\">\n\n## function __shuffle\\_horiz__\\(\\*obj\\)\n\nShuffles a DataMatrix, or several columns from a DataMatrix,\nhorizontally. That is, the values are shuffled between columns from the\nsame row.\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, operations as ops\n\ndm = DataMatrix(length=5)\ndm.A = 'a', 'b', 'c', 'd', 'e'\ndm.B = range(5)\ndm = ops.shuffle_horiz(dm.A, dm.B)\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+---+---+\n| # | A | B |\n+---+---+---+\n| 0 | a | 0 |\n| 1 | 1 | b |\n| 2 | 2 | c |\n| 3 | d | 3 |\n| 4 | e | 4 |\n+---+---+---+\n\n~~~\n\n\n__Argument list:__\n\n- `*desc`: A list of BaseColumns, or a single DataMatrix.\n- `*obj`: No description.\n\n__Returns:__\n\nThe shuffled DataMatrix.\n\n- Type: DataMatrix\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"sort\" markdown=\"1\">\n\n## function __sort__\\(obj, by=None\\)\n\nSorts a column or DataMatrix. In the case of a DataMatrix, a column\nmust be specified to determine the sort order. In the case of a column,\nthis needs to be specified if the column should be sorted by another\ncolumn.\n\nThe sort order is as follows:\n\n- `-INF`\n- `int` and `float` values in increasing order\n- `INF`\n- `str` values in alphabetical order, where uppercase letters come\n  first\n- `None`\n- `NAN`\n\nYou can also sort columns (but not DataMatrix objects) using the\nbuilt-in `sorted()` function. However, when sorting different mixed\ntypes, this may lead to Exceptions or (in the case of `NAN` values)\nunpredictable results.\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, operations as ops\n\ndm = DataMatrix(length=3)\ndm.A = 2, 0, 1\ndm.B = 'a', 'b', 'c'\ndm = ops.sort(dm, by=dm.A)\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+---+---+\n| # | A | B |\n+---+---+---+\n| 1 | 0 | b |\n| 2 | 1 | c |\n| 0 | 2 | a |\n+---+---+---+\n\n~~~\n\n\n__Arguments:__\n\n- `obj` -- No description\n\t- Type: DataMatrix, BaseColumn\n\n__Keywords:__\n\n- `by` -- The sort key, that is, the column that is used for sorting the DataMatrix, or the other column.\n\t- Type: BaseColumn\n\t- Default: None\n\n__Returns:__\n\nThe sorted DataMatrix, or the sorted column.\n\n- Type: DataMatrix, BaseColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"split\" markdown=\"1\">\n\n## function __split__\\(col, \\*values\\)\n\nSplits a DataMatrix by unique values in a column.\n\n*Version note:* As of 0.12.0, `split()` accepts multiple columns as\nshown below.\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, operations as ops\n\ndm = DataMatrix(length=4)\ndm.A = 0, 0, 1, 1\ndm.B = 'a', 'b', 'c', 'd'\n# If no values are specified, a (value, DataMatrix) iterator is\n# returned.\nprint('Splitting by a single column')\nfor A, sdm in ops.split(dm.A):\n    print('sdm.A = %s' % A)\n    print(sdm)\n# You can also split by multiple columns at the same time.\nprint('Splitting by two columns')\nfor A, B, sdm in ops.split(dm.A, dm.B):\n    print('sdm.A = %s, sdm.B = %s' % (A, B))\n# If values are specific an iterator over DataMatrix objects is\n", "url": "https://pydatamatrix.eu/1.0/operations", "title": "datamatrix.operations"}
{"content": "# returned.\nprint('Splitting by values')\ndm_a, dm_c = ops.split(dm.B, 'a', 'c')\nprint('dm.B == \"a\"')\nprint(dm_a)\nprint('dm.B == \"c\"')\nprint(dm_c)\n\n~~~\n\n__Output:__\n\n~~~ .text\nSplitting by a single column\nsdm.A = 0\n+---+---+---+\n| # | A | B |\n+---+---+---+\n| 0 | 0 | a |\n| 1 | 0 | b |\n+---+---+---+\nsdm.A = 1\n+---+---+---+\n| # | A | B |\n+---+---+---+\n| 2 | 1 | c |\n| 3 | 1 | d |\n+---+---+---+\nSplitting by two columns\nsdm.A = 0, sdm.B = a\nsdm.A = 0, sdm.B = b\nsdm.A = 1, sdm.B = c\nsdm.A = 1, sdm.B = d\nSplitting by values\ndm.B == \"a\"\n+---+---+---+\n| # | A | B |\n+---+---+---+\n| 0 | 0 | a |\n+---+---+---+\ndm.B == \"c\"\n+---+---+---+\n| # | A | B |\n+---+---+---+\n| 2 | 1 | c |\n+---+---+---+\n\n~~~\n\n\n__Arguments:__\n\n- `col` -- The column to split by.\n\t- Type: BaseColumn\n\n__Argument list:__\n\n- `*values`: Splits the DataMatrix based on these values. If this is provided, an iterator over DataMatrix objects is returned, rather than an iterator over (value, DataMatrix) tuples.\n\n__Returns:__\n\nA iterator over (value, DataMatrix) tuples if no values are provided; an iterator over DataMatrix objects if values are provided.\n\n- Type: Iterator\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"stack\" markdown=\"1\">\n\n## function __stack__\\(\\*dms\\)\n\nStacks multiple DataMatrix objects such that the resulting DataMatrix\nhas a length that is equal to the sum of all the stacked DataMatrix\nobjects. Phrased differently, this function vertically concatenates\nDataMatrix objects.\n\nSee also [`stack_multiprocess()`](https://pydatamatrix.eu/1.0/functional) for stacking\nDataMatrix objects that are returned by functions running in different\nprocesses.\n\nStacking two DataMatrix objects can also be done with the `<<`\noperator. However, when stacking more than two DataMatrix objects,\nusing `stack()` is much faster than iteratively stacking with `<<`.\n\n*Version note:* New in 1.0.0\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import operations as ops\n\ndm1 = DataMatrix(length=2)\ndm1.col = 'A'\ndm2 = DataMatrix(length=2)\ndm2.col = 'B'\ndm3 = DataMatrix(length=2)\ndm3.col = 'C'\ndm = ops.stack(dm1, dm2, dm3)\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+\n| # | col |\n+---+-----+\n| 0 |  A  |\n| 1 |  A  |\n| 2 |  B  |\n| 3 |  B  |\n| 4 |  C  |\n| 5 |  C  |\n+---+-----+\n\n~~~\n\n\n__Argument list:__\n\n- `*dms`: OrderedDict([('desc', 'A list of DataMatrix objects.'), ('type', 'list')])\n\n__Returns:__\n\nNo description\n\n- Type: DataMatrix\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"weight\" markdown=\"1\">\n\n## function __weight__\\(col\\)\n\nWeights a DataMatrix by a column. That is, each row from a DataMatrix\nis repeated as many times as the value in the weighting column.\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, operations as ops\n\ndm = DataMatrix(length=3)\ndm.A = 1, 2, 0\ndm.B = 'x', 'y', 'z'\nprint('Original:')\nprint(dm)\ndm = ops.weight(dm.A)\nprint('Weighted by A:')\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\nOriginal:\n+---+---+---+\n| # | A | B |\n+---+---+---+\n| 0 | 1 | x |\n| 1 | 2 | y |\n| 2 | 0 | z |\n+---+---+---+\nWeighted by A:\n+---+---+---+\n| # | A | B |\n+---+---+---+\n| 0 | 1 | x |\n| 1 | 2 | y |\n| 2 | 2 | y |\n+---+---+---+\n\n~~~\n\n\n__Arguments:__\n\n- `col` -- The column to weight by.\n\t- Type: BaseColumn\n\n__Returns:__\n\nNo description\n\n- Type: DataMatrix\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"z\" markdown=\"1\">\n\n## function __z__\\(col\\)\n\nTransforms a column into z scores such that the mean of all values is\n0 and the standard deviation is 1.\n\n*Version note:* As of 0.13.2, `z()` returns a `FloatColumn` when a\nregular column is give. For non-numeric values, the z score is NAN. If\nthe standard deviation is 0, z scores are also NAN.\n\n*Version note:* As of 0.15.3, `z()` also accepts series columns, in\nwhich case the series is z-transformed such that the grand mean of\nall samples is 0, and the grand standard deviation of all samples is\n1.\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, operations as ops\n\ndm = DataMatrix(length=5)\ndm.col = range(5)\ndm.z = ops.z(dm.col)\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+---------------------+\n| # | col |          z          |\n+---+-----+---------------------+\n| 0 |  0  | -1.2649110640673518 |\n| 1 |  1  | -0.6324555320336759 |\n| 2 |  2  |         0.0         |\n| 3 |  3  |  0.6324555320336759 |\n| 4 |  4  |  1.2649110640673518 |\n+---+-----+---------------------+\n\n~~~\n\n\n__Arguments:__\n\n- `col` -- The column to transform.\n\t- Type: BaseColumn\n\n__Returns:__\n\nNo description\n\n- Type: BaseColumn\n\n</div>\n\n</div>\n\n\n", "url": "https://pydatamatrix.eu/1.0/operations", "title": "datamatrix.operations"}
{"content": "title: datamatrix.multidimensional\n\n\nThis module is typically imported as `mdim` for brevity:\n\n\n\n~~~ .python\nfrom datamatrix import multidimensional as mdim\n\n~~~\n\n\n\n\n[TOC]\n\n## What are multidimensional columns?\n\nA `MultiDimensionalColumn` is a column that itself has a shape; that is, each cell is itself an array. This allows you to represent multidimensional data, such as images and time series.\n\n\n<div class=\" YAMLDoc\" id=\"\" markdown=\"1\">\n\n \n\n<div class=\"FunctionDoc YAMLDoc\" id=\"flatten\" markdown=\"1\">\n\n## function __flatten__\\(dm\\)\n\nFlattens all multidimensional columns of a datamatrix to float columns.\nThe result is a new datamatrix where each row of the original\ndatamatrix is repeated for each value of the multidimensional column.\nThe new datamatrix does not contain any multidimensional columns.\n\nThis function requires that all multidimensional columns in `dm` have\nthe same shape, or that `dm` doesn't contain any multidimensional\ncolumns, in which case a copy of `dm` is returned.\n\n*Version note:* Moved to `datamatrix.multidimensional` in 1.0.0\n\n*Version note:* New in 0.15.0\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, MultiDimensionalColumn,              multidimensional as mdim\n\ndm = DataMatrix(length=2)\ndm.col = 'a', 'b'\ndm.m1 = MultiDimensionalColumn(shape=(3,))\ndm.m1[:] = 1,2,3\ndm.m2 = MultiDimensionalColumn(shape=(3,))\ndm.m2[:] = 3,2,1\nflat_dm = mdim.flatten(dm)\nprint('Original:')\nprint(dm)\nprint('Flattened:')\nprint(flat_dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\nOriginal:\n+---+-----+------------+------------+\n| # | col |     m1     |     m2     |\n+---+-----+------------+------------+\n| 0 |  a  | [1. 2. 3.] | [3. 2. 1.] |\n| 1 |  b  | [1. 2. 3.] | [3. 2. 1.] |\n+---+-----+------------+------------+\nFlattened:\n+---+-----+-----+-----+\n| # | col |  m1 |  m2 |\n+---+-----+-----+-----+\n| 0 |  a  | 1.0 | 3.0 |\n| 1 |  a  | 2.0 | 2.0 |\n| 2 |  a  | 3.0 | 1.0 |\n| 3 |  b  | 1.0 | 3.0 |\n| 4 |  b  | 2.0 | 2.0 |\n| 5 |  b  | 3.0 | 1.0 |\n+---+-----+-----+-----+\n\n~~~\n\n\n__Arguments:__\n\n- `dm` -- A DataMatrix\n\t- Type: DataMatrix\n\n__Returns:__\n\nA 'flattened' DataMatrix without multidimensional columns\n\n- Type: DataMatrix\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"infcount\" markdown=\"1\">\n\n", "url": "https://pydatamatrix.eu/1.0/multidimensional", "title": "datamatrix.multidimensional"}
{"content": "## function __infcount__\\(col\\)\n\nCounts the number of `INF` values for each cell in a multidimensional\ncolumn, and returns this as an int column.\n\n*Version note:* Moved to `datamatrix.multidimensional` in 1.0.0\n\n*Version note:* New in 0.15.0\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, MultiDimensionalColumn,              multidimensional as mdim, INF\n\ndm = DataMatrix(length=3)\ndm.m = MultiDimensionalColumn(shape=(3,))\ndm.m[0] = 1, 2, 3\ndm.m[1] = 1, 2, INF\ndm.m[2] = INF, INF, INF\ndm.nr_of_inf = mdim.infcount(dm.m)\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+---------------+-----------+\n| # |       m       | nr_of_inf |\n+---+---------------+-----------+\n| 0 |   [1. 2. 3.]  |    0.0    |\n| 1 | [ 1.  2. inf] |    1.0    |\n| 2 | [inf inf inf] |    3.0    |\n+---+---------------+-----------+\n\n~~~\n\n\n__Arguments:__\n\n- `col` -- A multidimensional column to count the `INF` values in.\n\t- Type: MultiDimensionalColumn\n\n__Returns:__\n\nAn int column with the number of `INF` values in each cell.\n\n- Type: IntColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"nancount\" markdown=\"1\">\n\n## function __nancount__\\(col\\)\n\nCounts the number of `NAN` values for each cell in a multidimensional\ncolumn, and returns this as an int column.\n\n*Version note:* Moved to `datamatrix.multidimensional` in 1.0.0\n\n*Version note:* New in 0.15.0\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, MultiDimensionalColumn,              multidimensional as mdim, NAN\n\ndm = DataMatrix(length=3)\ndm.m = MultiDimensionalColumn(shape=(3,))\ndm.m[0] = 1, 2, 3\ndm.m[1] = 1, 2, NAN\ndm.m[2] = NAN, NAN, NAN\ndm.nr_of_nan = mdim.nancount(dm.m)\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+---------------+-----------+\n| # |       m       | nr_of_nan |\n+---+---------------+-----------+\n| 0 |   [1. 2. 3.]  |    0.0    |\n| 1 | [ 1.  2. nan] |    1.0    |\n| 2 | [nan nan nan] |    3.0    |\n+---+---------------+-----------+\n\n~~~\n\n\n__Arguments:__\n\n- `col` -- A column to count the `NAN` values in.\n\t- Type: MultiDimensionalColumn\n\n__Returns:__\n\nAn int column with the number of `NAN` values in each cell.\n\n- Type: IntColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"reduce\" markdown=\"1\">\n\n## function __reduce__\\(col, operation=<function nanmean at 0x7f31341e1090>\\)\n\nTransforms multidimensional values to single values by applying an\noperation (typically a mean) to each multidimensional value.\n\n*Version note:* Moved to `datamatrix.multidimensional` in 1.0.0\n\n*Version note:* As of 0.11.0, the function has been renamed to\n`reduce()`. The original `reduce_()` is deprecated.\n\n__Example:__\n\n\n~~~ .python\nimport numpy as np\nfrom datamatrix import DataMatrix, MultiDimensionalColumn,              multidimensional as mdim\n\ndm = DataMatrix(length=5)\ndm.m = MultiDimensionalColumn(shape=(3, 3))\ndm.m = np.random.random((5, 3, 3))\ndm.mean_y = mdim.reduce(dm.m)\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+--------------------------+--------------------+\n| # |            m             |       mean_y       |\n+---+--------------------------+--------------------+\n| 0 | [[0.4993 0.8302 0.9192]  | 0.6533534165621145 |\n|   |  [0.5513 0.746  0.4088]  |                    |\n|   |  [0.8511 0.8547 0.2194]] |                    |\n| 1 | [[0.2507 0.6255 0.6129]  | 0.5042268258371895 |\n|   |  [0.7033 0.2006 0.1146]  |                    |\n|   |  [0.6116 0.8773 0.5417]] |                    |\n| 2 | [[0.6461 0.1216 0.8411]  | 0.4648326984642021 |\n|   |  [0.9071 0.0286 0.2339]  |                    |\n|   |  [0.7419 0.3654 0.298 ]] |                    |\n| 3 | [[0.7501 0.2845 0.4244]  | 0.5337976626212995 |\n|   |  [0.636  0.0242 0.5617]  |                    |\n|   |  [0.8073 0.5034 0.8125]] |                    |\n| 4 | [[0.6751 0.0221 0.6798]  | 0.5542447509379135 |\n|   |  [0.8258 0.1754 0.8457]  |                    |\n|   |  [0.4953 0.7795 0.4896]] |                    |\n+---+--------------------------+--------------------+\n\n~~~\n\n\n__Arguments:__\n\n- `col` -- The column to reduce.\n\t- Type: MultiDimensionalColumn\n\n__Keywords:__\n\n- `operation` -- The operation function to use for the reduction. This function should accept `col` as first argument, and `axis=1` as keyword argument.\n\t- Default: <function nanmean at 0x7f31341e1090>\n\n__Returns:__\n\nA reduction of the signal.\n\n- Type: FloatColumn\n\n</div>\n\n</div>\n\n\n", "url": "https://pydatamatrix.eu/1.0/multidimensional", "title": "datamatrix.multidimensional"}
{"content": "title: datamatrix.io\n\nA set of functions for reading and writing `DataMatrix` objects from and to file.\n\n\n\n~~~ .python\nfrom datamatrix import io\n\n~~~\n\n\n\n\n[TOC]\n\n<div class=\" YAMLDoc\" id=\"\" markdown=\"1\">\n\n \n\n<div class=\"FunctionDoc YAMLDoc\" id=\"readbin\" markdown=\"1\">\n\n## function __readbin__\\(path\\)\n\nReads a DataMatrix from a binary file. This format allows you to read\nand write DataMatrix objects with unloaded columns, i.e. columns that\nare too large to fit in memory.\n\n__Example:__\n\n~~~.python\ndm = io.readbin('data.dm')\n~~~\n\n*Version note:* New in 1.0.0\n\n__Arguments:__\n\n- `path` -- The path to the binary file.\n\n__Returns:__\n\nA DataMatrix.\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"readpickle\" markdown=\"1\">\n\n## function __readpickle__\\(path\\)\n\nReads a DataMatrix from a pickle file.\n\n__Example:__\n\n~~~.python\ndm = io.readpickle('data.pkl')\n~~~\n\n__Arguments:__\n\n- `path` -- The path to the pickle file.\n\n__Returns:__\n\nA DataMatrix.\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"readtxt\" markdown=\"1\">\n\n## function __readtxt__\\(path, delimiter=u',', quotechar=u'\"', default\\_col\\_type=<class 'datamatrix\\.\\_datamatrix\\.\\_mixedcolumn\\.MixedColumn'>\\)\n\nReads a DataMatrix from a csv file.\n\n__Example:__\n\n~~~ .python\ndm = io.readtxt('data.csv')\n~~~\n\n*Version note:* As of 0.10.7, byte-order marks (BOMs) are silently\nstripped from column names.\n\n__Arguments:__\n\n- `path` -- The path to the pickle file.\n\n__Keywords:__\n\n- `delimiter` -- The delimiter characer.\n\t- Default: ','\n- `quotechar` -- The quote character.\n\t- Default: '\"'\n- `default_col_type` -- The default column type.\n\t- Default: <class 'datamatrix._datamatrix._mixedcolumn.MixedColumn'>\n\n__Returns:__\n\nA DataMatrix.\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"readxlsx\" markdown=\"1\">\n\n## function __readxlsx__\\(path, default\\_col\\_type=<class 'datamatrix\\.\\_datamatrix\\.\\_mixedcolumn\\.MixedColumn'>, sheet=None\\)\n\nReads a DataMatrix from an Excel 2010 xlsx file.\n\n__Example:__\n\n~~~.python\ndm = io.readxlsx('data.xlsx')\n~~~\n\n__Arguments:__\n\n- `path` -- The path to the xlsx file.\n\n__Keywords:__\n\n- `default_col_type` -- The default column type.\n\t- Default: <class 'datamatrix._datamatrix._mixedcolumn.MixedColumn'>\n- `sheet` -- The name of a sheet, or None to open the active sheet. The activate sheet is not necessarily the first sheet. *(New in 0.7.0)*\n\t- Default: None\n\n__Returns:__\n\nA DataMatrix.\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"writebin\" markdown=\"1\">\n\n## function __writebin__\\(dm, path\\)\n\nReads a DataMatrix to a binary file. This format allows you to read\nand write DataMatrix objects with unloaded columns, i.e. columns that\nare too large to fit in memory.\n\n__Example:__\n\n~~~ .python\nio.writebin(dm, 'data.dm')\n~~~\n\n*Version note:* New in 1.0.0\n\n__Arguments:__\n\n- `dm` -- The DataMatrix to write.\n- `path` -- The path to the binary file.\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"writepickle\" markdown=\"1\">\n\n## function __writepickle__\\(dm, path, protocol=-1\\)\n\nWrites a DataMatrix to a pickle file.\n\n__Example:__\n\n~~~ .python\nio.writepickle(dm, 'data.pkl')\n~~~\n\n__Arguments:__\n\n- `dm` -- The DataMatrix to write.\n- `path` -- The path to the pickle file.\n\n__Keywords:__\n\n- `protocol` -- The pickle protocol.\n\t- Default: -1\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"writetxt\" markdown=\"1\">\n\n## function __writetxt__\\(dm, path, delimiter=u',', quotechar=u'\"'\\)\n\nWrites a DataMatrix to a csv file.\n\n__Example:__\n\n~~~ .python\nio.writetxt(dm, 'data.csv')\n~~~\n\n__Arguments:__\n\n- `dm` -- The DataMatrix to write.\n- `path` -- The path to the pickle file.\n\n__Keywords:__\n\n- `delimiter` -- The delimiter characer.\n\t- Default: ','\n- `quotechar` -- The quote character.\n\t- Default: '\"'\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"writexlsx\" markdown=\"1\">\n\n## function __writexlsx__\\(dm, path\\)\n\nWrites a DataMatrix to an Excel 2010 xlsx file. The first sheet will\ncontain a regular table with all non-series columns. SeriesColumns are\nsaved as individual sheets.\n\n__Example:__\n\n~~~ .python\nio.writexlsx(dm, 'data.xlsx')\n~~~\n\n__Arguments:__\n\n- `dm` -- The DataMatrix to write.\n- `path` -- The path to the xlsx file.\n\n</div>\n\n</div>\n\n\n", "url": "https://pydatamatrix.eu/1.0/io", "title": "datamatrix.io"}
{"content": "title: datamatrix.functional\n\nA set of functions and decorators for [functional programming](https://docs.python.org/3.6/howto/functional.html). This module is typically imported as `fnc` for brevity:\n\n\n\n~~~ .python\nfrom datamatrix import functional as fnc\n\n~~~\n\n\n\n\n[TOC]\n\n## What is functional programming?\n\nFunctional programming is a style of programming that is characterized by the following:\n\n- __Lack of statements__\u2014In its purest form, functional programming does not use any statements. Statements are things like assignments (e.g. `x  = 1`), `for` loops, `if` statements, etc. Instead of statements, functional programs are chains of function calls.\n- __Short functions__\u2014In the purest form of functional programming, each function is a single expression. In Python, this can be implemented through `lambda` expressions.\n- __Referential transparency__\u2014Functions are referentially transparent when they always return the same result given the same set of arguments (i.e. they are *stateless*), and when they do not alter the state of the program (i.e. they have no *side effects*).\n\n<div class=\" YAMLDoc\" id=\"\" markdown=\"1\">\n\n \n\n<div class=\"FunctionDoc YAMLDoc\" id=\"curry\" markdown=\"1\">\n\n## function __curry__\\(fnc\\)\n\nA [currying](https://en.wikipedia.org/wiki/Currying) decorator that\nturns a function with multiple arguments into a chain of partial\nfunctions, each of which takes at least a single argument. The input\nfunction may accept keywords, but the output function no longer does\n(i.e. currying turns all keywords into positional arguments).\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import functional as fnc\n\n@fnc.curry\ndef add(a, b, c):\n\n   return a + b + c\n\nprint(add(1)(2)(3)) # Curried approach with single arguments\nprint(add(1, 2)(3)) # Partly curried approach\nprint(add(1)(2, 3)) # Partly curried approach\nprint(add(1, 2, 3)) # Original approach multiple arguments\n\n~~~\n\n__Output:__\n\n~~~ .text\n6\n6\n6\n6\n\n~~~\n\n\n__Arguments:__\n\n- `fnc` -- A function to curry.\n\t- Type: callable\n\n__Returns:__\n\nA curried function that accepts at least the first argument, and returns a function that accepts the second argument, etc.\n\n- Type: callable\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"filter_\" markdown=\"1\">\n\n## function __filter\\___\\(fnc, obj\\)\n\nFilters rows from a datamatrix or column based on filter function\n(`fnc`).\n\nIf `obj` is a column, `fnc` should be a function that accepts a single\nvalue. If `obj` is a datamatrix, `fnc` should be a function that\naccepts a keyword `dict`, where column names are keys and cells are \nvalues. In both cases, `fnc` should return a `bool` indicating whether \nthe row or value should be included.\n\n*New in v0.8.0*: You can also directly compare a column with a function\nor `lambda` expression. However, this is different from `filter_()` in\nthat it returns a datamatrix object and not a column.\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, functional as fnc\n\ndm = DataMatrix(length=5)\ndm.col = range(5)\n# Create a column with only odd values\ncol_new = fnc.filter_(lambda x: x % 2, dm.col)\nprint(col_new)\n# Create a new datamatrix with only odd values in col\ndm_new = fnc.filter_(lambda **d: d['col'] % 2, dm)\nprint(dm_new)\n\n~~~\n\n__Output:__\n\n~~~ .text\ncol[1, 3]\n+---+-----+\n| # | col |\n+---+-----+\n| 1 |  1  |\n| 3 |  3  |\n+---+-----+\n\n~~~\n\n\n__Arguments:__\n\n- `fnc` -- A filter function.\n\t- Type: callable\n- `obj` -- A datamatrix or column to filter.\n\t- Type: BaseColumn, DataMatrix\n\n__Returns:__\n\nA new column or datamatrix.\n\n- Type: BaseColumn, DataMatrix\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"map_\" markdown=\"1\">\n\n## function __map\\___\\(fnc, obj\\)\n\nMaps a function (`fnc`) onto rows of datamatrix or cells of a column.\n\nIf `obj` is a column, the function `fnc` is mapped is mapped onto each\ncell of the column, and a new column is returned. In this case,\n`fnc` should be a function that accepts and returns a single value.\n\nIf `obj` is a datamatrix, the function `fnc` is mapped onto each row,\nand a new datamatrix is returned. In this case, `fnc` should be a\nfunction that accepts a keyword `dict`, where column names are keys and\ncells are values. The return value should be another `dict`, again with\ncolumn names as keys, and cells as values. Columns that are not part of\nthe returned `dict` are left unchanged.\n\n*New in v0.8.0*: In Python 3.5 and later, you can also map a function\nonto a column using the `@` operator:\n`dm.new = dm.old @ (lambda i: i*2)`\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, functional as fnc\n\ndm = DataMatrix(length=3)\ndm.old = 0, 1, 2\n# Map a 2x function onto dm.old to create dm.new\ndm.new = fnc.map_(lambda i: i*2, dm.old)\nprint(dm)\n# Map a 2x function onto the entire dm to create dm_new, using a fancy\n", "url": "https://pydatamatrix.eu/1.0/functional", "title": "datamatrix.functional"}
{"content": "# dict comprehension wrapped inside a lambda function.\ndm_new = fnc.map_(\n   lambda **d: {col : 2*val for col, val in d.items()},\n   dm\n)\nprint(dm_new)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----+-----+\n| # | new | old |\n+---+-----+-----+\n| 0 |  0  |  0  |\n| 1 |  2  |  1  |\n| 2 |  4  |  2  |\n+---+-----+-----+\n+---+-----+-----+\n| # | new | old |\n+---+-----+-----+\n| 0 |  0  |  0  |\n| 1 |  4  |  2  |\n| 2 |  8  |  4  |\n+---+-----+-----+\n\n~~~\n\n\n__Arguments:__\n\n- `fnc` -- A function to map onto each row or each cell.\n\t- Type: callable\n- `obj` -- A datamatrix or column to map `fnc` onto.\n\t- Type: BaseColumn, DataMatrix\n\n__Returns:__\n\nA new column or datamatrix.\n\n- Type: BaseColumn, DataMatrix\n\n</div>\n\n<div class=\"ClassDoc YAMLDoc\" id=\"memoize\" markdown=\"1\">\n\n## class __memoize__\n\n*Requires json_tricks*\n\nA memoization decorator that stores the result of a function call, and\nreturns the stored value when the function is called again with the\nsame arguments. That is, memoization is a specific kind of caching that\nimproves performance for expensive function calls.\n\nThis decorator only works for return values that can be pickled, and\narguments that can be serialized to `json`.\n\nThe memoized function becomes a callable object. To clear the\nmemoization cache, call the `.clear()` function on the memoized\nfunction. The total size of all cached return values is available as \nthe `.cache_size` property.\n\nFor a more detailed description, see:\n\n- <https://pydatamatrix.eu/1.0/memoization>\n\n*Changed in v0.8.0*: You can no longer pass the `memoclear` keyword to\nthe memoized function. Use the `.clear()` function instead.\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import functional as fnc\n\n@fnc.memoize\ndef add(a, b):\n\n   print('add(%d, %d)' % (a, b))\n   return a + b\n\nthree = add(1, 2)  # Storing result in memory\nthree = add(1, 2)  # Re-using previous result\nadd.clear()  # Clear cache, but only for the next call\nthree = add(1, 2)  # Calculate again\n\n@fnc.memoize(persistent=True, key='persistent-add')\ndef persistent_add(a, b):\n\n   print('persistent_add(%d, %d)' % (a, b))\n   return a + b\n\nthree = persistent_add(1, 2)  # Writing result to disk\nthree = persistent_add(1, 2)  # Re-using previous result\n\n~~~\n\n__Output:__\n\n~~~ .text\nadd(1, 2)\nadd(1, 2)\npersistent_add(1, 2)\n\n~~~\n\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"profile\" markdown=\"1\">\n\n", "url": "https://pydatamatrix.eu/1.0/functional", "title": "datamatrix.functional"}
{"content": "## function __profile__\\(\\*args, \\*\\*kwds\\)\n\nA context manager (`with`) for easy profiling, using cProfile. The\nresults of the profile are written to the file specified in the `path`\nkeyword (default=`u'profile'`), and the sorting order, as accepted by\n`pstats.Stats.sort_stats()`, is specified in the the `sortby` keyword\n(default=`u'cumulative'`).\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import functional as fnc\n\nwith fnc.profile(path=u'profile.txt', sortby=u'cumulative'):\n    dm = DataMatrix(length=1000)\n    dm.col = range(1000)\n    dm.is_even = dm.col @ (lambda x: not x % 2)\n\n~~~\n\n\n__Argument list:__\n\n- `*args`: No description.\n\n__Keyword dict:__\n\n- `**kwds`: No description.\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"setcol\" markdown=\"1\">\n\n## function __setcol__\\(dm, name, value\\)\n\nReturns a new DataMatrix to which a column has been added or in which\na column has been modified.\n\nThe main difference with regular assignment (`dm.col = 'x'`) is that\n`setcol()` does not modify the original DataMatrix, and can be used in\n`lambda` expressions.\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, functional as fnc\n\ndm1 = DataMatrix(length=5)\ndm2 = fnc.setcol(dm1, 'y', range(5))\nprint(dm2)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+---+\n| # | y |\n+---+---+\n| 0 | 0 |\n| 1 | 1 |\n| 2 | 2 |\n| 3 | 3 |\n| 4 | 4 |\n+---+---+\n\n~~~\n\n\n__Arguments:__\n\n- `dm` -- A DataMatrix.\n\t- Type: DataMatrix\n- `name` -- A column name.\n\t- Type: str\n- `value` -- The value to be assigned to the column. This can be any value this is valid for a regular column assignment.\n\n__Returns:__\n\nA new DataMatrix.\n\n- Type: DataMatrix\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"stack_multiprocess\" markdown=\"1\">\n\n## function __stack\\_multiprocess__\\(fnc, args, processes=None\\)\n\nFacilitates multiprocessing for functions that return `DataMatrix`\nobjects.\n\nSpecifically, `stack_multiprocess()`, calls `fnc()` in separate\nprocesses, each time passing a different argument. Arguments are\nspecified in `args`, which should be a list (or other iterable) of\narguments that are passed to `fnc()` for each call separately. In other\nwords, as many processes are launched as there are elements in `args`.\n`fnc()` should be a function that accepts a single argument and returns\na `DataMatrix` object. The resulting `DataMatrix` objects are stacked\ntogether (similar to `ops.stack()`) and returned as a single \n`DataMatrix`.\n\nSee also:\n\n- <https://docs.python.org/3/library/multiprocessing.html>\n- <https://pydatamatrix.eu/1.0/operations>#function-stack\n\n*Version note:* New in 1.0.0.\n\n*Version note:* As of 1.0.4, if one of the processes crashes, and error\nis shown with the Exception, but the main process doesn't crash.\n\n__Example:__\n\n```python\nfrom datamatrix import DataMatrix, functional as fnc\n\ndef get_dm(i):\n    dm = DataMatrix(length=1)\n    dm.s = i\n    return dm\n\n# This will launch five separate processes and return a single dm\ndm = fnc.stack_multiprocess(get_dm, [1, 2, 3, 4, 5])\n```\n\narguments:\n    fnc:\n        desc: A function to call. This function should accept a single\n              argument and return a single `DataMatrix`.\n        type: callable\n    args:\n        desc: A `list` of arguments that are passes separately to\n              `fnc()`.\n\nkeywords:\n    processes:\n        desc: The number of processes that are launched simultaneously\n              or `None` to launch one process for each core on the\n              system.\n        type: [None, int]\n\nreturns:\n    type: `DataMatrix`\n\n__Arguments:__\n\n- `fnc` -- No description\n- `args` -- No description\n\n__Keywords:__\n\n- `processes` -- No description\n\t- Default: None\n\n</div>\n\n</div>\n\n\n", "url": "https://pydatamatrix.eu/1.0/functional", "title": "datamatrix.functional"}
{"content": "title: datamatrix.series\n\n\nThis module is typically imported as `srs` for brevity:\n\n\n\n~~~ .python\nfrom datamatrix import series as srs\n\n~~~\n\n\n\n\n\n[TOC]\n\n## What are series?\n\nA `SeriesColumn` is a column with a depth; that is, each cell contains multiple values. Data of this kind is very common. For example, imagine a psychology experiment in which participants see positive or negative pictures, while their brain activity is recorded using electroencephalography (EEG). Here, picture type (positive or negative) is a single value that could be stored in a normal table. But EEG activity is a continuous signal, and could be stored as `SeriesColumn`.\n\nA `SeriesColumn` is identical to a `MultiDimensionalColumn` with a shape of length 1. Therefore, all functions in the [`multidimensional` module](%url:multidimensional) can also be applied to `SeriesColumn`s.\n\nFor more information, see:\n\n- <https://pythontutorials.eu/numerical/time-series/>\n\n<div class=\" YAMLDoc\" id=\"\" markdown=\"1\">\n\n \n\n<div class=\"FunctionDoc YAMLDoc\" id=\"baseline\" markdown=\"1\">\n\n## function __baseline__\\(series, baseline, bl\\_start=-100, bl\\_end=None, reduce\\_fnc=None, method=u'subtractive'\\)\n\nApplies a baseline to a signal\n\n__Example:__\n\n\n~~~ .python\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom datamatrix import DataMatrix, SeriesColumn, series as srs\n\nLENGTH = 5 # Number of rows\nDEPTH = 10 # Depth (or length) of SeriesColumns\n\nsinewave = np.sin(np.linspace(0, 2*np.pi, DEPTH))\n\ndm = DataMatrix(length=LENGTH)\n# First create five identical rows with a sinewave\ndm.y = SeriesColumn(depth=DEPTH)\ndm.y.setallrows(sinewave)\n# Add a random offset to the Y values\ndm.y += np.random.random(LENGTH)\n# And also a bit of random jitter\ndm.y += .2*np.random.random( (LENGTH, DEPTH) )\n# Baseline-correct the traces, This will remove the vertical\n# offset\ndm.y2 = srs.baseline(dm.y, dm.y, bl_start=0, bl_end=10)\n\nplt.clf()\nplt.subplot(121)\nplt.title('Original')\nplt.plot(dm.y.plottable)\nplt.subplot(122)\nplt.title('Baseline corrected')\nplt.plot(dm.y2.plottable)\nplt.show()\n\n~~~\n\n![](/1.0/img/6.png)\n\n\n__Arguments:__\n\n- `series` -- The signal to apply a baseline to.\n\t- Type: SeriesColumn\n- `baseline` -- The signal to use as a baseline to.\n\t- Type: SeriesColumn\n\n__Keywords:__\n\n- `bl_start` -- The start of the window from `baseline` to use.\n\t- Type: int\n\t- Default: -100\n- `bl_end` -- The end of the window from `baseline` to use, or None to go to the end.\n\t- Type: int, None\n\t- Default: None\n- `reduce_fnc` -- The function to reduce the baseline epoch to a single value. If None, np.nanmedian() is used.\n\t- Type: FunctionType, None\n\t- Default: None\n- `method` -- Specifies whether divisive or subtractive baseline\ncorrection should be used. (*Changed in v0.7.0: subtractive\nis now the default*)\n\t- Type: str\n\t- Default: 'subtractive'\n\n__Returns:__\n\nA baseline-correct version of the signal.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"blinkreconstruct\" markdown=\"1\">\n\n## function __blinkreconstruct__\\(series, vt=5, vt\\_start=10, vt\\_end=5, maxdur=500, margin=10, smooth\\_winlen=21, std\\_thr=3, gap\\_margin=20, gap\\_vt=10, mode=u'original'\\)\n\nReconstructs pupil size during blinks. This algorithm has been designed\nand tested largely with the EyeLink 1000 eye tracker.\n\n*Version note:* As of 0.13.0, an advanced algorithm has been\nintroduced, wich can be specified through the `mode` keyword. The\nadvanced algorithm is recommended for new analyses, and will be made\nthe default in future releases.\n\n*Version note:* As of 1.0.5 the advanced algorithm has been updated\nwith a [bugfix](https://github.com/open-cogsci/datamatrix/pull/18) and\nthe end of a blink is defined as the moment where the velocity drops\nto 1% of the velocity standard deviation, as opposed to 0.\n\n__Source:__\n\n- Mathot, S., & Vilotijevi\u0107, A. (2022). Methods in cognitive \n  pupillometry: Design, preprocessing, and statitical analysis.\n  *Behavior Research Methods*.\n  <https://doi.org/10.3758/s13428-022-01957-7>\n\n__Arguments:__\n\n- `series` -- A signal to reconstruct.\n\t- Type: SeriesColumn\n\n__Keywords:__\n\n- `vt` -- A pupil-velocity threshold for blink detection. Lower tresholds more easily trigger blinks. This argument only applies to 'original' mode.\n\t- Type: int, float\n\t- Default: 5\n- `vt_start` -- A pupil-velocity threshold for detecting the onset of a blink. Lower tresholds more easily trigger blinks. This argument only applies to 'advanced' mode.\n\t- Type: int, float\n\t- Default: 10\n- `vt_end` -- A pupil-velocity threshold for detecting the offset of a blink. Lower tresholds more easily trigger blinks. This argument only applies to 'advanced' mode.\n\t- Type: int, float\n\t- Default: 5\n- `maxdur` -- The maximum duration (in samples) for a blink. Longer blinks are not reconstructed.\n\t- Type: int\n\t- Default: 500\n- `margin` -- The margin to take around missing data that is reconstructed.\n\t- Type: int\n\t- Default: 10\n- `smooth_winlen` -- The window length for a hanning window that is used to smooth the velocity profile.\n\t- Type: int\n\t- Default: 21\n- `std_thr` -- A standard-deviation threshold for when data should be considered invalid.\n\t- Type: float, int\n\t- Default: 3\n- `gap_margin` -- The margin to take around missing data that is not reconstructed. Only applies to advanced mode.\n\t- Type: int\n\t- Default: 20\n- `gap_vt` -- A pupil-velocity threshold for detection of invalid data. Lower tresholds mean more data marked as invalid. Only applies to advanced mode.\n\t- Type: int, float\n\t- Default: 10\n- `mode` -- The algorithm to be used for blink reconstruction. Should be 'original' or 'advanced'. An advanced algorith was introduced in v0.13., and should be used for new analysis. The original algorithm is still the default for backwards compatibility.\n\t- Type: str\n\t- Default: 'original'\n\n__Returns:__\n\nA reconstructed singal.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"concatenate\" markdown=\"1\">\n\n## function __concatenate__\\(\\*series\\)\n\nConcatenates multiple series such that a new series is created with a\ndepth that is equal to the sum of the depths of all input series.\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import series as srs\n\ndm = DataMatrix(length=1)\ndm.s1 = SeriesColumn(depth=3)\ndm.s1[:] = 1,2,3\ndm.s2 = SeriesColumn(depth=3)\ndm.s2[:] = 3,2,1\ndm.s = srs.concatenate(dm.s1, dm.s2)\nprint(dm.s)\n\n~~~\n\n__Output:__\n\n~~~ .text\ncol[[1. 2. 3. 3. 2. 1.]]\n\n~~~\n\n\n__Argument list:__\n\n- `*series`: A list of series.\n\n__Returns:__\n\nA new series.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"downsample\" markdown=\"1\">\n\n## function __downsample__\\(series, by, fnc=<function nanmean at 0x7f31341e1090>\\)\n\nDownsamples a series by a factor, so that it becomes 'by' times\nshorter. The depth of the downsampled series is the highest multiple of\nthe depth of the original series divided by 'by'. For example,\ndownsampling a series with a depth of 10 by 3 results in a depth of 3.\n\n__Example:__\n\n\n~~~ .python\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom datamatrix import DataMatrix, SeriesColumn, series as srs\n\nLENGTH = 1 # Number of rows\nDEPTH = 100 # Depth (or length) of SeriesColumns\n\nsinewave = np.sin(np.linspace(0, 2*np.pi, DEPTH))\n\ndm = DataMatrix(length=LENGTH)\ndm.y = SeriesColumn(depth=DEPTH)\ndm.y.setallrows(sinewave)\ndm.y2 = srs.downsample(dm.y, by=10)\n\nplt.clf()\nplt.subplot(121)\nplt.title('Original')\nplt.plot(dm.y.plottable, 'o-')\nplt.subplot(122)\nplt.title('Downsampled')\nplt.plot(dm.y2.plottable, 'o-')\nplt.show()\n\n~~~\n\n![](/1.0/img/7.png)\n\n\n__Arguments:__\n\n- `series` -- No description\n- `by` -- The downsampling factor.\n\t- Type: int\n\n__Keywords:__\n\n- `fnc` -- The function to average the samples that are combined into 1 value. Typically an average or a median.\n\t- Type: callable\n\t- Default: <function nanmean at 0x7f31341e1090>\n\n__Returns:__\n\nA downsampled series.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"endlock\" markdown=\"1\">\n\n## function __endlock__\\(series\\)\n\nLocks a series to the end, so that any nan-values that were at the end\nare moved to the start.\n\n__Example:__\n\n\n~~~ .python\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom datamatrix import DataMatrix, SeriesColumn, series as srs\n\nLENGTH = 5 # Number of rows\nDEPTH = 10 # Depth (or length) of SeriesColumns\n\nsinewave = np.sin(np.linspace(0, 2*np.pi, DEPTH))\n\ndm = DataMatrix(length=LENGTH)\n# First create five identical rows with a sinewave\ndm.y = SeriesColumn(depth=DEPTH)\ndm.y.setallrows(sinewave)\n# Add a random offset to the Y values\ndm.y += np.random.random(LENGTH)\n# Set some observations at the end to nan\nfor i, row in enumerate(dm):\n   row.y[-i:] = np.nan\n# Lock the degraded traces to the end, so that all nans\n# now come at the start of the trace\ndm.y2 = srs.endlock(dm.y)\n\nplt.clf()\nplt.subplot(121)\nplt.title('Original (nans at end)')\nplt.plot(dm.y.plottable)\nplt.subplot(122)\nplt.title('Endlocked (nans at start)')\nplt.plot(dm.y2.plottable)\nplt.show()\n\n~~~\n\n![](/1.0/img/8.png)\n\n\n__Arguments:__\n\n- `series` -- The signal to end-lock.\n\t- Type: SeriesColumn\n\n__Returns:__\n\nAn end-locked signal.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"fft\" markdown=\"1\">\n\n## function __fft__\\(series, truncate=True\\)\n\n*New in v0.9.2*\n\nPerforms a fast-fourrier transform (FFT) for the signal. For more\ninformation, see [`numpy.fft`](https://docs.scipy.org/doc/numpy/reference/routines.fft.html#module-numpy.fft).\n\n__Example:__\n\n\n~~~ .python\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom datamatrix import DataMatrix, SeriesColumn, series as srs\n\nLENGTH = 3\nDEPTH = 200\n\n# Create one fast oscillation, and two combined fast and slow\n# oscillations\ndm = DataMatrix(length=LENGTH)\ndm.s = SeriesColumn(depth=DEPTH)\ndm.s[0] = np.sin(np.linspace(0, 150 * np.pi, DEPTH))\ndm.s[1] = np.sin(np.linspace(0, 75 * np.pi, DEPTH))\ndm.s[2] = np.sin(np.linspace(0, 10 * np.pi, DEPTH))\ndm.f = srs.fft(dm.s)\n\n# Plot the original signal\nplt.clf()\nplt.subplot(121)\nplt.title('Original')\nplt.plot(dm.s[0])\nplt.plot(dm.s[1])\nplt.plot(dm.s[2])\nplt.subplot(122)\n# And the filtered signal!\nplt.title('FFT')\nplt.plot(dm.f[0])\nplt.plot(dm.f[1])\nplt.plot(dm.f[2])\nplt.show()\n\n~~~\n\n__Output:__\n\n~~~ .text\n/home/sebastiaan/anaconda3/envs/pydata/lib/python3.10/site-packages/datamatrix/_datamatrix/_multidimension\nalcolumn.py:524: ComplexWarning: Casting complex values to real discards the imaginary part\n  self._seq[indices] = value\n\u001b[32m\u2826\u001b[0m Generating...\n~~~\n\n![](/1.0/img/9.png)\n\n\n__Arguments:__\n\n- `series` -- A signal to determine the FFT for.\n\t- Type: SeriesColumn\n\n__Keywords:__\n\n- `truncate` -- FFT series of real signals are symmetric. The `truncate` keyword indicates whether the last (symmetric) part of the FFT should be removed.\n\t- Type: bool\n\t- Default: True\n\n__Returns:__\n\nThe FFT of the signal.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"filter_bandpass\" markdown=\"1\">\n\n## function __filter\\_bandpass__\\(series, freq\\_range, order=2, sampling\\_freq=None\\)\n\n*New in v0.9.2*\n\n*Changed in v0.11.0: added `sampling_freq` argument* \n\nApplies a Butterworth bandpass-pass filter to the signal.\n\nFor more information, see [`scipy.signal`](https://docs.scipy.org/doc/scipy/reference/signal.html).\n\n__Example:__\n\n\n~~~ .python\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom datamatrix import DataMatrix, SeriesColumn, series as srs\n\nLENGTH = 3\nDEPTH = 100\nSAMPLING_FREQ = 100\n\n# Create one fast oscillation, and two combined fast and slow\n# oscillations\ndm = DataMatrix(length=LENGTH)\ndm.s = SeriesColumn(depth=DEPTH)\ndm.s[0] = np.sin(np.linspace(0, 20 * np.pi, DEPTH))  # 10 Hz\ndm.s[1] = np.sin(np.linspace(0, 10 * np.pi, DEPTH)) + dm.s[0]  # 5 Hz\ndm.s[2] = np.cos(np.linspace(0, 2 * np.pi, DEPTH)) + dm.s[0]  # 1 Hz\ndm.f = srs.filter_bandpass(dm.s, freq_range=(4, 6), sampling_freq=SAMPLING_FREQ)\n\n# Plot the original signal\nplt.clf()\nplt.subplot(121)\nplt.title('Original')\nplt.plot(dm.s[0])\nplt.plot(dm.s[1])\nplt.plot(dm.s[2])\nplt.subplot(122)\n# And the filtered signal!\nplt.title('Bandpass')\nplt.plot(dm.f[0])\nplt.plot(dm.f[1])\nplt.plot(dm.f[2])\nplt.show()\n\n~~~\n\n![](/1.0/img/10.png)\n\n\n__Arguments:__\n\n- `series` -- A signal to filter.\n\t- Type: SeriesColumn\n- `freq_range` -- A `(min_freq, max_freq)` tuple.\n\t- Type: tuple\n\n__Keywords:__\n\n- `order` -- The order of the filter.\n\t- Type: int\n\t- Default: 2\n- `sampling_freq` -- The sampling frequence of the signal, or `None` to use the scipy default of 2 half-cycles per sample.\n\t- Type: int, None\n\t- Default: None\n\n__Returns:__\n\nThe filtered signal.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"filter_highpass\" markdown=\"1\">\n\n## function __filter\\_highpass__\\(series, freq\\_min, order=2, sampling\\_freq=None\\)\n\n*New in v0.9.2*\n\n*Changed in v0.11.0: added `sampling_freq` argument* \n\nApplies a Butterworth highpass-pass filter to the signal.\n\nFor more information, see [`scipy.signal`](https://docs.scipy.org/doc/scipy/reference/signal.html).\n\n__Example:__\n\n\n~~~ .python\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom datamatrix import DataMatrix, SeriesColumn, series as srs\n\nLENGTH = 3\nDEPTH = 100\nSAMPLING_FREQ = 100\n\n# Create one fast oscillation, and two combined fast and slow\n# oscillations\ndm = DataMatrix(length=LENGTH)\ndm.s = SeriesColumn(depth=DEPTH)\ndm.s[0] = np.sin(np.linspace(0, 20 * np.pi, DEPTH))  # 10 Hz\ndm.s[1] = np.sin(np.linspace(0, 2 * np.pi, DEPTH)) + dm.s[0]  # 1 Hz\ndm.s[2] = np.cos(np.linspace(0, 2 * np.pi, DEPTH)) + dm.s[0]  # 1 Hz\ndm.f = srs.filter_highpass(dm.s, freq_min=3, sampling_freq=SAMPLING_FREQ)\n\n# Plot the original signal\nplt.clf()\nplt.subplot(121)\nplt.title('Original')\nplt.plot(dm.s[0])\nplt.plot(dm.s[1])\nplt.plot(dm.s[2])\nplt.subplot(122)\n# And the filtered signal!\nplt.title('Highpass')\nplt.plot(dm.f[0])\nplt.plot(dm.f[1])\nplt.plot(dm.f[2])\nplt.show()\n\n~~~\n\n![](/1.0/img/11.png)\n\n\n__Arguments:__\n\n- `series` -- A signal to filter.\n\t- Type: SeriesColumn\n- `freq_min` -- The minimum filter frequency.\n\t- Type: int\n\n__Keywords:__\n\n- `order` -- The order of the filter.\n\t- Type: int\n\t- Default: 2\n- `sampling_freq` -- The sampling frequence of the signal, or `None` to use the scipy default of 2 half-cycles per sample.\n\t- Type: int, None\n\t- Default: None\n\n__Returns:__\n\nThe filtered signal.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"filter_lowpass\" markdown=\"1\">\n\n## function __filter\\_lowpass__\\(series, freq\\_max, order=2, sampling\\_freq=None\\)\n\n*New in v0.9.2*\n\n*Changed in v0.11.0: added `sampling_freq` argument* \n\nApplies a Butterworth low-pass filter to the signal.\n\nFor more information, see [`scipy.signal`](https://docs.scipy.org/doc/scipy/reference/signal.html).\n\n__Example:__\n\n\n~~~ .python\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom datamatrix import DataMatrix, SeriesColumn, series as srs\n\nLENGTH = 3\nDEPTH = 100\nSAMPLING_FREQ = 100\n\n# Create one fast oscillation, and two combined fast and slow\n# oscillations\ndm = DataMatrix(length=LENGTH)\ndm.s = SeriesColumn(depth=DEPTH)\ndm.s[0] = np.sin(np.linspace(0, 20 * np.pi, DEPTH))  # 10 Hz\ndm.s[1] = np.sin(np.linspace(0, 2 * np.pi, DEPTH)) + dm.s[0]  # 1 Hz\ndm.s[2] = np.cos(np.linspace(0, 2 * np.pi, DEPTH)) + dm.s[0]  # 1 Hz\ndm.f = srs.filter_lowpass(dm.s, freq_max=3, sampling_freq=SAMPLING_FREQ)\n\n# Plot the original signal\nplt.clf()\nplt.subplot(121)\nplt.title('Original')\nplt.plot(dm.s[0])\nplt.plot(dm.s[1])\nplt.plot(dm.s[2])\nplt.subplot(122)\n# And the filtered signal!\nplt.title('Lowpass')\nplt.plot(dm.f[0])\nplt.plot(dm.f[1])\nplt.plot(dm.f[2])\nplt.show()\n\n~~~\n\n![](/1.0/img/12.png)\n\n\n__Arguments:__\n\n- `series` -- A signal to filter.\n\t- Type: SeriesColumn\n- `freq_max` -- The maximum filter frequency.\n\t- Type: int\n\n__Keywords:__\n\n- `order` -- The order of the filter.\n\t- Type: int\n\t- Default: 2\n- `sampling_freq` -- The sampling frequence of the signal, or `None` to use the scipy default of 2 half-cycles per sample.\n\t- Type: int, None\n\t- Default: None\n\n__Returns:__\n\nThe filtered signal.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"first_occurrence\" markdown=\"1\">\n\n", "url": "https://pydatamatrix.eu/1.0/series", "title": "datamatrix.series"}
{"content": "## function __first\\_occurrence__\\(series, value, equal=True\\)\n\nFinds the first occurence of a value for each row of a series column\nand returns the result as a float column of sample indices.\n\n*Version note:* New in 0.15.0\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, SeriesColumn, NAN, series as srs\n\ndm = DataMatrix(length=3)\ndm.s = SeriesColumn(depth=3)\ndm.s[0] = 1, 2, 3\ndm.s[1] = 1, 2, NAN\ndm.s[2] = NAN, NAN, NAN\ndm.first_nan = srs.first_occurrence(dm.s, value=NAN)\ndm.first_non_1 = srs.first_occurrence(dm.s, value=1, equal=False)\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----------+-------------+---------------+\n| # | first_nan | first_non_1 |       s       |\n+---+-----------+-------------+---------------+\n| 0 |    nan    |     1.0     |   [1. 2. 3.]  |\n| 1 |    2.0    |     1.0     | [ 1.  2. nan] |\n| 2 |    0.0    |     0.0     | [nan nan nan] |\n+---+-----------+-------------+---------------+\n\n~~~\n\n\n__Arguments:__\n\n- `series` -- The series column to search\n\t- Type: SeriesColumn\n- `value` -- The value to find in the series column. If `value` is a\nsequence, which has to be of the same length as the series,\nthen each row is searched for the value indicated by the\ncorresponding value in `value`.\n\t- Type: float, int, Sequence\n\n__Keywords:__\n\n- `equal` -- If `True`, the index of the first matching sample is returned. If `False`, the index of the first non-matching sample is returned.\n\t- Type: bool\n\t- Default: True\n\n__Returns:__\n\nA float column with sample indices or `NAN` for cells in which there was no match (or no mismatch if `equal=False`).\n\n- Type: FloatColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"interpolate\" markdown=\"1\">\n\n## function __interpolate__\\(series\\)\n\nLinearly interpolates missing (`nan`) data.\n\n__Example:__\n\n\n~~~ .python\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom datamatrix import DataMatrix, SeriesColumn, series as srs\n\nLENGTH = 1 # Number of rows\nDEPTH = 100 # Depth (or length) of SeriesColumns\nMISSING = 50 # Nr of missing samples\n\n# Create a sine wave with missing data\nsinewave = np.sin(np.linspace(0, 2*np.pi, DEPTH))\nsinewave[np.random.choice(np.arange(DEPTH), MISSING)] = np.nan\n# And turns this into a DataMatrix\ndm = DataMatrix(length=LENGTH)\ndm.y = SeriesColumn(depth=DEPTH)\ndm.y = sinewave\n# Now interpolate the missing data!\ndm.i = srs.interpolate(dm.y)\n\n# And plot the original data as circles and the interpolated data as\n# dotted lines\nplt.clf()\nplt.plot(dm.i.plottable, ':')\nplt.plot(dm.y.plottable, 'o')\nplt.show()\n\n~~~\n\n![](/1.0/img/13.png)\n\n\n__Arguments:__\n\n- `series` -- A signal to interpolate.\n\t- Type: SeriesColumn\n\n__Returns:__\n\nThe interpolated signal.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"last_occurrence\" markdown=\"1\">\n\n## function __last\\_occurrence__\\(series, value, equal=True\\)\n\nFinds the last occurence of a value for each row of a series column\nand returns the result as a float column of sample indices.\n\n*Version note:* New in 0.15.0\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, SeriesColumn, NAN\n\ndm = DataMatrix(length=3)\ndm.s = SeriesColumn(depth=3)\ndm.s[0] = 1, 2, 3\ndm.s[1] = 1, 2, NAN\ndm.s[2] = NAN, NAN, NAN\ndm.last_nan = srs.last_occurrence(dm.s, value=NAN)\ndm.last_non_1 = srs.last_occurrence(dm.s, value=1, equal=False)\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+----------+------------+---------------+\n| # | last_nan | last_non_1 |       s       |\n+---+----------+------------+---------------+\n| 0 |   nan    |    2.0     |   [1. 2. 3.]  |\n| 1 |   2.0    |    2.0     | [ 1.  2. nan] |\n| 2 |   2.0    |    2.0     | [nan nan nan] |\n+---+----------+------------+---------------+\n\n~~~\n\n\n__Arguments:__\n\n- `series` -- The series column to search\n\t- Type: SeriesColumn\n- `value` -- The value to find in the series column. If `value` is a\nsequence, which has to be of the same length as the series,\nthen each row is searched for the value indicated by the\ncorresponding value in `value`.\n\t- Type: float, int, Sequence\n\n__Keywords:__\n\n- `equal` -- If `True`, the index of the last matching sample is returned. If `False`, the index of the last non-matching sample is returned.\n\t- Type: bool\n\t- Default: True\n\n__Returns:__\n\nA float column with sample indices or `NAN` for cells in which there was no match (or no mismatch if `equal=False`).\n\n- Type: FloatColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"lock\" markdown=\"1\">\n\n## function __lock__\\(series, lock\\)\n\nShifts each row from a series by a certain number of steps along its\ndepth. This is useful to lock, or align, a series based on a sequence\nof values.\n\n__Example:__\n\n\n~~~ .python\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom datamatrix import DataMatrix, SeriesColumn, series as srs\n\nLENGTH = 5 # Number of rows\nDEPTH = 10 # Depth (or length) of SeriesColumns\n\ndm = DataMatrix(length=LENGTH)\n# First create five traces with a partial cosinewave. Each row is\n# offset slightly on the x and y axes\ndm.y = SeriesColumn(depth=DEPTH)\ndm.x_offset = -1\ndm.y_offset = -1\nfor row in dm:\n   row.x_offset = np.random.randint(0, DEPTH)\n   row.y_offset = np.random.random()\n   row.y = np.roll(\n       np.cos(np.linspace(0, np.pi, DEPTH)),\n       row.x_offset\n   ) + row.y_offset\n# Now use the x offset to lock the traces to the 0 point of the\n# cosine, i.e. to their peaks.\ndm.y2, zero_point = srs.lock(dm.y, lock=dm.x_offset)\n\nplt.clf()\nplt.subplot(121)\nplt.title('Original')\nplt.plot(dm.y.plottable)\nplt.subplot(122)\nplt.title('Locked to peak')\nplt.plot(dm.y2.plottable)\nplt.axvline(zero_point, color='black', linestyle=':')\nplt.show()\n\n~~~\n\n![](/1.0/img/14.png)\n\n\n__Arguments:__\n\n- `series` -- The signal to lock.\n\t- Type: SeriesColumn\n- `lock` -- A sequence of lock values with the same length as the Series. This can be a column, a list, a numpy array, etc.\n\n__Returns:__\n\nA `(series, zero_point)` tuple, in which `series` is a `SeriesColumn` and `zero_point` is the zero point to which the signal has been locked.\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"normalize_time\" markdown=\"1\">\n\n## function __normalize\\_time__\\(dataseries, timeseries\\)\n\n*New in v0.7.0*\n\nCreates a new series in which a series of timestamps (`timeseries`) is\nused as the indices for a series of data point (`dataseries`). This is\nuseful, for example, if you have a series of measurements and a\nseparate series of timestamps, and you want to combine the two.\n\nThe resulting series will generally contain a lot of `nan` values,\nwhich you can interpolate with `interpolate()`.\n\n__Example:__\n\n\n~~~ .python\nfrom matplotlib import pyplot as plt\nfrom datamatrix import DataMatrix, SeriesColumn, series as srs, NAN\n\n# Create a DataMatrix with one series column that contains samples\n# and one series column that contains timestamps.\ndm = DataMatrix(length=2)\ndm.samples = SeriesColumn(depth=3)\ndm.time = SeriesColumn(depth=3)\ndm.samples[0] = 3, 1, 2\ndm.time[0]    = 1, 2, 3\ndm.samples[1] = 1, 3, 2\ndm.time[1]    = 0, 5, 10\n# Create a normalized column with samples spread out according to\n# the timestamps, and also create an interpolate version of this\n# column for smooth plotting.\ndm.normalized = srs.normalize_time(\n   dataseries=dm.samples,\n   timeseries=dm.time\n)\ndm.interpolated = srs.interpolate(dm.normalized)\n# And plot!\nplt.clf()\nplt.plot(dm.normalized.plottable, 'o')\nplt.plot(dm.interpolated.plottable, ':')\nplt.xlabel('Time')\nplt.ylabel('Data')\nplt.show()\n\n~~~\n\n![](/1.0/img/15.png)\n\n\n__Arguments:__\n\n- `dataseries` -- A column with datapoints.\n\t- Type: SeriesColumn\n- `timeseries` -- A column with timestamps. This should be an increasing list of the same depth as `dataseries`. NAN values are allowed, but only at the end.\n\t- Type: SeriesColumn\n\n__Returns:__\n\nA new series in which the data points are spread according to the timestamps.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"roll\" markdown=\"1\">\n\n", "url": "https://pydatamatrix.eu/1.0/series", "title": "datamatrix.series"}
{"content": "## function __roll__\\(series, shift\\)\n\nRolls (or shifts) the elements along the depth of the series. Elements\nthat run off the last position are re-introduced at the first position\nand vice versa.\n\n*Version note:* New in 0.15.0\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, SeriesColumn, series as srs\n\ndm = DataMatrix(length=3)\ndm.s = SeriesColumn(depth=4)\ndm.s = [[1, 2, 3, 4],\n        [10, 20, 30, 40],\n        [100, 200, 300, 400]]\ndm.t = srs.roll(dm.s, shift=1)\ndm.u = srs.roll(dm.s, shift=[1, 0, -1])\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-----------------------+-----------------------+-----------------------+\n| # |           s           |           t           |           u           |\n+---+-----------------------+-----------------------+-----------------------+\n| 0 |     [1. 2. 3. 4.]     |     [4. 1. 2. 3.]     |     [4. 1. 2. 3.]     |\n| 1 |   [10. 20. 30. 40.]   |   [40. 10. 20. 30.]   |   [10. 20. 30. 40.]   |\n| 2 | [100. 200. 300. 400.] | [400. 100. 200. 300.] | [200. 300. 400. 100.] |\n+---+-----------------------+-----------------------+-----------------------+\n\n~~~\n\n\n__Arguments:__\n\n- `series` -- The series column to roll\n\t- Type: SeriesColumn\n- `shift` -- The number of places to roll by. If `shift` is an `int`, each row is shifted by the same amount. If `shift` is a sequence, which has to be of the same length as the series, then each row is shifted by the amounted indicated by the corresponding value in `shift`.\n\t- Type: int, Sequence\n\n__Returns:__\n\nThe rolled series.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"smooth\" markdown=\"1\">\n\n## function __smooth__\\(series, winlen=11, wintype=u'hanning'\\)\n\nSmooths a signal using a window with requested size.\n\nThis method is based on the convolution of a scaled window with the\nsignal. The signal is prepared by introducing reflected copies of the\nsignal (with the window size) in both ends so that transient parts are\nminimized in the begining and end part of the output signal.\n\n__Adapted from:__\n\n- <http://www.scipy.org/Cookbook/SignalSmooth>\n\n__Example:__\n\n\n~~~ .python\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom datamatrix import DataMatrix, SeriesColumn, series as srs\n\nLENGTH = 5 # Number of rows\nDEPTH = 100 # Depth (or length) of SeriesColumns\n\nsinewave = np.sin(np.linspace(0, 2*np.pi, DEPTH))\n\ndm = DataMatrix(length=LENGTH)\n# First create five identical rows with a sinewave\ndm.y = SeriesColumn(depth=DEPTH)\ndm.y.setallrows(sinewave)\n# And add a bit of random jitter\ndm.y += np.random.random( (LENGTH, DEPTH) )\n# Smooth the traces to reduce the jitter\ndm.y2 = srs.smooth(dm.y)\n\nplt.clf()\nplt.subplot(121)\nplt.title('Original')\nplt.plot(dm.y.plottable)\nplt.subplot(122)\nplt.title('Smoothed')\nplt.plot(dm.y2.plottable)\nplt.show()\n\n~~~\n\n![](/1.0/img/16.png)\n\n\n__Arguments:__\n\n- `series` -- A signal to smooth.\n\t- Type: SeriesColumn\n\n__Keywords:__\n\n- `winlen` -- The width of the smoothing window. This should be an odd integer.\n\t- Type: int\n\t- Default: 11\n- `wintype` -- The type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'. A flat window produces a moving average smoothing.\n\t- Type: str\n\t- Default: 'hanning'\n\n__Returns:__\n\nA smoothed signal.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"threshold\" markdown=\"1\">\n\n## function __threshold__\\(series, fnc, min\\_length=1\\)\n\nFinds samples that satisfy some threshold criterion for a given period.\n\n__Example:__\n\n\n~~~ .python\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom datamatrix import DataMatrix, SeriesColumn, series as srs\n\nLENGTH = 1 # Number of rows\nDEPTH = 100 # Depth (or length) of SeriesColumns\n\nsinewave = np.sin(np.linspace(0, 2*np.pi, DEPTH))\n\ndm = DataMatrix(length=LENGTH)\n# First create five identical rows with a sinewave\ndm.y = SeriesColumn(depth=DEPTH)\ndm.y.setallrows(sinewave)\n# And also a bit of random jitter\ndm.y += np.random.random( (LENGTH, DEPTH) )\n# Threshold the signal by > 0 for at least 10 samples\ndm.t = srs.threshold(dm.y, fnc=lambda y: y > 0, min_length=10)\n\nplt.clf()\n", "url": "https://pydatamatrix.eu/1.0/series", "title": "datamatrix.series"}
{"content": "# Mark the thresholded signal\nplt.fill_between(np.arange(DEPTH), dm.t[0], color='black', alpha=.25)\nplt.plot(dm.y.plottable)\nprint(dm)\nplt.show()\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-------------------+-----------------------------------+\n| # |         t         |                 y                 |\n+---+-------------------+-----------------------------------+\n| 0 | [1. 1. ... 0. 0.] | [0.3588 0.9991 ... 0.1106 0.3037] |\n+---+-------------------+-----------------------------------+\n\n~~~\n\n![](/1.0/img/17.png)\n\n\n__Arguments:__\n\n- `series` -- A signal to threshold.\n\t- Type: SeriesColumn\n- `fnc` -- A function that takes a single value and returns True if this value exceeds a threshold, and False otherwise.\n\t- Type: FunctionType\n\n__Keywords:__\n\n- `min_length` -- The minimum number of samples for which `fnc` must return True.\n\t- Type: int\n\t- Default: 1\n\n__Returns:__\n\nA series where 0 indicates below threshold, and 1 indicates above threshold.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"trim\" markdown=\"1\">\n\n## function __trim__\\(series, value=nan, start=False, end=True\\)\n\nTrims trailing and/ or leading values from a series. This is useful,\nfor example, to discard the end (or beginning) of a series that\nconsists exclusively of invalid data, such as `NAN` or 0 values.\n\n*Version note:* New in 0.15.0\n\n__Example:__\n\n\n~~~ .python\nfrom datamatrix import DataMatrix, SeriesColumn, series as srs\n\ndm = DataMatrix(length=3)\ndm.s = SeriesColumn(depth=5)\ndm.s[0] = 0, 0, 2, 0, 0\ndm.s[1] = 0, 0, 0, 3, 0\ndm.s[2] = 0, 0, 2, 3, 0\ndm.trimmed = srs.trim(dm.s, value=0, start=True, end=True)\nprint(dm)\n\n~~~\n\n__Output:__\n\n~~~ .text\n+---+-------------------+---------+\n| # |         s         | trimmed |\n+---+-------------------+---------+\n| 0 | [0. 0. ... 0. 0.] | [2. 0.] |\n| 1 | [0. 0. ... 3. 0.] | [0. 3.] |\n| 2 | [0. 0. ... 3. 0.] | [2. 3.] |\n+---+-------------------+---------+\n\n~~~\n\n\n__Arguments:__\n\n- `series` -- The series column to trim\n\t- Type: SeriesColumn\n\n__Keywords:__\n\n- `value` -- The value to trim\n\t- Type: int, float\n\t- Default: nan\n- `start` -- Indicates whether the start of the series should be trimmed\n\t- Type: bool\n\t- Default: False\n- `end` -- Indicates whether the end of the series should be trimmed\n\t- Type: bool\n\t- Default: True\n\n__Returns:__\n\nA trimmed copy of the series column\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"window\" markdown=\"1\">\n\n", "url": "https://pydatamatrix.eu/1.0/series", "title": "datamatrix.series"}
{"content": "## function __window__\\(series, start=0, end=None\\)\n\nExtracts a window from a signal.\n\n*Version note:* As of 0.9.4, the preferred way to get a window from a\nseries is with a slice: `dm.s[:, start:end]`.\n\n__Example:__\n\n\n~~~ .python\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom datamatrix import DataMatrix, SeriesColumn, series as srs\n\nLENGTH = 5 # Number of rows\nDEPTH = 10 # Depth (or length) of SeriesColumns\n\nsinewave = np.sin(np.linspace(0, 2*np.pi, DEPTH))\n\ndm = DataMatrix(length=LENGTH)\n# First create five identical rows with a sinewave\ndm.y = SeriesColumn(depth=DEPTH)\ndm.y.setallrows(sinewave)\n# Add a random offset to the Y values\ndm.y += np.random.random(LENGTH)\n# Look only the middle half of the signal\ndm.y2 = srs.window(dm.y, start=DEPTH//4, end=-DEPTH//4)\n\nplt.clf()\nplt.subplot(121)\nplt.title('Original')\nplt.plot(dm.y.plottable)\nplt.subplot(122)\nplt.title('Window (middle half)')\nplt.plot(dm.y2.plottable)\nplt.show()\n\n~~~\n\n![](/1.0/img/18.png)\n\n\n__Arguments:__\n\n- `series` -- The signal to get a window from.\n\t- Type: SeriesColumn\n\n__Keywords:__\n\n- `start` -- The window start.\n\t- Type: int\n\t- Default: 0\n- `end` -- The window end, or None to go to the signal end.\n\t- Type: int, None\n\t- Default: None\n\n__Returns:__\n\nA window of the signal.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"z\" markdown=\"1\">\n\n## function __z__\\(series\\)\n\nApplies a *z*-transform to the signal such that each trace has a mean\nvalue of 0 and a standard deviation of 1. That is, each trace is\n*z*-transformed individually.\n\n*Note:* If you want to *z*-transform a series column such that the mean\nof the full series is 0 with a standard deviation of 1, then use\n`operations.z()`.\n\n__Example:__\n\n\n~~~ .python\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom datamatrix import DataMatrix, SeriesColumn, series as srs\n\nLENGTH = 3\nDEPTH = 200\n\n# Create one fast oscillation, and two combined fast and slow\n# oscillations\ndm = DataMatrix(length=LENGTH)\ndm.s = SeriesColumn(depth=DEPTH)\ndm.s[0] = 1 * np.sin(np.linspace(0, 4 * np.pi, DEPTH))\ndm.s[1] = 2 * np.sin(np.linspace(.4, 4.4 * np.pi, DEPTH))\ndm.s[2] = 3 * np.sin(np.linspace(.8, 4.8 * np.pi, DEPTH))\ndm.z = srs.z(dm.s)\n\n# Plot the original signal\nplt.clf()\nplt.subplot(121)\nplt.title('Original')\nplt.plot(dm.s[0])\nplt.plot(dm.s[1])\nplt.plot(dm.s[2])\nplt.subplot(122)\n# And the filtered signal!\nplt.title('Z transform')\nplt.plot(dm.z[0])\nplt.plot(dm.z[1])\nplt.plot(dm.z[2])\nplt.show()\n\n~~~\n\n![](/1.0/img/19.png)\n\n\n__Arguments:__\n\n- `series` -- A signal to determine the z-transform for.\n\t- Type: SeriesColumn\n\n__Returns:__\n\nThe z-transform of the signal.\n\n- Type: SeriesColumn\n\n</div>\n\n</div>\n\n\n", "url": "https://pydatamatrix.eu/1.0/series", "title": "datamatrix.series"}
